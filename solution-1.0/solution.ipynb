{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score, confusion_matrix\n",
    "\n",
    "random_seed = 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data-Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. diet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuthorId      object\n",
       "Diet        category\n",
       "Age            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_diet_data = pd.read_csv('../aufgabe/training_dataset/diet.csv')\n",
    "# print(users_diet.head())\n",
    "# users_diet.shape\n",
    "# users_diet.dtypes\n",
    "\n",
    "# Nullwerte\n",
    "# users_diet.isnull().sum()\n",
    "\n",
    "# 1. Spalte \"Diet\"\n",
    "# -> ein Nullwert drin \n",
    "#users_diet[users_diet[\"Diet\"].isna()]\n",
    "\n",
    "# Author 646062A ohne Wert für Diet -> Weg mit dem Hund\n",
    "users_diet_data = users_diet_data.drop(users_diet_data[users_diet_data[\"Diet\"].isna()].index).reset_index(drop=True)\n",
    "\n",
    "# Weitere missing values finden: z.B. <empty field>, \"0\", \".\", \"999\", \"NA\" ...\n",
    "# users_diet[(users_diet[\"Diet\"] == \"\") | (users_diet[\"Diet\"] == \".\") | (users_diet[\"Diet\"] == \"999\")]\n",
    "\n",
    "\n",
    "# 2. Spalte \"Age\"\n",
    "# users_diet[users_diet[\"Age\"] > 100]\n",
    "# users_diet[users_diet[\"Age\"] < 5]\n",
    "# print(\"Range Alter\", users_diet[\"Age\"].min(), users_diet[\"Age\"].max())\n",
    "\n",
    "# 3. Spalte \"AuthorId\"\n",
    "# print(\"Einzigartige IDs: \", users_diet[\"AuthorId\"].nunique(), users_diet.shape[0])\n",
    "\n",
    "\n",
    "# Datentyp bei \"Diet\" zu Category ändern\n",
    "users_diet_data[\"Diet\"] = users_diet_data['Diet'].astype(\"category\")\n",
    "users_diet_data.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      AuthorId  RecipeId  Rating Like  TestSetId\n",
      "0     2492191A     33671     2.0  NaN        1.0\n",
      "1  2002019979A     92647     2.0  NaN        2.0\n",
      "3  2001625557E    108231     2.0  NaN        4.0\n",
      "6      588901B     87380     2.0  NaN        7.0\n",
      "7     1038235B      9475     2.0  NaN        8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/z37d__mj75g9k_jcd30hbw0c0000gn/T/ipykernel_91848/2431628837.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews_data = pd.read_csv(\"../aufgabe/training_dataset/reviews.csv\")\n"
     ]
    }
   ],
   "source": [
    "reviews_data = pd.read_csv(\"../aufgabe/training_dataset/reviews.csv\")\n",
    "\n",
    "# reviews_data_test.dtypes\n",
    "reviews_userToPredict_data = reviews_data[reviews_data[\"Rating\"].notna()]\n",
    "print(reviews_userToPredict_data.head())\n",
    "\n",
    "# Test-Daten -> TestSetId != NaN, die anderen nicht für Modelle verwenden\n",
    "reviews_data = reviews_data[reviews_data[\"TestSetId\"].isna()].reset_index(drop=True)\n",
    "reviews_data.loc[reviews_data['Rating'].isna(), \"Rating\"] = 999\n",
    "reviews_data[\"Rating\"] = reviews_data[\"Rating\"].astype(\"category\")\n",
    "\n",
    "# Like column zu boolean\n",
    "reviews_data[\"Like\"] = reviews_data[\"Like\"].astype(\"bool\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. requests.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_data = pd.read_csv(\"../aufgabe/training_dataset/requests.csv\")\n",
    "requests_data.head()\n",
    "\n",
    "# Column: Time\n",
    "# Runden\n",
    "\"\"\" \n",
    "Column Time -> runden, da Nachkommastellen bei Kochzeit irrelevant\n",
    "Beschreibung: The duration a recipe should take at most (including the time reserved\n",
    "for the preparation and cooking).\n",
    "\"\"\"\n",
    "requests_data[\"Time\"] = requests_data[\"Time\"].round(1)\n",
    "\n",
    "# Teilweise negative Werte -> 0\n",
    "requests_data.loc[requests_data[\"Time\"] <= 0, \"Time\"] = 0\n",
    "\"\"\"\n",
    "Test, ob negative Werte immer False als \"Like\" haben -> stimmt aber nicht siehe Code:\n",
    "Requests mit [AuthorId, RecipeId] time <= 0 mit reviews [AuthorId, RecipeId] joinen und dort like checken\n",
    "joined_data = requests_data.merge(reviews_data_test, on=[\"AuthorId\", \"RecipeId\"], how=\"left\")\n",
    "joined_data = joined_data[joined_data[\"Time\"] <= 0]\n",
    "joined_data = joined_data[~joined_data[\"Like\"].isna()]\n",
    "\"\"\"\n",
    "\n",
    "# Column: HighCalories\n",
    "requests_data[\"HighCalories\"] = requests_data[\"HighCalories\"].astype(\"bool\")\n",
    "\n",
    "# Column: HighProtein\n",
    "\"\"\"\n",
    "2 Werte: Indifferent und Yes\n",
    "Daraus wird boolean indifferent = False und Yes = True\n",
    "\"\"\"\n",
    "requests_data.loc[requests_data[\"HighProtein\"] == \"Indifferent\", \"HighProtein\"] = 0\n",
    "requests_data.loc[requests_data[\"HighProtein\"] == \"Yes\", \"HighProtein\"] = 1\n",
    "requests_data[\"HighProtein\"] = requests_data[\"HighProtein\"].astype(\"bool\")\n",
    "\n",
    "# Column: LowFat\n",
    "requests_data[\"LowFat\"] = requests_data[\"LowFat\"].astype(\"bool\")\n",
    "\n",
    "# Column: LowSugar\n",
    "\"\"\"\n",
    "2 Werte: Indifferent und 0. Interpretation: 0 -> user braucht kein low-sugar Inhalt, Indifferent -> User ist es egal\n",
    "Daraus wird boolean 0 = False und indifferent = True\n",
    "\"\"\"\n",
    "requests_data.loc[requests_data[\"LowSugar\"] == \"0\", \"LowSugar\"] = 0\n",
    "requests_data.loc[requests_data[\"LowSugar\"] == \"Indifferent\", \"LowSugar\"] = 1\n",
    "requests_data[\"LowSugar\"] = requests_data[\"LowSugar\"].astype(\"bool\")\n",
    "\n",
    "# Column HighFiber\n",
    "requests_data[\"HighFiber\"] = requests_data[\"HighFiber\"].astype(\"bool\")\n",
    "\n",
    "# requests_data[\"HighFiber\"].unique()\n",
    "# requests_data.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. recipes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape vorher:  (75604, 18)\n",
      "Shape nach Kürzung der Outlier:  (75524, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# fehlende Werte in RecipeServings durch random forest imputieren\\nfeatures = [\\'CookTime\\', \\'PrepTime\\', \\'Calories\\', \\'FatContent\\', \\'FiberContent\\', \\'ProteinContent\\']\\n\\n# test_data with and without RecipeServings, training_data only with RecipeServings\\nknown_servings = recipes_data[recipes_data[\\'ServingClass\\'].notna()]\\nunknown_servings = recipes_data[recipes_data[\\'ServingClass\\'].isna()]\\n\\nX = known_servings[features]\\ny_known = known_servings[\"ServingClass\"]\\n\\n# print(X)\\n# print(y_known)\\nX_train, X_val, y_train, y_val = train_test_split(X, y_known, test_size=0.3, random_state=random_seed)\\n# print(\\'Training Features Shape:\\', X_train.shape)\\n# print(\\'Training Labels Shape:\\', y_train.shape)\\n# print(\\'Validation Features Shape:\\', X_val.shape)\\n# print(\\'Validation Labels Shape:\\', y_val.shape)\\n\\n\\n# Model trainieren\\nmodel = RandomForestClassifier(n_estimators=200, random_state=random_seed)\\nmodel.fit(X_train, y_train)\\n\\n# Model evaluieren\\ny_pred = model.predict(X_val)\\n\\n# Bewertung des Modells\\nmse = mean_squared_error(y_val, y_pred)\\n# calculate r squared\\nr_squared = model.score(X_val, y_val)\\nprint(\\'R Squared: \\', r_squared)\\nprint(\\'MSE Mean Squred Error: \\', mse) \\n\\n\\n# y_val together with y_pred \\ny_val_pred = pd.DataFrame({\\'y_val\\': y_val, \\'y_pred\\': y_pred})\\n# Extract RecipeId for the validation set\\nrecipe_ids_val = recipes_data.loc[y_val.index, \\'RecipeId\\']\\n# Add RecipeId to the y_val_pred dataframe\\ny_val_pred[\\'RecipeId\\'] = recipe_ids_val\\n\\n# Display the dataframe\\nprint(y_val_pred.head(20))\\n\\n\\n# Model anwenden um fehlende Daten zu analysieren\\n# X_unknown = unknown_servings[features]\\n# y_unknown = unknown_servings[\"RecipeServings\"]\\n# y_unknown_pred = model.predict(X_unknown)\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_data = pd.read_csv(\"../aufgabe/training_dataset/recipes.csv\")\n",
    "threshold = 1\n",
    "threshold_RecipeServings = 3.5\n",
    "\n",
    "\"\"\"\n",
    "Outlier Detection:\n",
    "Außerhalb von 3,5*Standardabweichung -> Outlier\n",
    "\n",
    "=> Outlier werden entfernt\n",
    "\"\"\"\n",
    "# Name to lower\n",
    "recipes_data[\"Name\"] = recipes_data[\"Name\"].str.lower()\n",
    "\n",
    "# Column CookTime und Column PrepTime\n",
    "print(\"Shape vorher: \", recipes_data.shape)\n",
    "\n",
    "\n",
    "# Column RecipeCategory\n",
    "recipes_data[\"RecipeCategory\"] = recipes_data[\"RecipeCategory\"].apply(lambda x: \"Beverage\" if x == \"Beverages\" else (\"Meals\" if (x == \"Lunch\") or (x == \"Breakfast\")  else \"Other\"))\n",
    "recipes_data[\"RecipeCategory\"] = recipes_data[\"RecipeCategory\"].astype(\"category\")\n",
    "\n",
    "\n",
    "# Column RecipeIngredientQuantities und Column RecipeIngredientParts\n",
    "# zu liste von strings umwandeln\n",
    "recipes_data[\"RecipeIngredientQuantities\"] = recipes_data[\"RecipeIngredientQuantities\"].str.replace('character(0)', '').str.lstrip('\"c(\"').str.replace('\"', '').str.replace(\")\", \"\").str.replace('\\\\', '').str.split(\",\")\n",
    "recipes_data[\"RecipeIngredientParts\"] = recipes_data[\"RecipeIngredientParts\"].str.lower().replace('character(0)', '').str.lstrip('\"c(\"').str.replace('\"', '').str.replace(\")\", \"\").str.replace('\\\\', '').str.split(\",\")\n",
    "\n",
    "# Einteilung ob die Zutat in der Liste ist oder nicht\n",
    "# Fleisch\n",
    "value_meat = [\"chicken\", \"veal\", \"pork\", \"beef\", \"turkey\", \"ham\", \"bacon\", \"lamb\", \"duck\", \"goose\", \"rabbit\", \"venison\", \"quail\", \"pheasant\", \"alligator\", \"sausage\"]\n",
    "# Meeresfrüchte\n",
    "value_sea = [\"fish\", \"crab\", \"lobster\", \"shrimp\", \"prawn\", \"clam\", \"mussel\", \"scallop\", \"squid\", \"octopus\", \"anchovy\", \"sardine\", \"tuna\", \"salmon\", \"trout\", \"herring\", \"cod\", \"mackerel\", \"bass\", \"swordfish\", \"sturgeon\", \"walleye\", \"caviar\", \"crayfish\", \"cuttlefish\", \"sea cucumber\", \"sea snail\", \"sea bass\", \"sea bream\", \"sea trout\", \"seafood\", \"shellfish\"]\n",
    "# vegetarisch\n",
    "value_vegetarian = [\"tofu\", \"seitan\", \"tempeh\", \"plant-based\"]\n",
    "# vegan\n",
    "value_vegan = [\"vegan\"]\n",
    "\n",
    "def beinhaltet_substring(ingredient_list, category_list):\n",
    "    for ingredient in ingredient_list:\n",
    "        if any(cat in ingredient for cat in category_list):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "recipes_data[\"Meat\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_meat))\n",
    "recipes_data[\"Seafood\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_sea))\n",
    "recipes_data[\"Vegetarian\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_vegetarian))\n",
    "recipes_data[\"Vegan\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_vegan))\n",
    "\n",
    "\n",
    "# weitere Unterteilung, falls mehrere Kategorien anschlagen und falls alles 0, dann vegetarisch 1\n",
    "for index, row in recipes_data.iterrows():\n",
    "    if(beinhaltet_substring([row[\"Name\"]], value_vegan) == 1): \n",
    "        recipes_data.loc[index, [\"Vegan\"]] = 1\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 0\n",
    "        recipes_data.loc[index, [\"Seafood\"]] = 0\n",
    "        recipes_data.loc[index, [\"Meat\"]] = 0\n",
    "    elif(row[\"Meat\"] == 1 or row[\"Seafood\"] == 1):\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 0\n",
    "        recipes_data.loc[index, [\"Vegan\"]] = 0\n",
    "    elif(row[\"Vegetarian\"] == 0 and row[\"Vegan\"] == 0):\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 1\n",
    "\n",
    "\n",
    "# print(recipes_data[[\"RecipeId\", \"Meat\", \"Seafood\", \"Vegetarian\", \"Vegan\"]].head(20))\n",
    "# print(recipes_data[(recipes_data[\"Vegan\"] == 0) & (recipes_data[\"Vegetarian\"] == 0) & (recipes_data[\"Meat\"] == 0) & (recipes_data[\"Seafood\"] == 0)].value_counts())\n",
    "\n",
    "\n",
    "# Column Calories\n",
    "# Outlier weg\n",
    "recipes_data[\"Calories\"] = recipes_data[\"Calories\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "\n",
    "std_Calories = recipes_data['Calories'].std()\n",
    "mean_Calories = recipes_data['Calories'].mean()\n",
    "upper_limit_Calories = mean_Calories + threshold * std_Calories\n",
    "lower_limit_Calories = mean_Calories - threshold * std_Calories\n",
    "\n",
    "recipes_data[\"High-Calories-class\"] = recipes_data[\"Calories\"].apply(lambda x: 1 if x > upper_limit_Calories else 0)\n",
    "\n",
    "\n",
    "\n",
    "# recipes_data = recipes_data[(recipes_data[\"Calories\"] >= lower_limit_Calories) & (recipes_data['Calories'] <= upper_limit_Calories)]\n",
    "\n",
    "\n",
    "# Column FatContent\n",
    "# Oulier weg\n",
    "\n",
    "recipes_data[\"FatContent\"] = recipes_data[\"FatContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_FatContent = recipes_data['FatContent'].std()\n",
    "mean_FatContent = recipes_data['FatContent'].mean()\n",
    "upper_limit_FatContent = mean_FatContent + threshold * std_FatContent\n",
    "lower_limit_FatContent = mean_FatContent - threshold * std_FatContent\n",
    "\n",
    "recipes_data[\"Low-FatContent-class\"] = recipes_data[\"FatContent\"].apply(lambda x: 1 if x > lower_limit_FatContent else 0)\n",
    "\n",
    "# recipes_data = recipes_data[(recipes_data[\"FatContent\"] >= lower_limit_FatContent) & (recipes_data['FatContent'] <= upper_limit_FatContent)]\n",
    "\n",
    "\n",
    "# Column SaturatedFatContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SaturatedFatContent\"] = recipes_data[\"SaturatedFatContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_SaturatedFatContent = recipes_data['SaturatedFatContent'].std()\n",
    "mean_SaturatedFatContent = recipes_data['SaturatedFatContent'].mean()\n",
    "upper_limit_SaturatedFatContent = mean_SaturatedFatContent + threshold * std_SaturatedFatContent\n",
    "lower_limit_SaturatedFatContent = mean_SaturatedFatContent - threshold * std_SaturatedFatContent\n",
    "\n",
    "recipes_data[\"Low-SaturatedFatContent-class\"] = recipes_data[\"SaturatedFatContent\"].apply(lambda x: 1 if x > lower_limit_SaturatedFatContent else 0)\n",
    "\n",
    "# recipes_data = recipes_data[(recipes_data[\"SaturatedFatContent\"] >= lower_limit_SaturatedFatContent) & (recipes_data['SaturatedFatContent'] <= upper_limit_SaturatedFatContent)]\n",
    "\n",
    "\n",
    "# Column CholesterolContent\n",
    "# Outlier weg\n",
    "recipes_data[\"CholesterolContent\"] = recipes_data[\"CholesterolContent\"].apply(lambda x: np.log(x + 1) if x > 0 else x)\n",
    "\n",
    "std_CholesterolContent = recipes_data['CholesterolContent'].std()\n",
    "mean_CholesterolContent = recipes_data['CholesterolContent'].mean()\n",
    "upper_limit_CholesterolContent = mean_CholesterolContent + threshold * std_CholesterolContent\n",
    "lower_limit_CholesterolContent = mean_CholesterolContent - threshold * std_CholesterolContent\n",
    "\n",
    "recipes_data[\"High-CholesterolContent-class\"] = recipes_data[\"CholesterolContent\"].apply(lambda x: 1 if x > upper_limit_CholesterolContent else 0)\n",
    "\n",
    "# recipes_data = recipes_data[(recipes_data[\"CholesterolContent\"] >= lower_limit_CholesterolContent) & (recipes_data['CholesterolContent'] <= upper_limit_CholesterolContent)]\n",
    "\n",
    "\n",
    "# Column SodiumContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SodiumContent\"] = recipes_data[\"SodiumContent\"].apply(lambda x: np.log(x + 1) if x > 0 else x)\n",
    "\n",
    "std_SodiumContent = recipes_data['SodiumContent'].std()\n",
    "mean_SodiumContent = recipes_data['SodiumContent'].mean()\n",
    "upper_limit_SodiumContent = mean_SodiumContent + threshold * std_SodiumContent\n",
    "lower_limit_SodiumContent = mean_SodiumContent - threshold * std_SodiumContent\n",
    "\n",
    "recipes_data[\"High-SodiumContent-class\"] = recipes_data[\"SodiumContent\"].apply(lambda x: 1 if x > upper_limit_SodiumContent else 0)\n",
    "\n",
    "# recipes_data = recipes_data[(recipes_data[\"SodiumContent\"] >= lower_limit_SodiumContent) & (recipes_data['SodiumContent'] <= upper_limit_SodiumContent)]\n",
    "\n",
    "\n",
    "# Column CarbohydrateContent\n",
    "# Outlier weg\n",
    "recipes_data[\"CarbohydrateContent\"] = recipes_data[\"CarbohydrateContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_CarbohydrateContent = recipes_data['CarbohydrateContent'].std()\n",
    "mean_CarbohydrateContent = recipes_data['CarbohydrateContent'].mean()\n",
    "upper_limit_CarbohydrateContent = mean_CarbohydrateContent + threshold * std_CarbohydrateContent\n",
    "lower_limit_CarbohydrateContent = mean_CarbohydrateContent - threshold * std_CarbohydrateContent\n",
    "\n",
    "recipes_data[\"Low-CarbsContent-class\"] = recipes_data[\"CarbohydrateContent\"].apply(lambda x: 1 if x > lower_limit_CarbohydrateContent else 0)\n",
    "\n",
    "# recipes_data = recipes_data[(recipes_data[\"CarbohydrateContent\"] >= lower_limit_CarbohydrateContent) & (recipes_data['CarbohydrateContent'] <= upper_limit_CarbohydrateContent)]\n",
    "\n",
    "\n",
    "# Column FiberContent\n",
    "# Outlier weg\n",
    "recipes_data[\"FiberContent\"] = recipes_data[\"FiberContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_FiberContent = recipes_data['FiberContent'].std()\n",
    "mean_FiberContent = recipes_data['FiberContent'].mean()\n",
    "upper_limit_FiberContent = mean_FiberContent + threshold * std_FiberContent\n",
    "lower_limit_FiberContent = mean_FiberContent - threshold * std_FiberContent\n",
    "\n",
    "recipes_data[\"High-FiberContent-class\"] = recipes_data[\"FiberContent\"].apply(lambda x: 1 if x > upper_limit_FiberContent else 0)\n",
    "\n",
    "# recipes_data = recipes_data[(recipes_data[\"FiberContent\"] >= lower_limit_FiberContent) & (recipes_data['FiberContent'] <= upper_limit_FiberContent)]\n",
    "\n",
    "\n",
    "# Column SugarContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SugarContent\"] = recipes_data['SugarContent'].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_SugarContent = recipes_data['SugarContent'].std()\n",
    "mean_SugarContent = recipes_data['SugarContent'].mean()\n",
    "upper_limit_SugarContent = mean_SugarContent + threshold * std_SugarContent\n",
    "lower_limit_SugarContent = mean_SugarContent - threshold * std_SugarContent\n",
    "\n",
    "recipes_data[\"Low-SugarContent-class\"] = recipes_data[\"SugarContent\"].apply(lambda x: 1 if x > lower_limit_SugarContent else 0)\n",
    "\n",
    "# recipes_data = recipes_data[(recipes_data[\"SugarContent\"] >= lower_limit_SugarContent) & (recipes_data['SugarContent'] <= upper_limit_SugarContent)]\n",
    "\n",
    "\n",
    "\n",
    "# Column ProteinContent\n",
    "# Outlier weg\n",
    "recipes_data[\"ProteinContent\"] = recipes_data[\"ProteinContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_ProteinContent = recipes_data['ProteinContent'].std()\n",
    "mean_ProteinContent = recipes_data['ProteinContent'].mean()\n",
    "upper_limit_ProteinContent = mean_ProteinContent + threshold * std_ProteinContent\n",
    "lower_limit_ProteinContent = mean_ProteinContent - threshold * std_ProteinContent\n",
    "\n",
    "recipes_data[\"High-ProteinContent-class\"] = recipes_data[\"ProteinContent\"].apply(lambda x: 1 if x > upper_limit_ProteinContent else 0)\n",
    "\n",
    "# recipes_data = recipes_data[(recipes_data[\"ProteinContent\"] >= lower_limit_ProteinContent) & (recipes_data['ProteinContent'] <= upper_limit_ProteinContent)]\n",
    "\n",
    "\n",
    "# Column RecipeServings & Column RecipeYield\n",
    "\"\"\"\n",
    "RecipeServings: Anzahl der Portionen, die das Rezept ergibt\n",
    "RecipeYield: Gibt an, wie viele Stücke man aus dem Rezept erhält. Ein Rezept ergibt zum Beispiel 1/2 Liter Suppe, was 2 Portionen entspricht.\n",
    "-> Versuch: RecipeYield zu standardisieren z.B. in Liter, Gramm, Stück, ... -> Problem: >2000 verschiedene Einheiten (zu viele) -> RecipeYield nicht verwenden\n",
    "-> Stattdessen auf RecipeServings zurückgreifen und fehlende Werte durch randomforest imputieren\n",
    "\n",
    "Wichtig: Da RecipeServings schlecht verteilt ist (z.b. meisten Werte zwischen 0 und 10, aber auch Werte >1000) werden die Modelle ungenau\n",
    "-> Lösung: Verwendung von Klassen: Einteilung in Serving-Size: small, medium, large, ...\n",
    "-> Wichtig: One-hot-encoding nutzen (da es sich um Kategorien handelt) -> 3 Spalten: small, medium, large\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Correlation Matrix von numerischen Werten\n",
    "# corr = recipes_data[[\"CookTime\", \"PrepTime\", \"Calories\", \"FatContent\", \"CarbohydrateContent\", \"FiberContent\", \"SugarContent\", \"ProteinContent\", \"RecipeServings\"]].corr(numeric_only=True)\n",
    "# print(corr)\n",
    "\n",
    "# RecipeYield\n",
    "# recipes_data[\"RecipeYield_Quantity\"] = recipes_data[\"RecipeYield\"].str.split(\" \").str[0]\n",
    "# recipes_data[\"RecipeYield_Unit\"] = recipes_data[\"RecipeYield\"].str.split(\" \").str[1]\n",
    "recipes_data.drop(columns=[\"RecipeYield\"], inplace=True)\n",
    "\n",
    "\n",
    "# RecipeServings\n",
    "# Logarithmieren von RecipeServings, um die Verteilung zu verbessern\n",
    "recipes_data[\"RecipeServings\"] = recipes_data[\"RecipeServings\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(recipes_data['RecipeServings'], bins=10)\n",
    "# plt.title('Histogram of Recipe Servings')\n",
    "# plt.xlabel('Recipe Servings')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# Outlier weg\n",
    "std_RecipeServings = recipes_data[recipes_data['RecipeServings'].notna()][\"RecipeServings\"].std()\n",
    "mean_RecipeServings = recipes_data[recipes_data['RecipeServings'].notna()][\"RecipeServings\"].mean()\n",
    "# print(recipes_data['RecipeServings'].describe())\n",
    "\"\"\"\n",
    "mean         1.770398\n",
    "std          0.781950\n",
    "min          0.000000\n",
    "25%          1.386294 -> 25% der Werte sind 1.386294 oder kleiner -> small\n",
    "50%          1.791759 -> 50% der Werte sind 1.791759 oder kleiner -> medium\n",
    "75%          2.079442 -> 75% der Werte sind 2.079442 oder kleiner -> large\n",
    "max          6.907755\n",
    "\"\"\"\n",
    "\n",
    "upper_limit_RecipeServings = mean_RecipeServings + threshold_RecipeServings * std_RecipeServings\n",
    "lower_limit_RecipeServings = mean_RecipeServings - threshold_RecipeServings * std_RecipeServings\n",
    "\n",
    "recipes_data = recipes_data[((recipes_data[\"RecipeServings\"] >= lower_limit_RecipeServings) & (recipes_data['RecipeServings'] <= upper_limit_RecipeServings)) | (recipes_data['RecipeServings'].isna())]\n",
    "print(\"Shape nach Kürzung der Outlier: \", recipes_data.shape)\n",
    "\n",
    "recipes_data_description = recipes_data.describe()\n",
    "\n",
    "# Conditions\n",
    "conditions = [\n",
    "    (recipes_data[\"RecipeServings\"] <= recipes_data_description.loc[\"25%\", \"RecipeServings\"]),\n",
    "    (recipes_data[\"RecipeServings\"] > recipes_data_description.loc[\"25%\", \"RecipeServings\"]) & (recipes_data[\"RecipeServings\"] <= recipes_data_description.loc[\"75%\", \"RecipeServings\"]),\n",
    "    (recipes_data[\"RecipeServings\"] > recipes_data_description.loc[\"75%\", \"RecipeServings\"])\n",
    "]\n",
    "# choices: small - 0, medium - 1, large - 2\n",
    "choices = [0, 1, 2]\n",
    "\n",
    "# Discretization von RecipeServings -> Klassen: small = 0, medium = 1, large = 2 -> one-hot-encoding\n",
    "recipes_data[\"ServingClass\"] = np.select(conditions, choices, default=np.nan)\n",
    "# Werte mit NA werden durch median 1 ersetzt.\n",
    "recipes_data.loc[recipes_data['RecipeServings'].isna(), \"ServingClass\"] = 1\n",
    "\n",
    "# In Csv\n",
    "# path_recipesData_marco = \"../cvs/Marco/recipes_data.csv\"\n",
    "# recipes_data.to_csv(path_recipesData_marco, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Versuch: Fehlende Daten zu imputieren, nicht geklappt!\n",
    "\"\"\"\n",
    "# fehlende Werte in RecipeServings durch random forest imputieren\n",
    "features = ['CookTime', 'PrepTime', 'Calories', 'FatContent', 'FiberContent', 'ProteinContent']\n",
    "\n",
    "# test_data with and without RecipeServings, training_data only with RecipeServings\n",
    "known_servings = recipes_data[recipes_data['ServingClass'].notna()]\n",
    "unknown_servings = recipes_data[recipes_data['ServingClass'].isna()]\n",
    "\n",
    "X = known_servings[features]\n",
    "y_known = known_servings[\"ServingClass\"]\n",
    "\n",
    "# print(X)\n",
    "# print(y_known)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_known, test_size=0.3, random_state=random_seed)\n",
    "# print('Training Features Shape:', X_train.shape)\n",
    "# print('Training Labels Shape:', y_train.shape)\n",
    "# print('Validation Features Shape:', X_val.shape)\n",
    "# print('Validation Labels Shape:', y_val.shape)\n",
    "\n",
    "\n",
    "# Model trainieren\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=random_seed)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluieren\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Bewertung des Modells\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "# calculate r squared\n",
    "r_squared = model.score(X_val, y_val)\n",
    "print('R Squared: ', r_squared)\n",
    "print('MSE Mean Squred Error: ', mse) \n",
    "\n",
    "\n",
    "# y_val together with y_pred \n",
    "y_val_pred = pd.DataFrame({'y_val': y_val, 'y_pred': y_pred})\n",
    "# Extract RecipeId for the validation set\n",
    "recipe_ids_val = recipes_data.loc[y_val.index, 'RecipeId']\n",
    "# Add RecipeId to the y_val_pred dataframe\n",
    "y_val_pred['RecipeId'] = recipe_ids_val\n",
    "\n",
    "# Display the dataframe\n",
    "print(y_val_pred.head(20))\n",
    "\n",
    "\n",
    "# Model anwenden um fehlende Daten zu analysieren\n",
    "# X_unknown = unknown_servings[features]\n",
    "# y_unknown = unknown_servings[\"RecipeServings\"]\n",
    "# y_unknown_pred = model.predict(X_unknown)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape davor (97381, 43)\n",
      "Shape danach (97281, 43)\n",
      "final_table.csv\n"
     ]
    }
   ],
   "source": [
    "# Verbinden von reviews_df mit diet_df über AuthorId\n",
    "combined_df1 = pd.merge(reviews_data, users_diet_data, on='AuthorId', how='left')\n",
    "\n",
    "# Verbinden von combined_df1 mit requests_df über AuthorId und RecipeId\n",
    "combined_df2 = pd.merge(combined_df1, requests_data, on=['AuthorId', 'RecipeId'], how='left')\n",
    "\n",
    "# Verbinden von combined_df2 mit recipes_df über RecipeId\n",
    "final_combined_df = pd.merge(combined_df2, recipes_data, on='RecipeId', how='left')\n",
    "\n",
    "\n",
    "# Clean data without nan values - z.B. falls Rezept gelöscht wurde usw.\n",
    "print(\"Shape davor\", final_combined_df.shape)\n",
    "final_combined_df = final_combined_df[final_combined_df[\"Time\"].notna()]\n",
    "final_combined_df = final_combined_df[final_combined_df[\"Name\"].notna()]\n",
    "print(\"Shape danach\", final_combined_df.shape)\n",
    "\n",
    "\n",
    "csv_file_path = 'final_table.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "final_combined_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Returning the file path for download\n",
    "print(csv_file_path)\n",
    "\n",
    "\n",
    "# print(final_combined_df[\"ServingClass\"].describe())\n",
    "# plt.hist(final_combined_df[\"ServingClass\"], bins=3)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Daten nochmal weiter verfeinern - Marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 0. Nützliche Funktionen\n",
    "def perfomance_measure(y_true, y_pred):\n",
    "    TP, FP, TN, FN = confusion_matrix(y_true, y_pred).ravel()\n",
    "    recall = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    BAC = (recall + specificity) / 2\n",
    "    return BAC\n",
    "\n",
    "def oneHotEncoding(df, columns):\n",
    "    df = pd.get_dummies(df, columns=columns, prefix=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def addNewColumns(df):\n",
    "\n",
    "    # 1. Falls time - (cooktime + preptime) > 0 oder nur um 1% abweicht, dann in-time, ansonsten nicht in-time\n",
    "    df[\"InTime\"] = df[\"Time\"] - (df[\"CookTime\"] + df[\"PrepTime\"])\n",
    "    df[\"InTime_binary\"] = df.apply(\n",
    "        lambda row: 1 if row[\"Time\"] - (row[\"CookTime\"] + row[\"PrepTime\"]) >= -0.02 * row[\"Time\"] else 0, \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # df[(df[\"InTime_binary\"] == 0) & (df[\"Like\"] == True)]\n",
    "\n",
    "\n",
    "    # 2. Falls (HighCalories == True & True und High-Calories-class == True) dann HighCaloriesMatch = True, (HighCalories == False & True und High-Calories-class == False) dann HighCaloriesMatch = True, ansonsten False\n",
    "\n",
    "    df[\"HighCaloriesMatch\"] = df.apply(\n",
    "        lambda row: 1 if (row[\"HighCalories\"] == True and row[\"High-Calories-class\"] == True) or (row[\"HighCalories\"] == False and row[\"High-Calories-class\"] == False) else 0, \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # df[\"HighCaloriesMatch\"] = df.apply(\n",
    "    #     lambda row: 1 if row[\"HighCalories\"] == True and row[\"High-Calories-class\"] == True else 0, \n",
    "    #     axis=1\n",
    "    # )\n",
    "\n",
    "    # df[df[\"HighCaloriesMatch\"] == 1][['HighCalories', 'High-Calories-class', 'HighCaloriesMatch']]\n",
    "\n",
    "\n",
    "    # 3. Falls (LowFat == True & True und Low-FatContent-class == True) dann LowFatMatch = True, dann true, ansonsten False\n",
    "    df[\"LowFatMatch\"] = df.apply(\n",
    "        lambda row: 1 if (row[\"LowFat\"] == True and row[\"Low-FatContent-class\"] == True) or (row[\"LowFat\"] == False and row[\"Low-FatContent-class\"] == False) else 0, \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # 4. Falls (LowSugar == True & True und Low-SugarContent-class == True) dann LowSugarMatch = True, dann true, ansonsten False\n",
    "    df[\"LowSugarMatch\"] = df.apply(\n",
    "        lambda row: 1 if (row[\"LowSugar\"] == True and row[\"Low-SugarContent-class\"] == True) or (row[\"LowSugar\"] == False and row[\"Low-SugarContent-class\"] == False) else 0, \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # 5. Falls (HighProtein == True & True und High-ProteinContent-class == True) dann HighProteinMatch = True, dann HighProteinMatch = True, ansonsten False\n",
    "    df[\"HighProteinMatch\"] = df.apply(\n",
    "        lambda row: 1 if (row[\"HighProtein\"] == True and row[\"High-ProteinContent-class\"] == True) or (row[\"HighProtein\"] == False and row[\"High-ProteinContent-class\"] == False) else 0, \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # 6. Falls (HighFiber == True & True und High-FiberContent-class == True) dann HighFiberMatch = True, dann HighFiberMatch = True, ansonsten False\n",
    "    df[\"HighFiberMatch\"] = df.apply(\n",
    "        lambda row: 1 if (row[\"HighFiber\"] == True and row[\"High-FiberContent-class\"] == True) or (row[\"HighFiber\"] == False and row[\"High-FiberContent-class\"] == False) else 0, \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # 7. One-hot-encoding für Diet, RecipeCategory, ServingClass\n",
    "    df = oneHotEncoding(df, [\"Diet\", \"RecipeCategory\"])\n",
    "\n",
    "\n",
    "    # 8. Falls (Diet_Vegan == True & Vegan == True)dann VeganMatch = True, ansonsten False\n",
    "    df[\"VeganMatch\"] = df.apply(\n",
    "        lambda row: 1 if row[\"Diet_Vegan\"] == True and row[\"Vegan\"] == True else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # 9. Falls (Diet_Vegetarian == True & Vegetarian == True) or (Diet_Vegetarian == True & Vegan == True) dann VegetarianMatch = True, ansonsten False\n",
    "    df[\"VegetarianMatch\"] = df.apply(\n",
    "        lambda row: 1 if (row[\"Diet_Vegetarian\"] == True and row[\"Vegetarian\"] == True) or (row[\"Diet_Vegetarian\"] == True and row[\"Vegan\"] == True) else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    df.to_csv(\"final_table_Marco.csv\", index=False)\n",
    "\n",
    "\n",
    "    df.columns\n",
    "    # principal_components_final_df.columns\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def logData(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "    return df\n",
    "\n",
    "# Daten normalisieren\n",
    "def dataNormalize(df, normalized_columns):\n",
    "    scaler = StandardScaler()\n",
    "    for column in normalized_columns:\n",
    "        df[column] = scaler.fit_transform(df[[column]])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bereits hier in Testdaten und Trainingsdaten aufteilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_final_combined_df = final_combined_df.copy()\n",
    "clean_final_combined_df = addNewColumns(clean_final_combined_df)\n",
    "\n",
    "# Prozentzahl Test- und Trainingsdaten\n",
    "testSize = 0.1\n",
    "\n",
    "# Aufteilen in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_final_combined_df.drop(\"Like\", axis=1), clean_final_combined_df[\"Like\"].astype(\"int\"), test_size=testSize, random_state=random_seed)\n",
    "\n",
    "columns_die_logarithmiert_werden = [\"Time\", \"CookTime\", \"PrepTime\"]\n",
    "X_train = logData(X_train, columns_die_logarithmiert_werden)\n",
    "\n",
    "# Nach dem Aufteilen in Trainings- und Testdaten wird normalisiert -> davor schlecht, da sonst Testdaten mit in die Normalisierung einfließen -> Data Leakage\n",
    "# Normalisierung hoffentlich besser für Modelle\n",
    "columns_die_normalisiert_werden = [\"Time\", \"CookTime\", \"PrepTime\", \"Calories\", \"FatContent\", \n",
    "                                   \"SaturatedFatContent\", \"CholesterolContent\", \"SodiumContent\", \"CarbohydrateContent\", \n",
    "                                   \"FiberContent\", \"SugarContent\", \"ProteinContent\", \"RecipeServings\", \"InTime\"\n",
    "                                   , \"ServingClass\"]\n",
    "\n",
    "\n",
    "X_train = dataNormalize(X_train, columns_die_normalisiert_werden)\n",
    "X_test = dataNormalize(X_test, columns_die_normalisiert_werden)\n",
    "\n",
    "X_train.to_csv(\"final_table_Marco.csv\", index=False)\n",
    "\n",
    "# histogramme nach der Normalisierung\n",
    "# X_train.hist(bins=50, figsize=(20,15))\n",
    "\n",
    "# clean_final_combined_df.to_csv(\"final_table_Marco.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_columns = [\n",
    "    \"Age\", \n",
    "    \"Calories\", \"FiberContent\", \"ProteinContent\", \n",
    "    # \"SugarContent\", \n",
    "    # \"FatContent\", \"CarbohydrateContent\",\n",
    "    # \"SaturatedFatContent\", \"CholesterolContent\", \"SodiumContent\", \n",
    "    \"InTime\", \n",
    "    \"HighCaloriesMatch\", \"HighProteinMatch\", \"HighFiberMatch\", \n",
    "    # \"LowFatMatch\", \n",
    "    # \"LowSugarMatch\", \n",
    "    # \"RecipeCategory_Beverage\", \"RecipeCategory_Meals\", \n",
    "    # \"RecipeCategory_Other\", \n",
    "    # \"ServingClass\",\n",
    "    # \"VeganMatch\", \n",
    "    # \"VegetarianMatch\",\n",
    "    \n",
    "    # nicht ganz sicher, ob die hier nötig sind, eigentlich abhängig von adernen Spalten\n",
    "    \"Time\", \n",
    "    \"CookTime\", \"PrepTime\", \n",
    "    # \"LowFat\", \"LowSugar\",\n",
    "    # \"HighCalories\", \"HighProtein\", \"HighFiber\",\n",
    "    # \"InTime_binary\",\n",
    "    # \"Diet_Omnivore\", \"Diet_Vegan\", \"Diet_Vegetarian\", \n",
    "    # \"Meat\", \"Seafood\", \"Vegetarian\", \n",
    "    # \"Vegan\", \n",
    "]\n",
    "\n",
    "X_train_clean = X_train[necessary_columns]\n",
    "X_test_clean = X_test[necessary_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Modelle Marco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "BAC train:  0.6908756130224143\n",
      "Test Set Evaluation:\n",
      "BAC:  0.6845786883100315\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "\n",
    "\n",
    "# Model 1: Linear Regression\n",
    "# Convariance Matrix\n",
    "X_train_clean.corr().to_csv(\"covariance_matrix.csv\")\n",
    "\n",
    "\n",
    "X_train_clean = X_train_clean.astype(float)\n",
    "X_train_clean = sm.add_constant(X_train_clean)\n",
    "X_test_clean = X_test_clean.astype(float)\n",
    "X_test_clean = sm.add_constant(X_test_clean)\n",
    "\n",
    "# print(X_train_clean.columns)\n",
    "\n",
    "\n",
    "# for index, variable_name in enumerate(X_train_clean.columns):\n",
    "#     if variable_name == \"const\": \n",
    "#         continue\n",
    "#     print(f\"VIF for variable {variable_name} is {vif(X_train_clean, index)}\")\n",
    "\n",
    "# Gewichtung der Daten - 8 mal so viel Gewicht auf Like = True\n",
    "weigths = y_train.apply(lambda x: 9 if x == 1 else 1)\n",
    "\n",
    "model_OLS = sm.WLS(y_train, X_train_clean, weights=weigths).fit()\n",
    "# print(model_OMS.summary())\n",
    "\n",
    "# Prediction\n",
    "# Training set\n",
    "y_pred_train_OLS = model_OLS.predict(X_train_clean)\n",
    "y_pred_train_OLS_binary = (y_pred_train_OLS > 0.5).astype(int)\n",
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train, y_pred_train_OLS_binary).ravel()\n",
    "recall_train, specificity_train, BAC_train = perfomance_measure(tp_train, fp_train, tn_train, fn_train)\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(\"BAC train: \", BAC_train)\n",
    "# r_squeared_train = r2_score(y_train, y_pred_train_OLS)\n",
    "# print(\"R Squared train: \", r_squeared_train)\n",
    "\n",
    "# Validation set\n",
    "y_pred_OLS = model_OLS.predict(X_test_clean)\n",
    "y_pred_OLS_binary = (y_pred_OLS > 0.5).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_OLS_binary).ravel()\n",
    "recall, specificity, BAC = perfomance_measure(tp, fp, tn, fn)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"BAC: \", BAC)\n",
    "# r_squeared = r2_score(y_test, y_pred_OLS)\n",
    "# print(\"R Squared: \", r_squeared)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, counts of label '1': 75972\n",
      "After OverSampling, counts of label '0': 75972\n",
      "y_pred:  [0 0 0 ... 1 0 1]\n",
      "Training Set Evaluation:\n",
      "BAC train:  0.7002316642973727\n",
      "Test Set Evaluation:\n",
      "BAC:  0.6889988121331405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "smote_LogReg = SMOTE(random_state=random_seed, sampling_strategy = 1)\n",
    "X_train_resampled, y_train_resampled = smote_LogReg.fit_resample(X_train_clean, y_train)\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_resampled == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_resampled == 0)))\n",
    "\n",
    "\n",
    "model_LogReg = LogisticRegression(random_state=random_seed, max_iter=1000)\n",
    "model_LogReg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Prediction\n",
    "y_pred_LogReg = model_LogReg.predict(X_test_clean)\n",
    "print(\"y_pred: \", y_pred_LogReg)\n",
    "\n",
    "# Bewertung\n",
    "tn_train_LogReg, fp_train_LogReg, fn_train_LogReg, tp_train_LogReg = confusion_matrix(y_train_resampled, model_LogReg.predict(X_train_resampled)).ravel()\n",
    "recall_train_LogReg, specificity_train, BAC_train_LogReg = perfomance_measure(tp_train_LogReg, fp_train_LogReg, tn_train_LogReg, fn_train_LogReg)\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(\"BAC train: \", BAC_train_LogReg)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_LogReg).ravel()\n",
    "recall, specificity, BAC = perfomance_measure(tp, fp, tn, fn)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"BAC: \", BAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "BAC:  0.937834106029238\n",
      "Test Set Evaluation:\n",
      "BAC:  0.9335271766943793\n",
      "[0.92368464 0.9381871  0.95014273 0.93880881 0.94789656 0.95174777\n",
      " 0.93818662 0.91864899 0.92075938 0.93521461]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "smote_NB = SMOTE(random_state=random_seed, sampling_strategy = 0.2)\n",
    "X_train_resampled_NB, y_train_resampled_NB = smote_NB.fit_resample(X_train_clean, y_train)\n",
    "\n",
    "model_NB = GaussianNB()\n",
    "# model_NB.fit(X_train_resampled_NB, y_train_resampled_NB)\n",
    "\n",
    "bagging_nb = BaggingClassifier(\n",
    "    estimator=model_NB,\n",
    "    n_estimators=60,\n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "bagging_nb.fit(X_train_resampled_NB, y_train_resampled_NB)\n",
    "\n",
    "# Prediction - Bias vs Variance\n",
    "# Training Data \n",
    "y_train_pred_NB = bagging_nb.predict(X_train_resampled_NB)\n",
    "BAC_train = perfomance_measure(y_train_resampled_NB, y_train_pred_NB)\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(\"BAC: \", BAC_train)\n",
    "\n",
    "# Test Data\n",
    "y_pred_NB = bagging_nb.predict(X_test_clean)\n",
    "BAC = perfomance_measure(y_test, y_pred_NB)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"BAC: \", BAC)\n",
    "\n",
    "k = 10  # Anzahl folgs\n",
    "cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
    "\n",
    "scores = cross_val_score(bagging_nb, X_train_resampled_NB, y_train_resampled_NB, \n",
    "                         scoring=make_scorer(perfomance_measure), cv=cv)\n",
    "\n",
    "print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "BAC:  0.9478886958353077\n",
      "Test Set Evaluation:\n",
      "BAC:  0.6003122085211637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "smote_RF = SMOTE(random_state=random_seed, sampling_strategy = 1)\n",
    "X_train_resampled_RF, y_train_resampled_RF = smote_RF.fit_resample(X_train_clean, y_train)\n",
    "\n",
    "model_RF = RandomForestClassifier(random_state=random_seed, \n",
    "                                  n_estimators=100,\n",
    "                                  max_features=\"sqrt\",   # Einsatz der Qaudratwurzel der Feature-Anzahl = default\n",
    "                                  max_depth=17  \n",
    "                                  )\n",
    "model_RF.fit(X_train_resampled_RF, y_train_resampled_RF)\n",
    "# model_RF.fit(X_train_clean, y_train)\n",
    "\n",
    "# Prediction - Bias vs Variance Tradeoff\n",
    "# Training Data\n",
    "y_train_pred_RF = model_RF.predict(X_train_resampled_RF)\n",
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train_resampled_RF, y_train_pred_RF).ravel()\n",
    "recall_train, specificity_train, BAC_train = perfomance_measure(tp_train, fp_train, tn_train, fn_train)\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(\"BAC: \", BAC_train)\n",
    "\n",
    "# Validation Data\n",
    "y_pred_RF = model_RF.predict(X_test_clean)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_RF).ravel()\n",
    "recall, specificity, BAC = perfomance_measure(tp, fp, tn, fn)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"BAC: \", BAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stacking - Kombination der besten Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Evaluation:\n",
      "BAC:  0.9709197258786894\n",
      "Test Set Evaluation:\n",
      "BAC:  0.7697667175279116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "\n",
    "smote_Stacking = SMOTE(random_state=random_seed, sampling_strategy = 0.9)\n",
    "X_train_resampled_Stacking, y_train_resampled_Stacking = smote_Stacking.fit_resample(X_train_clean, y_train)\n",
    "\n",
    "# Base Models\n",
    "# model_LogReg = LogisticRegression(random_state=random_seed, max_iter=1000)\n",
    "model_NB = GaussianNB()\n",
    "model_RF = RandomForestClassifier(random_state=random_seed, \n",
    "                                  n_estimators=100,\n",
    "                                  max_features=\"sqrt\",   # Einsatz der Qaudratwurzel der Feature-Anzahl = default\n",
    "                                  max_depth=17   \n",
    "                                  )\n",
    "model_Ridge = RidgeClassifier(random_state=random_seed)\n",
    "\n",
    "base_models = [\n",
    "    # (\"LogReg\", model_LogReg),\n",
    "    (\"NB\", model_NB),\n",
    "    (\"RF\", model_RF),\n",
    "    # (\"Ridge\", model_Ridge)\n",
    "]\n",
    "\n",
    "# Meta Model\n",
    "meta_model = LogisticRegression(random_state=random_seed, max_iter=1000)\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model\n",
    "    # , cv=5 # Cross Validation\n",
    ")\n",
    "\n",
    "stacking.fit(X_train_resampled_Stacking, y_train_resampled_Stacking)\n",
    "\n",
    "# Prediction - Bias vs Variance Tradeoff\n",
    "# Training Data\n",
    "y_train_pred_Stacking = stacking.predict(X_train_resampled_Stacking)\n",
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train_resampled_Stacking, y_train_pred_Stacking).ravel()\n",
    "recall_train, specificity_train, BAC_train = perfomance_measure(tp_train, fp_train, tn_train, fn_train)\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(\"BAC: \", BAC_train)\n",
    "\n",
    "# Validation Data\n",
    "y_pred_Stacking = stacking.predict(X_test_clean)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_Stacking).ravel()\n",
    "recall, specificity, BAC = perfomance_measure(tp, fp, tn, fn)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"BAC: \", BAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. PCA nutzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac-cup-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
