{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "\n",
    "random_seed = 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data-Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. diet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>Diet</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>271906</td>\n",
       "      <td>271906</td>\n",
       "      <td>271906.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>271906</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>10000120E</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>143383</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.503674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.898141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AuthorId        Diet            Age\n",
       "count      271906      271906  271906.000000\n",
       "unique     271906           3            NaN\n",
       "top     10000120E  Vegetarian            NaN\n",
       "freq            1      143383            NaN\n",
       "mean          NaN         NaN      48.503674\n",
       "std           NaN         NaN      17.898141\n",
       "min           NaN         NaN      18.000000\n",
       "25%           NaN         NaN      33.000000\n",
       "50%           NaN         NaN      48.000000\n",
       "75%           NaN         NaN      64.000000\n",
       "max           NaN         NaN      79.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_diet = pd.read_csv('../aufgabe/training_dataset/diet.csv')\n",
    "# print(users_diet.head())\n",
    "# users_diet.shape\n",
    "# users_diet.dtypes\n",
    "\n",
    "# Nullwerte\n",
    "# users_diet.isnull().sum()\n",
    "\n",
    "# 1. Spalte \"Diet\"\n",
    "# -> ein Nullwert drin \n",
    "#users_diet[users_diet[\"Diet\"].isna()]\n",
    "# Author 646062A ohne Wert für Diet -> Weg mit dem Hund\n",
    "users_diet = users_diet.drop(users_diet[users_diet[\"Diet\"].isna()].index).reset_index(drop=True)\n",
    "\n",
    "# 2.  Weitere missing values finden: z.B. <empty field>, \"0\", \".\", \"999\", \"NA\" ...\n",
    "# users_diet[(users_diet[\"Diet\"] == \"\") | (users_diet[\"Diet\"] == \".\") | (users_diet[\"Diet\"] == \"999\")]\n",
    "#keine Nullwerte mehr\n",
    "\n",
    "\n",
    "# 3. Spalte \"Age\"\n",
    "#print(users_diet[users_diet[\"Age\"] > 100])\n",
    "#print (users_diet[users_diet[\"Age\"] < 5])\n",
    "# print(\"Range Alter\", users_diet[\"Age\"].min(), users_diet[\"Age\"].max())\n",
    "# -> keine werte < 5 oder > 100\n",
    "\n",
    "# 3. Spalte \"AuthorId\"\n",
    "# print(\"Einzigartige IDs: \", users_diet[\"AuthorId\"].nunique(), users_diet.shape[0])\n",
    "#duplicate_authorid_counts = users_diet['AuthorId'].duplicated().sum()\n",
    "#print(duplicate_authorid_counts)\n",
    "# -> keine Duplikate\n",
    "\n",
    "\n",
    "# Datentyp bei \"Diet\" zu Category ändern\n",
    "users_diet[\"Diet\"] = users_diet['Diet'].astype(\"category\")\n",
    "users_diet.dtypes\n",
    "users_diet.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AuthorId  RecipeId  Rating   Like\n",
      "0  1000036C    320576   False  False\n",
      "1  1000216B    189335   False  False\n",
      "2  1000221A    133043    True  False\n",
      "3  1000221A     90537    True  False\n",
      "4  1000221A    334314    True  False\n",
      "      AuthorId  RecipeId  Rating  TestSetId\n",
      "0     2492191A     33671    True        1.0\n",
      "1  2002019979A     92647    True        2.0\n",
      "2      408594E    161770   False        3.0\n",
      "3  2001625557E    108231    True        4.0\n",
      "4  2001427116E     71109   False        5.0\n",
      "      AuthorId  RecipeId  Rating Like  TestSetId\n",
      "0     2492191A     33671    True  NaN        1.0\n",
      "1  2002019979A     92647    True  NaN        2.0\n",
      "2      408594E    161770   False  NaN        3.0\n",
      "3  2001625557E    108231    True  NaN        4.0\n",
      "4  2001427116E     71109   False  NaN        5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/z37d__mj75g9k_jcd30hbw0c0000gn/T/ipykernel_22685/2939460380.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews_data = pd.read_csv(\"../aufgabe/training_dataset/reviews.csv\")\n"
     ]
    }
   ],
   "source": [
    "reviews_data = pd.read_csv(\"../aufgabe/training_dataset/reviews.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# 1. Adjusting the 'Rating' column\n",
    "# Set 'Rating' to True if it's 2, and False otherwise (including NaN)\n",
    "reviews_data['Rating'] = reviews_data['Rating'] == 2\n",
    "reviews_data[\"Rating\"] = reviews_data[\"Rating\"].astype(\"bool\")\n",
    "\n",
    "\n",
    "# 2. check for duplicates (gleiche AuthorId und RecipeId)\n",
    "duplicate_entries_counts = reviews_data.duplicated(subset=['AuthorId', 'RecipeId']).sum()\n",
    "duplicate_entries_counts\n",
    "# -> keine Duplikate\n",
    "\n",
    "# 3.  Test-Daten -> TestSetId != NaN, die anderen nicht für Modelle verwenden\n",
    "reviews_data_test = reviews_data[reviews_data[\"TestSetId\"].isna()].drop('TestSetId', axis=1).reset_index(drop=True)\n",
    "reviews_data_test[\"Like\"] = reviews_data_test[\"Like\"].astype(\"bool\")\n",
    "\n",
    "# 4. Trainings-Daten -> TestSetId == NaN\n",
    "reviews_data_training = reviews_data[reviews_data[\"TestSetId\"].notna()].drop('Like', axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#Output: \n",
    "# rating ist true bei 2, false bei NA\n",
    "# reviews_data_test: 10000 rows × 4 columns, 0 missing values, AuthorId, RecipeId, Rating, Like (TestSetId nicht mehr vorhanden)\n",
    "# reviews_data_training: 40000 rows × 4 columns, 0 missing values, AuthorId, RecipeId, Rating, TestSetId  (like nicht mehr vorhanden)\n",
    "# reviews_data: 50000 rows × 5 columns, 0 missing values, AuthorId, RecipeId, Rating, Like, TestSetId (alles vorhanden, und clean)\n",
    "\n",
    "print(reviews_data_test.head())\n",
    "print(reviews_data_training.head())\n",
    "print(reviews_data.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. requests.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132993 94.8628695745212\n",
      "Number of duplicate entries: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>Time</th>\n",
       "      <th>HighCalories</th>\n",
       "      <th>HighProtein</th>\n",
       "      <th>LowFat</th>\n",
       "      <th>LowSugar</th>\n",
       "      <th>HighFiber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>132993</td>\n",
       "      <td>132993.000000</td>\n",
       "      <td>132993.000000</td>\n",
       "      <td>132993</td>\n",
       "      <td>132993</td>\n",
       "      <td>132993</td>\n",
       "      <td>132993</td>\n",
       "      <td>132993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>47423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1930181E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79416</td>\n",
       "      <td>79967</td>\n",
       "      <td>93160</td>\n",
       "      <td>93108</td>\n",
       "      <td>79713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>153198.593730</td>\n",
       "      <td>2996.475371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>130559.843136</td>\n",
       "      <td>2729.684319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>47111.000000</td>\n",
       "      <td>1200.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>109817.000000</td>\n",
       "      <td>2397.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>233056.000000</td>\n",
       "      <td>3602.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>541195.000000</td>\n",
       "      <td>16019.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AuthorId       RecipeId           Time HighCalories HighProtein  \\\n",
       "count     132993  132993.000000  132993.000000       132993      132993   \n",
       "unique     47423            NaN            NaN            2           2   \n",
       "top     1930181E            NaN            NaN        False       False   \n",
       "freq         816            NaN            NaN        79416       79967   \n",
       "mean         NaN  153198.593730    2996.475371          NaN         NaN   \n",
       "std          NaN  130559.843136    2729.684319          NaN         NaN   \n",
       "min          NaN      40.000000       0.000000          NaN         NaN   \n",
       "25%          NaN   47111.000000    1200.600000          NaN         NaN   \n",
       "50%          NaN  109817.000000    2397.600000          NaN         NaN   \n",
       "75%          NaN  233056.000000    3602.000000          NaN         NaN   \n",
       "max          NaN  541195.000000   16019.200000          NaN         NaN   \n",
       "\n",
       "        LowFat LowSugar HighFiber  \n",
       "count   132993   132993    132993  \n",
       "unique       2        2         2  \n",
       "top      False    False     False  \n",
       "freq     93160    93108     79713  \n",
       "mean       NaN      NaN       NaN  \n",
       "std        NaN      NaN       NaN  \n",
       "min        NaN      NaN       NaN  \n",
       "25%        NaN      NaN       NaN  \n",
       "50%        NaN      NaN       NaN  \n",
       "75%        NaN      NaN       NaN  \n",
       "max        NaN      NaN       NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests_data = pd.read_csv(\"../aufgabe/training_dataset/requests.csv\")\n",
    "requests_data.head()\n",
    "\n",
    "# 1. Column: Time\n",
    "# Runden\n",
    "\"\"\" \n",
    "Column Time -> runden, da Nachkommastellen bei Kochzeit irrelevant\n",
    "Beschreibung: The duration a recipe should take at most (including the time reserved\n",
    "for the preparation and cooking).\n",
    "\"\"\"\n",
    "requests_data[\"Time\"] = requests_data[\"Time\"].round(1)\n",
    "\n",
    "#remove outliers    \n",
    "# Calculating IQR and determining the thresholds for outliers\n",
    "Q1 = np.quantile(requests_data[\"Time\"], 0.25)\n",
    "Q3 = np.quantile(requests_data[\"Time\"], 0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 4 * IQR\n",
    "upper_bound = Q3 + 4 * IQR\n",
    "\n",
    "# Removing outliers\n",
    "time_data_filtered = requests_data[\"Time\"][(requests_data[\"Time\"] >= lower_bound) & (requests_data[\"Time\"] <= upper_bound)]\n",
    "\n",
    "# Percentage of data retained after removing outliers\n",
    "percentage_retained = len(time_data_filtered) / len(requests_data[\"Time\"]) * 100\n",
    "\n",
    "\n",
    "print(len(time_data_filtered), percentage_retained)\n",
    "requests_data[\"Time\"] = time_data_filtered\n",
    "requests_data = requests_data.dropna(subset=[\"Time\"])\n",
    "\n",
    "\n",
    "\n",
    "# Visualization of the 'Time' column before and after removing outliers\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Plotting the original 'Time' data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(requests_data[\"Time\"])\n",
    "plt.title('Original Time Data (with Outliers)')\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Plotting the 'Time' data after removing outliers\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(time_data_filtered)\n",
    "plt.title('Time Data After Removing Outliers')\n",
    "plt.xlabel('Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Creating histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(requests_data[\"Time\"], bins=50, kde=True)\n",
    "plt.title('Histogram of Original Time Data')\n",
    "plt.xlabel('Time')\n",
    "plt.xlim(left=0)  # Adjust the x-axis as needed\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(time_data_filtered, bins=50, kde=True)\n",
    "plt.title('Histogram of Time Data (Without Outliers)')\n",
    "plt.xlabel('Time')\n",
    "plt.xlim(left=0)  # Adjust the x-axis as needed\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\"\"\n",
    "\n",
    "# 2.Teilweise negative Werte -> 0\n",
    "requests_data.loc[requests_data[\"Time\"] <= 0, \"Time\"] = 0\n",
    "\"\"\"\n",
    "Test, ob negative Werte immer False als \"Like\" haben -> stimmt aber nicht siehe Code:\n",
    "Requests mit [AuthorId, RecipeId] time <= 0 mit reviews [AuthorId, RecipeId] joinen und dort like checken\n",
    "joined_data = requests_data.merge(reviews_data_test, on=[\"AuthorId\", \"RecipeId\"], how=\"left\")\n",
    "joined_data = joined_data[joined_data[\"Time\"] <= 0]\n",
    "joined_data = joined_data[~joined_data[\"Like\"].isna()]\n",
    "\"\"\"\n",
    "\n",
    "# 3. Column: HighCalories, HighProtein, LowFat, LowSugar, HighFiber\n",
    "unique_values = {\n",
    "        \"HighCalories\": requests_data[\"HighCalories\"].unique(),\n",
    "        \"HighProtein\": requests_data[\"HighProtein\"].unique(),\n",
    "        \"LowFat\": requests_data[\"LowFat\"].unique(),\n",
    "        \"LowSugar\": requests_data[\"LowSugar\"].unique(),\n",
    "        \"HighFiber\": requests_data[\"HighFiber\"].unique()\n",
    "    }\n",
    "\n",
    "# 4. Column: HighCalories\n",
    "requests_data[\"HighCalories\"] = requests_data[\"HighCalories\"].astype(\"bool\")\n",
    "\n",
    "# 5. Column: HighProtein\n",
    "\"\"\"\n",
    "2 Werte: Indifferent und Yes\n",
    "Daraus wird boolean indifferent = False und Yes = True\n",
    "\"\"\"\n",
    "requests_data.loc[requests_data[\"HighProtein\"] == \"Indifferent\", \"HighProtein\"] = 0\n",
    "requests_data.loc[requests_data[\"HighProtein\"] == \"Yes\", \"HighProtein\"] = 1\n",
    "requests_data[\"HighProtein\"] = requests_data[\"HighProtein\"].astype(\"bool\")\n",
    "\n",
    "# 6. Column: LowFat\n",
    "requests_data[\"LowFat\"] = requests_data[\"LowFat\"].astype(\"bool\")\n",
    "\n",
    "# 7. Column: LowSugar\n",
    "\"\"\"\n",
    "2 Werte: Indifferent und 0. Interpretation: 0 -> user braucht kein low-sugar Inhalt, Indifferent -> User ist es egal\n",
    "Daraus wird boolean 0 = False und indifferent = True\n",
    "\"\"\"\n",
    "requests_data.loc[requests_data[\"LowSugar\"] == \"0\", \"LowSugar\"] = 0\n",
    "requests_data.loc[requests_data[\"LowSugar\"] == \"Indifferent\", \"LowSugar\"] = 1\n",
    "requests_data[\"LowSugar\"] = requests_data[\"LowSugar\"].astype(\"bool\")\n",
    "\n",
    "# 8. Column HighFiber\n",
    "requests_data[\"HighFiber\"] = requests_data[\"HighFiber\"].astype(\"bool\")\n",
    "\n",
    "# 9. check for duplicates (gleiche AuthorId und RecipeId)\n",
    "duplicate_entries_counts = requests_data.duplicated().sum()\n",
    "print(\"Number of duplicate entries:\", duplicate_entries_counts)\n",
    "\n",
    "\n",
    "# requests_data[\"HighFiber\"].unique()\n",
    "requests_data.head(20)\n",
    "requests_data.describe(include=\"all\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Output: alle columns sind sauber, keine missing values, keine duplicates, alle datentypen passen\n",
    "# alle werte auf boolean geändert, außer Time (float) und Ids (int)\n",
    "# requests_data: 50000 rows × 9 columns, 0 missing values, AuthorId, RecipeId, Time, HighCalories, HighProtein, LowFat, LowSugar, HighFiber\n",
    "# time < 0 -> 0, time > 0 -> gerundet auf 1 Nachkommastelle, time outliers entfernt (4*IQR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. recipes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape vorher:  (73253, 18)\n",
      "    RecipeId  Meat  Seafood  Vegetarian  Vegan\n",
      "0      73440     0        0           1      0\n",
      "1     365718     1        0           0      0\n",
      "2     141757     1        0           0      0\n",
      "3     280351     1        0           0      0\n",
      "4     180505     0        0           1      0\n",
      "5     350271     0        0           1      0\n",
      "6      21518     0        0           1      0\n",
      "7     137143     1        0           0      0\n",
      "8     211563     0        0           1      0\n",
      "9      29280     0        0           1      0\n",
      "10     57685     0        0           1      0\n",
      "11     13148     1        0           0      0\n",
      "12     87782     0        0           1      0\n",
      "13    117043     0        0           1      0\n",
      "14     48153     1        0           0      0\n",
      "15     83407     0        0           1      0\n",
      "16    375021     1        0           0      0\n",
      "17    139664     1        0           0      0\n",
      "18    157936     0        0           1      0\n",
      "19    461793     1        0           0      0\n",
      "Shape nach Kürzung der Outlier:  (72073, 21)\n",
      "0        2.0\n",
      "1        1.0\n",
      "2        1.0\n",
      "3        1.0\n",
      "4        1.0\n",
      "        ... \n",
      "75599    NaN\n",
      "75600    NaN\n",
      "75601    0.0\n",
      "75602    NaN\n",
      "75603    NaN\n",
      "Name: ServingClass, Length: 72073, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# fehlende Werte in RecipeServings durch random forest imputieren\\nfeatures = [\\'CookTime\\', \\'PrepTime\\', \\'Calories\\', \\'FatContent\\', \\'FiberContent\\', \\'ProteinContent\\']\\n\\n# test_data with and without RecipeServings, training_data only with RecipeServings\\nknown_servings = recipes_data[recipes_data[\\'ServingClass\\'].notna()]\\nunknown_servings = recipes_data[recipes_data[\\'ServingClass\\'].isna()]\\n\\nX = known_servings[features]\\ny_known = known_servings[\"ServingClass\"]\\n\\n# print(X)\\n# print(y_known)\\nX_train, X_val, y_train, y_val = train_test_split(X, y_known, test_size=0.3, random_state=random_seed)\\n# print(\\'Training Features Shape:\\', X_train.shape)\\n# print(\\'Training Labels Shape:\\', y_train.shape)\\n# print(\\'Validation Features Shape:\\', X_val.shape)\\n# print(\\'Validation Labels Shape:\\', y_val.shape)\\n\\n\\n# Model trainieren\\nmodel = RandomForestClassifier(n_estimators=200, random_state=random_seed)\\nmodel.fit(X_train, y_train)\\n\\n# Model evaluieren\\ny_pred = model.predict(X_val)\\n\\n# Bewertung des Modells\\nmse = mean_squared_error(y_val, y_pred)\\n# calculate r squared\\nr_squared = model.score(X_val, y_val)\\nprint(\\'R Squared: \\', r_squared)\\nprint(\\'MSE Mean Squred Error: \\', mse) \\n\\n\\n# y_val together with y_pred \\ny_val_pred = pd.DataFrame({\\'y_val\\': y_val, \\'y_pred\\': y_pred})\\n# Extract RecipeId for the validation set\\nrecipe_ids_val = recipes_data.loc[y_val.index, \\'RecipeId\\']\\n# Add RecipeId to the y_val_pred dataframe\\ny_val_pred[\\'RecipeId\\'] = recipe_ids_val\\n\\n# Display the dataframe\\nprint(y_val_pred.head(20))\\n\\n\\n# Model anwenden um fehlende Daten zu analysieren\\n# X_unknown = unknown_servings[features]\\n# y_unknown = unknown_servings[\"RecipeServings\"]\\n# y_unknown_pred = model.predict(X_unknown)\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_data = pd.read_csv(\"../aufgabe/training_dataset/recipes.csv\")\n",
    "threshold = 3.5\n",
    "\n",
    "# Column CookTime und Column PrepTime\n",
    "\"\"\"\n",
    "Outlier Detection:\n",
    "Außerhalb von 3,5*Standardabweichung -> Outlier\n",
    "\n",
    "=> Outlier werden entfernt\n",
    "\"\"\"\n",
    "# log transformation\n",
    "recipes_data[\"CookTime\"] = recipes_data[\"CookTime\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "recipes_data[\"PrepTime\"] = recipes_data[\"PrepTime\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "# print(recipes_data[\"PrepTime\"].describe())\n",
    "\n",
    "# plt.hist(recipes_data['PrepTime'], bins=10)\n",
    "# plt.title('Histogram of Cook Time')\n",
    "# plt.xlabel('Cook Time')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "std_CookTime = recipes_data['CookTime'].std()\n",
    "mean_CookTime = recipes_data['CookTime'].mean()\n",
    "upper_limit_CookTime = mean_CookTime + threshold * std_CookTime\n",
    "lower_limit_CookTime = mean_CookTime - threshold * std_CookTime\n",
    "\n",
    "\n",
    "std_PrepTime = recipes_data['PrepTime'].std()\n",
    "mean_PrepTime = recipes_data['PrepTime'].mean()\n",
    "upper_limit_PrepTime = mean_PrepTime + threshold * std_PrepTime\n",
    "lower_limit_PrepTime = mean_PrepTime - threshold * std_PrepTime\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"CookTime\"] >= lower_limit_CookTime) & (recipes_data['CookTime'] <= upper_limit_CookTime)]\n",
    "recipes_data = recipes_data[(recipes_data[\"PrepTime\"] >= lower_limit_PrepTime) & (recipes_data['PrepTime'] <= upper_limit_PrepTime)]\n",
    "print(\"Shape vorher: \", recipes_data.shape)\n",
    "\n",
    "\n",
    "# Column RecipeCategory\n",
    "recipes_data[\"RecipeCategory\"] = recipes_data[\"RecipeCategory\"].astype(\"category\")\n",
    "\n",
    "\n",
    "# Column RecipeIngredientQuantities und Column RecipeIngredientParts\n",
    "# zu liste von strings umwandeln\n",
    "recipes_data[\"RecipeIngredientQuantities\"] = recipes_data[\"RecipeIngredientQuantities\"].str.replace('character(0)', '').str.lstrip('\"c(\"').str.replace('\"', '').str.replace(\")\", \"\").str.replace('\\\\', '').str.split(\",\")\n",
    "recipes_data[\"RecipeIngredientParts\"] = recipes_data[\"RecipeIngredientParts\"].str.lower().replace('character(0)', '').str.lstrip('\"c(\"').str.replace('\"', '').str.replace(\")\", \"\").str.replace('\\\\', '').str.split(\",\")\n",
    "\n",
    "# Einteilung ob die Zutat in der Liste ist oder nicht\n",
    "# Fleisch\n",
    "value_meat = [\"chicken\", \"veal\", \"pork\", \"beef\", \"turkey\", \"ham\", \"bacon\", \"lamb\", \"duck\", \"goose\", \"rabbit\", \"venison\", \"quail\", \"pheasant\", \"alligator\"]\n",
    "# Meeresfrüchte\n",
    "value_sea = [\"fish\", \"crab\", \"lobster\", \"shrimp\", \"prawn\", \"clam\", \"mussel\", \"scallop\", \"squid\", \"octopus\", \"anchovy\", \"sardine\", \"tuna\", \"salmon\", \"trout\", \"herring\", \"cod\", \"mackerel\", \"bass\", \"swordfish\", \"sturgeon\", \"walleye\", \"caviar\", \"crayfish\", \"cuttlefish\", \"sea cucumber\", \"sea snail\", \"sea bass\", \"sea bream\", \"sea trout\", \"seafood\", \"shellfish\"]\n",
    "# vegetarisch\n",
    "value_vegetarian = [\"tofu\", \"seitan\", \"tempeh\", \"plant-based\"]\n",
    "# vegan\n",
    "value_vegan = [\"vegan\"]\n",
    "\n",
    "def beinhaltet_substring(ingredient_list, category_list):\n",
    "    for ingredient in ingredient_list:\n",
    "        if any(cat in ingredient for cat in category_list):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "recipes_data[\"Meat\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_meat))\n",
    "recipes_data[\"Seafood\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_sea))\n",
    "recipes_data[\"Vegetarian\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_vegetarian))\n",
    "recipes_data[\"Vegan\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_vegan))\n",
    "\n",
    "# weitere Unterteilung, falls mehrere Kategorien anschlagen und falls alles 0, dann vegetarisch 1\n",
    "for index, row in recipes_data.iterrows():\n",
    "    if(row[\"Meat\"] == 1 or row[\"Seafood\"] == 1):\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 0\n",
    "        recipes_data.loc[index, [\"Vegan\"]] = 0\n",
    "    elif(row[\"Vegetarian\"] == 0 and row[\"Vegan\"] == 0):\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 1\n",
    "\n",
    "\n",
    "print(recipes_data[[\"RecipeId\", \"Meat\", \"Seafood\", \"Vegetarian\", \"Vegan\"]].head(20))\n",
    "\n",
    "\n",
    "# Column Calories\n",
    "# Outlier weg\n",
    "recipes_data[\"Calories\"] = recipes_data[\"Calories\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "\n",
    "std_Calories = recipes_data['Calories'].std()\n",
    "mean_Calories = recipes_data['Calories'].mean()\n",
    "upper_limit_Calories = mean_Calories + threshold * std_Calories\n",
    "lower_limit_Calories = mean_Calories - threshold * std_Calories\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"Calories\"] >= lower_limit_Calories) & (recipes_data['Calories'] <= upper_limit_Calories)]\n",
    "\n",
    "\n",
    "# Column FatContent\n",
    "# Oulier weg\n",
    "\n",
    "recipes_data[\"FatContent\"] = recipes_data[\"FatContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_FatContent = recipes_data['FatContent'].std()\n",
    "mean_FatContent = recipes_data['FatContent'].mean()\n",
    "upper_limit_FatContent = mean_FatContent + threshold * std_FatContent\n",
    "lower_limit_FatContent = mean_FatContent - threshold * std_FatContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"FatContent\"] >= lower_limit_FatContent) & (recipes_data['FatContent'] <= upper_limit_FatContent)]\n",
    "\n",
    "\n",
    "# Column SaturatedFatContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SaturatedFatContent\"] = recipes_data[\"SaturatedFatContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_SaturatedFatContent = recipes_data['SaturatedFatContent'].std()\n",
    "mean_SaturatedFatContent = recipes_data['SaturatedFatContent'].mean()\n",
    "upper_limit_SaturatedFatContent = mean_SaturatedFatContent + threshold * std_SaturatedFatContent\n",
    "lower_limit_SaturatedFatContent = mean_SaturatedFatContent - threshold * std_SaturatedFatContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"SaturatedFatContent\"] >= lower_limit_SaturatedFatContent) & (recipes_data['SaturatedFatContent'] <= upper_limit_SaturatedFatContent)]\n",
    "\n",
    "\n",
    "# Column CholesterolContent\n",
    "# Outlier weg\n",
    "recipes_data[\"CholesterolContent\"] = recipes_data[\"CholesterolContent\"].apply(lambda x: np.log(x + 1) if x > 0 else x)\n",
    "\n",
    "std_CholesterolContent = recipes_data['CholesterolContent'].std()\n",
    "mean_CholesterolContent = recipes_data['CholesterolContent'].mean()\n",
    "upper_limit_CholesterolContent = mean_CholesterolContent + threshold * std_CholesterolContent\n",
    "lower_limit_CholesterolContent = mean_CholesterolContent - threshold * std_CholesterolContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"CholesterolContent\"] >= lower_limit_CholesterolContent) & (recipes_data['CholesterolContent'] <= upper_limit_CholesterolContent)]\n",
    "\n",
    "\n",
    "# Column SodiumContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SodiumContent\"] = recipes_data[\"SodiumContent\"].apply(lambda x: np.log(x + 1) if x > 0 else x)\n",
    "\n",
    "std_SodiumContent = recipes_data['SodiumContent'].std()\n",
    "mean_SodiumContent = recipes_data['SodiumContent'].mean()\n",
    "upper_limit_SodiumContent = mean_SodiumContent + threshold * std_SodiumContent\n",
    "lower_limit_SodiumContent = mean_SodiumContent - threshold * std_SodiumContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"SodiumContent\"] >= lower_limit_SodiumContent) & (recipes_data['SodiumContent'] <= upper_limit_SodiumContent)]\n",
    "\n",
    "\n",
    "# Column CarbohydrateContent\n",
    "# Outlier weg\n",
    "recipes_data[\"CarbohydrateContent\"] = recipes_data[\"CarbohydrateContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_CarbohydrateContent = recipes_data['CarbohydrateContent'].std()\n",
    "mean_CarbohydrateContent = recipes_data['CarbohydrateContent'].mean()\n",
    "upper_limit_CarbohydrateContent = mean_CarbohydrateContent + threshold * std_CarbohydrateContent\n",
    "lower_limit_CarbohydrateContent = mean_CarbohydrateContent - threshold * std_CarbohydrateContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"CarbohydrateContent\"] >= lower_limit_CarbohydrateContent) & (recipes_data['CarbohydrateContent'] <= upper_limit_CarbohydrateContent)]\n",
    "\n",
    "\n",
    "# Column FiberContent\n",
    "# Outlier weg\n",
    "recipes_data[\"FiberContent\"] = recipes_data[\"FiberContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_FiberContent = recipes_data['FiberContent'].std()\n",
    "mean_FiberContent = recipes_data['FiberContent'].mean()\n",
    "upper_limit_FiberContent = mean_FiberContent + threshold * std_FiberContent\n",
    "lower_limit_FiberContent = mean_FiberContent - threshold * std_FiberContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"FiberContent\"] >= lower_limit_FiberContent) & (recipes_data['FiberContent'] <= upper_limit_FiberContent)]\n",
    "\n",
    "\n",
    "# Column SugarContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SugarContent\"] = recipes_data['SugarContent'].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_SugarContent = recipes_data['SugarContent'].std()\n",
    "mean_SugarContent = recipes_data['SugarContent'].mean()\n",
    "upper_limit_SugarContent = mean_SugarContent + threshold * std_SugarContent\n",
    "lower_limit_SugarContent = mean_SugarContent - threshold * std_SugarContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"SugarContent\"] >= lower_limit_SugarContent) & (recipes_data['SugarContent'] <= upper_limit_SugarContent)]\n",
    "\n",
    "\n",
    "\n",
    "# Column ProteinContent\n",
    "# Outlier weg\n",
    "recipes_data[\"ProteinContent\"] = recipes_data[\"ProteinContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_ProteinContent = recipes_data['ProteinContent'].std()\n",
    "mean_ProteinContent = recipes_data['ProteinContent'].mean()\n",
    "upper_limit_ProteinContent = mean_ProteinContent + threshold * std_ProteinContent\n",
    "lower_limit_ProteinContent = mean_ProteinContent - threshold * std_ProteinContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"ProteinContent\"] >= lower_limit_ProteinContent) & (recipes_data['ProteinContent'] <= upper_limit_ProteinContent)]\n",
    "\n",
    "\n",
    "# Column RecipeServings & Column RecipeYield\n",
    "\"\"\"\n",
    "RecipeServings: Anzahl der Portionen, die das Rezept ergibt\n",
    "RecipeYield: Gibt an, wie viele Stücke man aus dem Rezept erhält. Ein Rezept ergibt zum Beispiel 1/2 Liter Suppe, was 2 Portionen entspricht.\n",
    "-> Versuch: RecipeYield zu standardisieren z.B. in Liter, Gramm, Stück, ... -> Problem: >2000 verschiedene Einheiten (zu viele) -> RecipeYield nicht verwenden\n",
    "-> Stattdessen auf RecipeServings zurückgreifen und fehlende Werte durch randomforest imputieren\n",
    "\n",
    "Wichtig: Da RecipeServings schlecht verteilt ist (z.b. meisten Werte zwischen 0 und 10, aber auch Werte >1000) werden die Modelle ungenau\n",
    "-> Lösung: Verwendung von Klassen: Einteilung in Serving-Size: small, medium, large, ...\n",
    "-> Wichtig: One-hot-encoding nutzen (da es sich um Kategorien handelt) -> 3 Spalten: small, medium, large\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Correlation Matrix von numerischen Werten\n",
    "# corr = recipes_data[[\"CookTime\", \"PrepTime\", \"Calories\", \"FatContent\", \"CarbohydrateContent\", \"FiberContent\", \"SugarContent\", \"ProteinContent\", \"RecipeServings\"]].corr(numeric_only=True)\n",
    "# print(corr)\n",
    "\n",
    "# RecipeYield\n",
    "# recipes_data[\"RecipeYield_Quantity\"] = recipes_data[\"RecipeYield\"].str.split(\" \").str[0]\n",
    "# recipes_data[\"RecipeYield_Unit\"] = recipes_data[\"RecipeYield\"].str.split(\" \").str[1]\n",
    "recipes_data.drop(columns=[\"RecipeYield\"], inplace=True)\n",
    "\n",
    "\n",
    "# RecipeServings\n",
    "# Logarithmieren von RecipeServings, um die Verteilung zu verbessern\n",
    "recipes_data[\"RecipeServings\"] = recipes_data[\"RecipeServings\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(recipes_data['RecipeServings'], bins=10)\n",
    "# plt.title('Histogram of Recipe Servings')\n",
    "# plt.xlabel('Recipe Servings')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# Outlier weg\n",
    "std_RecipeServings = recipes_data[recipes_data['RecipeServings'].notna()][\"RecipeServings\"].std()\n",
    "mean_RecipeServings = recipes_data[recipes_data['RecipeServings'].notna()][\"RecipeServings\"].mean()\n",
    "# print(recipes_data['RecipeServings'].describe())\n",
    "\"\"\"\n",
    "mean         1.770398\n",
    "std          0.781950\n",
    "min          0.000000\n",
    "25%          1.386294 -> 25% der Werte sind 1.386294 oder kleiner -> small\n",
    "50%          1.791759 -> 50% der Werte sind 1.791759 oder kleiner -> medium\n",
    "75%          2.079442 -> 75% der Werte sind 2.079442 oder kleiner -> large\n",
    "max          6.907755\n",
    "\"\"\"\n",
    "\n",
    "upper_limit_RecipeServings = mean_RecipeServings + threshold * std_RecipeServings\n",
    "lower_limit_RecipeServings = mean_RecipeServings - threshold * std_RecipeServings\n",
    "\n",
    "recipes_data = recipes_data[((recipes_data[\"RecipeServings\"] >= lower_limit_RecipeServings) & (recipes_data['RecipeServings'] <= upper_limit_RecipeServings)) | (recipes_data['RecipeServings'].isna())]\n",
    "print(\"Shape nach Kürzung der Outlier: \", recipes_data.shape)\n",
    "\n",
    "recipes_data_description = recipes_data.describe()\n",
    "\n",
    "# Conditions\n",
    "conditions = [\n",
    "    (recipes_data[\"RecipeServings\"] <= recipes_data_description.loc[\"25%\", \"RecipeServings\"]),\n",
    "    (recipes_data[\"RecipeServings\"] > recipes_data_description.loc[\"25%\", \"RecipeServings\"]) & (recipes_data[\"RecipeServings\"] <= recipes_data_description.loc[\"75%\", \"RecipeServings\"]),\n",
    "    (recipes_data[\"RecipeServings\"] > recipes_data_description.loc[\"75%\", \"RecipeServings\"])\n",
    "]\n",
    "# choices: small - 0, medium - 1, large - 2\n",
    "choices = [0, 1, 2]\n",
    "\n",
    "# Discretization von RecipeServings -> Klassen: small = 0, medium = 1, large = 2 -> one-hot-encoding\n",
    "recipes_data[\"ServingClass\"] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "print(recipes_data[\"ServingClass\"])\n",
    "\n",
    "\n",
    "\n",
    "# Versuch: Fehlende Daten zu imputieren, nicht geklappt!\n",
    "\"\"\"\n",
    "# fehlende Werte in RecipeServings durch random forest imputieren\n",
    "features = ['CookTime', 'PrepTime', 'Calories', 'FatContent', 'FiberContent', 'ProteinContent']\n",
    "\n",
    "# test_data with and without RecipeServings, training_data only with RecipeServings\n",
    "known_servings = recipes_data[recipes_data['ServingClass'].notna()]\n",
    "unknown_servings = recipes_data[recipes_data['ServingClass'].isna()]\n",
    "\n",
    "X = known_servings[features]\n",
    "y_known = known_servings[\"ServingClass\"]\n",
    "\n",
    "# print(X)\n",
    "# print(y_known)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_known, test_size=0.3, random_state=random_seed)\n",
    "# print('Training Features Shape:', X_train.shape)\n",
    "# print('Training Labels Shape:', y_train.shape)\n",
    "# print('Validation Features Shape:', X_val.shape)\n",
    "# print('Validation Labels Shape:', y_val.shape)\n",
    "\n",
    "\n",
    "# Model trainieren\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=random_seed)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluieren\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Bewertung des Modells\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "# calculate r squared\n",
    "r_squared = model.score(X_val, y_val)\n",
    "print('R Squared: ', r_squared)\n",
    "print('MSE Mean Squred Error: ', mse) \n",
    "\n",
    "\n",
    "# y_val together with y_pred \n",
    "y_val_pred = pd.DataFrame({'y_val': y_val, 'y_pred': y_pred})\n",
    "# Extract RecipeId for the validation set\n",
    "recipe_ids_val = recipes_data.loc[y_val.index, 'RecipeId']\n",
    "# Add RecipeId to the y_val_pred dataframe\n",
    "y_val_pred['RecipeId'] = recipe_ids_val\n",
    "\n",
    "# Display the dataframe\n",
    "print(y_val_pred.head(20))\n",
    "\n",
    "\n",
    "# Model anwenden um fehlende Daten zu analysieren\n",
    "# X_unknown = unknown_servings[features]\n",
    "# y_unknown = unknown_servings[\"RecipeServings\"]\n",
    "# y_unknown_pred = model.predict(X_unknown)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        AuthorId       RecipeId           Time HighCalories HighProtein  \\\n",
      "count     132993  132993.000000  132993.000000       132993      132993   \n",
      "unique     47423            NaN            NaN            2           2   \n",
      "top     1930181E            NaN            NaN        False       False   \n",
      "freq         816            NaN            NaN        79416       79967   \n",
      "mean         NaN  153198.593730    2996.475371          NaN         NaN   \n",
      "std          NaN  130559.843136    2729.684319          NaN         NaN   \n",
      "min          NaN      40.000000       0.000000          NaN         NaN   \n",
      "25%          NaN   47111.000000    1200.600000          NaN         NaN   \n",
      "50%          NaN  109817.000000    2397.600000          NaN         NaN   \n",
      "75%          NaN  233056.000000    3602.000000          NaN         NaN   \n",
      "max          NaN  541195.000000   16019.200000          NaN         NaN   \n",
      "\n",
      "        LowFat LowSugar HighFiber  \n",
      "count   132993   132993    132993  \n",
      "unique       2        2         2  \n",
      "top      False    False     False  \n",
      "freq     93160    93108     79713  \n",
      "mean       NaN      NaN       NaN  \n",
      "std        NaN      NaN       NaN  \n",
      "min        NaN      NaN       NaN  \n",
      "25%        NaN      NaN       NaN  \n",
      "50%        NaN      NaN       NaN  \n",
      "75%        NaN      NaN       NaN  \n",
      "max        NaN      NaN       NaN  \n",
      "final_table.csv\n",
      "Shape davor (97381, 33)\n",
      "Shape danach (88380, 33)\n"
     ]
    }
   ],
   "source": [
    "# Laden der Datensätze\n",
    "\n",
    "\n",
    "\"\"\"diet_df = pd.read_csv('../aufgabe/training_dataset/diet.csv')\n",
    "recipes_df = pd.read_csv('../aufgabe/training_dataset/recipes.csv')\n",
    "requests_df = pd.read_csv('../aufgabe/training_dataset/requests.csv')\n",
    "reviews_df = pd.read_csv('../aufgabe/training_dataset/reviews.csv')\"\"\"\n",
    "\n",
    "\n",
    "diet_df = users_diet \n",
    "recipes_df = recipes_data\n",
    "requests_df = requests_data\n",
    "reviews_df = reviews_data_test\n",
    "print (requests_data.describe(include=\"all\"))\n",
    "\n",
    "# Anzeigen der ersten Reihen jedes Dataframes, um ihre Struktur zu verstehen\n",
    "diet_df.head(), recipes_df.head(), requests_df.head(), reviews_df.head()\n",
    "\n",
    "# Verbinden von reviews_df mit diet_df über AuthorId\n",
    "combined_df1 = pd.merge(reviews_df, diet_df, on='AuthorId', how='left')\n",
    "\n",
    "# Verbinden von combined_df1 mit requests_df über AuthorId und RecipeId\n",
    "combined_df2 = pd.merge(combined_df1, requests_df, on=['AuthorId', 'RecipeId'], how='left')\n",
    "\n",
    "# Verbinden von combined_df2 mit recipes_df über RecipeId\n",
    "final_combined_df = pd.merge(combined_df2, recipes_df, on='RecipeId', how='left')\n",
    "\n",
    "# Anzeigen der ersten Reihen des endgültigen integrierten Datensatzes\n",
    "final_combined_df\n",
    "\n",
    "\n",
    "csv_file_path = 'final_table.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "final_combined_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Returning the file path for download\n",
    "print (csv_file_path)\n",
    "\n",
    "\n",
    "\n",
    "# Clean data without nan values\n",
    "print(\"Shape davor\", final_combined_df.shape)\n",
    "clean_data = final_combined_df[final_combined_df[\"Time\"].notna()]\n",
    "clean_data = clean_data[clean_data[\"Name\"].notna()]\n",
    "print(\"Shape danach\", clean_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Werte:  Like\n",
      "False    76745\n",
      "True     11635\n",
      "Name: count, dtype: int64\n",
      "R Squared:  0.8688617334238515\n",
      "Anzahl true:  0\n",
      "Anzahl false:  8838\n"
     ]
    }
   ],
   "source": [
    "# 1. Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "features = ['Age', 'Time', 'Diet', 'CookTime', 'PrepTime','HighCalories','HighProtein','LowFat','LowSugar','HighFiber', 'Calories', 'FatContent', 'FiberContent', 'ProteinContent', 'Meat', 'Seafood', 'Vegetarian', 'Vegan', 'HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber', 'Time', 'ServingClass']\n",
    "\n",
    "# Diet in numerischen Wert umwandeln -> 1 = Vegetarian, 2 = Vegan, 3 = Omnivore\n",
    "clean_data[\"Diet\"] = clean_data[\"Diet\"].apply(lambda x: 1 if x == \"Vegetarian\" else (2 if x == \"Vegan\" else 3))\n",
    "\n",
    "print(\"Werte: \", clean_data[\"Like\"].value_counts())\n",
    "\n",
    "bool_columns = ['HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber', 'Meat', 'Seafood', 'Vegetarian', 'Vegan']\n",
    "for col in bool_columns:\n",
    "    clean_data[col] = clean_data[col].astype(int)\n",
    "\n",
    "\n",
    "X = clean_data[features]\n",
    "y = clean_data['Like']\n",
    "\n",
    "# column names of final_combined_df\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=random_seed)\n",
    "tree = DecisionTreeClassifier(random_state=random_seed)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluieren\n",
    "y_pred = tree.predict(X_val)\n",
    "\n",
    "# Bewertung des Modells\n",
    "rsquared = tree.score(X_val, y_val)\n",
    "\n",
    "print('R Squared: ', rsquared)\n",
    "\n",
    "# Nebeneinander setzen von y_val und y_pred\n",
    "y_val_pred = pd.DataFrame({'y_val': y_val, 'y_pred': y_pred})   \n",
    "\n",
    "summeTrue = 0\n",
    "summeFalse = 0\n",
    "for i in range(y_pred.shape[0]):\n",
    "    if y_pred[i] == True:\n",
    "        summeTrue += 1\n",
    "    else:\n",
    "        summeFalse += 1\n",
    "\n",
    "print(\"Anzahl true: \", summeTrue)\n",
    "print(\"Anzahl false: \", summeFalse)\n",
    "    \n",
    "\n",
    "# Menge an true und false in y_pred\n",
    "\n",
    "\n",
    "# final_combined_df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac-cup-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
