{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 44,
=======
   "execution_count": 1,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "\n",
    "random_seed = 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data-Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. diet.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 16,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>Diet</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>271906</td>\n",
       "      <td>271906</td>\n",
       "      <td>271906.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>271906</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>10000120E</td>\n",
       "      <td>Vegetarian</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>143383</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.503674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.898141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AuthorId        Diet            Age\n",
       "count      271906      271906  271906.000000\n",
       "unique     271906           3            NaN\n",
       "top     10000120E  Vegetarian            NaN\n",
       "freq            1      143383            NaN\n",
       "mean          NaN         NaN      48.503674\n",
       "std           NaN         NaN      17.898141\n",
       "min           NaN         NaN      18.000000\n",
       "25%           NaN         NaN      33.000000\n",
       "50%           NaN         NaN      48.000000\n",
       "75%           NaN         NaN      64.000000\n",
       "max           NaN         NaN      79.000000"
      ]
     },
<<<<<<< HEAD
     "execution_count": 15,
=======
     "execution_count": 16,
>>>>>>> refs/remotes/origin/main
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_diet_data = pd.read_csv('../aufgabe/training_dataset/diet.csv')\n",
    "# print(users_diet.head())\n",
    "# users_diet.shape\n",
    "# users_diet.dtypes\n",
    "\n",
    "# Nullwerte\n",
    "# users_diet.isnull().sum()\n",
    "\n",
    "# 1. Spalte \"Diet\"\n",
    "# -> ein Nullwert drin \n",
    "#users_diet[users_diet[\"Diet\"].isna()]\n",
    "# Author 646062A ohne Wert für Diet -> Weg mit dem Hund\n",
    "users_diet_data = users_diet.drop(users_diet[users_diet[\"Diet\"].isna()].index).reset_index(drop=True)\n",
    "\n",
    "# 2.  Weitere missing values finden: z.B. <empty field>, \"0\", \".\", \"999\", \"NA\" ...\n",
    "# users_diet[(users_diet[\"Diet\"] == \"\") | (users_diet[\"Diet\"] == \".\") | (users_diet[\"Diet\"] == \"999\")]\n",
    "#keine Nullwerte mehr\n",
    "\n",
    "\n",
    "# 3. Spalte \"Age\"\n",
    "#print(users_diet[users_diet[\"Age\"] > 100])\n",
    "#print (users_diet[users_diet[\"Age\"] < 5])\n",
    "# print(\"Range Alter\", users_diet[\"Age\"].min(), users_diet[\"Age\"].max())\n",
    "# -> keine werte < 5 oder > 100\n",
    "\n",
    "# 3. Spalte \"AuthorId\"\n",
    "# print(\"Einzigartige IDs: \", users_diet[\"AuthorId\"].nunique(), users_diet.shape[0])\n",
    "#duplicate_authorid_counts = users_diet['AuthorId'].duplicated().sum()\n",
    "#print(duplicate_authorid_counts)\n",
    "# -> keine Duplikate\n",
    "\n",
    "\n",
    "# Datentyp bei \"Diet\" zu Category ändern\n",
<<<<<<< HEAD
    "users_diet[\"Diet\"] = users_diet['Diet'].astype(\"category\")\n",
    "users_diet.dtypes\n",
    "users_diet.describe(include=\"all\")\n"
=======
    "users_diet_data[\"Diet\"] = users_diet_data['Diet'].astype(\"category\")\n",
    "users_diet_data.dtypes\n"
>>>>>>> refs/remotes/origin/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. reviews.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 17,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AuthorId  RecipeId  Rating   Like\n",
      "0  1000036C    320576   False  False\n",
      "1  1000216B    189335   False  False\n",
      "2  1000221A    133043    True  False\n",
      "3  1000221A     90537    True  False\n",
      "4  1000221A    334314    True  False\n",
      "      AuthorId  RecipeId  Rating  TestSetId\n",
      "0     2492191A     33671    True        1.0\n",
      "1  2002019979A     92647    True        2.0\n",
      "2      408594E    161770   False        3.0\n",
      "3  2001625557E    108231    True        4.0\n",
      "4  2001427116E     71109   False        5.0\n",
      "      AuthorId  RecipeId  Rating Like  TestSetId\n",
      "0     2492191A     33671    True  NaN        1.0\n",
      "1  2002019979A     92647    True  NaN        2.0\n",
      "2      408594E    161770   False  NaN        3.0\n",
      "3  2001625557E    108231    True  NaN        4.0\n",
      "4  2001427116E     71109   False  NaN        5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/var/folders/ld/b6k2sw2912sg5m2xrc0hr5qh0000gn/T/ipykernel_53230/490489713.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
=======
      "/var/folders/j0/z37d__mj75g9k_jcd30hbw0c0000gn/T/ipykernel_45417/1333568789.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
>>>>>>> refs/remotes/origin/main
      "  reviews_data = pd.read_csv(\"../aufgabe/training_dataset/reviews.csv\")\n"
     ]
    }
   ],
   "source": [
    "reviews_data = pd.read_csv(\"../aufgabe/training_dataset/reviews.csv\")\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "# 1. Adjusting the 'Rating' column\n",
    "# Set 'Rating' to True if it's 2, and False otherwise (including NaN)\n",
    "reviews_data['Rating'] = reviews_data['Rating'] == 2\n",
    "reviews_data[\"Rating\"] = reviews_data[\"Rating\"].astype(\"bool\")\n",
    "\n",
    "\n",
    "# 2. check for duplicates (gleiche AuthorId und RecipeId)\n",
    "duplicate_entries_counts = reviews_data.duplicated(subset=['AuthorId', 'RecipeId']).sum()\n",
    "duplicate_entries_counts\n",
    "# -> keine Duplikate\n",
    "\n",
    "# 3.  Test-Daten -> TestSetId != NaN, die anderen nicht für Modelle verwenden\n",
    "reviews_data_test = reviews_data[reviews_data[\"TestSetId\"].isna()].drop('TestSetId', axis=1).reset_index(drop=True)\n",
    "reviews_data_test[\"Like\"] = reviews_data_test[\"Like\"].astype(\"bool\")\n",
=======
    "# Test-Daten -> TestSetId != NaN, die anderen nicht für Modelle verwenden\n",
    "reviews_data = reviews_data[reviews_data[\"TestSetId\"].isna()].reset_index(drop=True)\n",
    "reviews_data.loc[reviews_data['Rating'].isna(), \"Rating\"] = 999\n",
    "reviews_data[\"Rating\"] = reviews_data[\"Rating\"].astype(\"category\")\n",
    "\n",
    "# Like column zu boolean\n",
    "reviews_data[\"Like\"] = reviews_data[\"Like\"].astype(\"bool\")\n",
>>>>>>> refs/remotes/origin/main
    "\n",
    "# 4. Trainings-Daten -> TestSetId == NaN\n",
    "reviews_data_training = reviews_data[reviews_data[\"TestSetId\"].notna()].drop('Like', axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#Output: \n",
    "# rating ist true bei 2, false bei NA\n",
    "# reviews_data_test: 10000 rows × 4 columns, 0 missing values, AuthorId, RecipeId, Rating, Like (TestSetId nicht mehr vorhanden)\n",
    "# reviews_data_training: 40000 rows × 4 columns, 0 missing values, AuthorId, RecipeId, Rating, TestSetId  (like nicht mehr vorhanden)\n",
    "# reviews_data: 50000 rows × 5 columns, 0 missing values, AuthorId, RecipeId, Rating, Like, TestSetId (alles vorhanden, und clean)\n",
    "\n",
    "print(reviews_data_test.head())\n",
    "print(reviews_data_training.head())\n",
    "print(reviews_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. requests.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132993 94.8628695745212\n",
      "Number of duplicate entries: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>Time</th>\n",
       "      <th>HighCalories</th>\n",
       "      <th>HighProtein</th>\n",
       "      <th>LowFat</th>\n",
       "      <th>LowSugar</th>\n",
       "      <th>HighFiber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>132993</td>\n",
       "      <td>132993.000000</td>\n",
       "      <td>132993.000000</td>\n",
       "      <td>132993</td>\n",
       "      <td>132993</td>\n",
       "      <td>132993</td>\n",
       "      <td>132993</td>\n",
       "      <td>132993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>47423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1930181E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79416</td>\n",
       "      <td>79967</td>\n",
       "      <td>93160</td>\n",
       "      <td>93108</td>\n",
       "      <td>79713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>153198.593730</td>\n",
       "      <td>2996.475371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>130559.843136</td>\n",
       "      <td>2729.684319</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>47111.000000</td>\n",
       "      <td>1200.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>109817.000000</td>\n",
       "      <td>2397.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>233056.000000</td>\n",
       "      <td>3602.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>541195.000000</td>\n",
       "      <td>16019.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AuthorId       RecipeId           Time HighCalories HighProtein  \\\n",
       "count     132993  132993.000000  132993.000000       132993      132993   \n",
       "unique     47423            NaN            NaN            2           2   \n",
       "top     1930181E            NaN            NaN        False       False   \n",
       "freq         816            NaN            NaN        79416       79967   \n",
       "mean         NaN  153198.593730    2996.475371          NaN         NaN   \n",
       "std          NaN  130559.843136    2729.684319          NaN         NaN   \n",
       "min          NaN      40.000000       0.000000          NaN         NaN   \n",
       "25%          NaN   47111.000000    1200.600000          NaN         NaN   \n",
       "50%          NaN  109817.000000    2397.600000          NaN         NaN   \n",
       "75%          NaN  233056.000000    3602.000000          NaN         NaN   \n",
       "max          NaN  541195.000000   16019.200000          NaN         NaN   \n",
       "\n",
       "        LowFat LowSugar HighFiber  \n",
       "count   132993   132993    132993  \n",
       "unique       2        2         2  \n",
       "top      False    False     False  \n",
       "freq     93160    93108     79713  \n",
       "mean       NaN      NaN       NaN  \n",
       "std        NaN      NaN       NaN  \n",
       "min        NaN      NaN       NaN  \n",
       "25%        NaN      NaN       NaN  \n",
       "50%        NaN      NaN       NaN  \n",
       "75%        NaN      NaN       NaN  \n",
       "max        NaN      NaN       NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
>>>>>>> refs/remotes/origin/main
   "source": [
    "requests_data = pd.read_csv(\"../aufgabe/training_dataset/requests.csv\")\n",
    "requests_data.head()\n",
    "\n",
    "# 1. Column: Time\n",
    "# Runden\n",
    "\"\"\" \n",
    "Column Time -> runden, da Nachkommastellen bei Kochzeit irrelevant\n",
    "Beschreibung: The duration a recipe should take at most (including the time reserved\n",
    "for the preparation and cooking).\n",
    "\"\"\"\n",
    "requests_data[\"Time\"] = requests_data[\"Time\"].round(1)\n",
    "\n",
    "#remove outliers    \n",
    "# Calculating IQR and determining the thresholds for outliers\n",
    "Q1 = np.quantile(requests_data[\"Time\"], 0.25)\n",
    "Q3 = np.quantile(requests_data[\"Time\"], 0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 4 * IQR\n",
    "upper_bound = Q3 + 4 * IQR\n",
    "\n",
    "# Removing outliers\n",
    "time_data_filtered = requests_data[\"Time\"][(requests_data[\"Time\"] >= lower_bound) & (requests_data[\"Time\"] <= upper_bound)]\n",
    "\n",
    "# Percentage of data retained after removing outliers\n",
    "percentage_retained = len(time_data_filtered) / len(requests_data[\"Time\"]) * 100\n",
    "\n",
    "\n",
    "print(len(time_data_filtered), percentage_retained)\n",
    "requests_data[\"Time\"] = time_data_filtered\n",
    "requests_data = requests_data.dropna(subset=[\"Time\"])\n",
    "\n",
    "\n",
    "\n",
    "# Visualization of the 'Time' column before and after removing outliers\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Plotting the original 'Time' data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(requests_data[\"Time\"])\n",
    "plt.title('Original Time Data (with Outliers)')\n",
    "plt.xlabel('Time')\n",
    "\n",
    "# Plotting the 'Time' data after removing outliers\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(time_data_filtered)\n",
    "plt.title('Time Data After Removing Outliers')\n",
    "plt.xlabel('Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Creating histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(requests_data[\"Time\"], bins=50, kde=True)\n",
    "plt.title('Histogram of Original Time Data')\n",
    "plt.xlabel('Time')\n",
    "plt.xlim(left=0)  # Adjust the x-axis as needed\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(time_data_filtered, bins=50, kde=True)\n",
    "plt.title('Histogram of Time Data (Without Outliers)')\n",
    "plt.xlabel('Time')\n",
    "plt.xlim(left=0)  # Adjust the x-axis as needed\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\"\"\"\"\"\n",
    "\n",
    "# 2.Teilweise negative Werte -> 0\n",
    "requests_data.loc[requests_data[\"Time\"] <= 0, \"Time\"] = 0\n",
    "\"\"\"\n",
    "Test, ob negative Werte immer False als \"Like\" haben -> stimmt aber nicht siehe Code:\n",
    "Requests mit [AuthorId, RecipeId] time <= 0 mit reviews [AuthorId, RecipeId] joinen und dort like checken\n",
    "joined_data = requests_data.merge(reviews_data_test, on=[\"AuthorId\", \"RecipeId\"], how=\"left\")\n",
    "joined_data = joined_data[joined_data[\"Time\"] <= 0]\n",
    "joined_data = joined_data[~joined_data[\"Like\"].isna()]\n",
    "\"\"\"\n",
    "\n",
    "# 3. Column: HighCalories, HighProtein, LowFat, LowSugar, HighFiber\n",
    "unique_values = {\n",
    "        \"HighCalories\": requests_data[\"HighCalories\"].unique(),\n",
    "        \"HighProtein\": requests_data[\"HighProtein\"].unique(),\n",
    "        \"LowFat\": requests_data[\"LowFat\"].unique(),\n",
    "        \"LowSugar\": requests_data[\"LowSugar\"].unique(),\n",
    "        \"HighFiber\": requests_data[\"HighFiber\"].unique()\n",
    "    }\n",
    "\n",
    "# 4. Column: HighCalories\n",
    "requests_data[\"HighCalories\"] = requests_data[\"HighCalories\"].astype(\"bool\")\n",
    "\n",
    "# 5. Column: HighProtein\n",
    "\"\"\"\n",
    "2 Werte: Indifferent und Yes\n",
    "Daraus wird boolean indifferent = False und Yes = True\n",
    "\"\"\"\n",
    "requests_data.loc[requests_data[\"HighProtein\"] == \"Indifferent\", \"HighProtein\"] = 0\n",
    "requests_data.loc[requests_data[\"HighProtein\"] == \"Yes\", \"HighProtein\"] = 1\n",
    "requests_data[\"HighProtein\"] = requests_data[\"HighProtein\"].astype(\"bool\")\n",
    "\n",
    "# 6. Column: LowFat\n",
    "requests_data[\"LowFat\"] = requests_data[\"LowFat\"].astype(\"bool\")\n",
    "\n",
    "# 7. Column: LowSugar\n",
    "\"\"\"\n",
    "2 Werte: Indifferent und 0. Interpretation: 0 -> user braucht kein low-sugar Inhalt, Indifferent -> User ist es egal\n",
    "Daraus wird boolean 0 = False und indifferent = True\n",
    "\"\"\"\n",
    "requests_data.loc[requests_data[\"LowSugar\"] == \"0\", \"LowSugar\"] = 0\n",
    "requests_data.loc[requests_data[\"LowSugar\"] == \"Indifferent\", \"LowSugar\"] = 1\n",
    "requests_data[\"LowSugar\"] = requests_data[\"LowSugar\"].astype(\"bool\")\n",
    "\n",
    "# 8. Column HighFiber\n",
    "requests_data[\"HighFiber\"] = requests_data[\"HighFiber\"].astype(\"bool\")\n",
    "\n",
    "# 9. check for duplicates (gleiche AuthorId und RecipeId)\n",
    "duplicate_entries_counts = requests_data.duplicated().sum()\n",
    "print(\"Number of duplicate entries:\", duplicate_entries_counts)\n",
    "\n",
    "\n",
    "# requests_data[\"HighFiber\"].unique()\n",
    "requests_data.head(20)\n",
    "requests_data.describe(include=\"all\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Output: alle columns sind sauber, keine missing values, keine duplicates, alle datentypen passen\n",
    "# alle werte auf boolean geändert, außer Time (float) und Ids (int)\n",
    "# requests_data: 50000 rows × 9 columns, 0 missing values, AuthorId, RecipeId, Time, HighCalories, HighProtein, LowFat, LowSugar, HighFiber\n",
    "# time < 0 -> 0, time > 0 -> gerundet auf 1 Nachkommastelle, time outliers entfernt (4*IQR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. recipes.csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 33,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape vorher:  (73253, 18)\n",
      "Shape nach Kürzung der Outlier:  (72073, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# fehlende Werte in RecipeServings durch random forest imputieren\\nfeatures = [\\'CookTime\\', \\'PrepTime\\', \\'Calories\\', \\'FatContent\\', \\'FiberContent\\', \\'ProteinContent\\']\\n\\n# test_data with and without RecipeServings, training_data only with RecipeServings\\nknown_servings = recipes_data[recipes_data[\\'ServingClass\\'].notna()]\\nunknown_servings = recipes_data[recipes_data[\\'ServingClass\\'].isna()]\\n\\nX = known_servings[features]\\ny_known = known_servings[\"ServingClass\"]\\n\\n# print(X)\\n# print(y_known)\\nX_train, X_val, y_train, y_val = train_test_split(X, y_known, test_size=0.3, random_state=random_seed)\\n# print(\\'Training Features Shape:\\', X_train.shape)\\n# print(\\'Training Labels Shape:\\', y_train.shape)\\n# print(\\'Validation Features Shape:\\', X_val.shape)\\n# print(\\'Validation Labels Shape:\\', y_val.shape)\\n\\n\\n# Model trainieren\\nmodel = RandomForestClassifier(n_estimators=200, random_state=random_seed)\\nmodel.fit(X_train, y_train)\\n\\n# Model evaluieren\\ny_pred = model.predict(X_val)\\n\\n# Bewertung des Modells\\nmse = mean_squared_error(y_val, y_pred)\\n# calculate r squared\\nr_squared = model.score(X_val, y_val)\\nprint(\\'R Squared: \\', r_squared)\\nprint(\\'MSE Mean Squred Error: \\', mse) \\n\\n\\n# y_val together with y_pred \\ny_val_pred = pd.DataFrame({\\'y_val\\': y_val, \\'y_pred\\': y_pred})\\n# Extract RecipeId for the validation set\\nrecipe_ids_val = recipes_data.loc[y_val.index, \\'RecipeId\\']\\n# Add RecipeId to the y_val_pred dataframe\\ny_val_pred[\\'RecipeId\\'] = recipe_ids_val\\n\\n# Display the dataframe\\nprint(y_val_pred.head(20))\\n\\n\\n# Model anwenden um fehlende Daten zu analysieren\\n# X_unknown = unknown_servings[features]\\n# y_unknown = unknown_servings[\"RecipeServings\"]\\n# y_unknown_pred = model.predict(X_unknown)\\n\\n\\n'"
      ]
     },
<<<<<<< HEAD
     "execution_count": 18,
=======
     "execution_count": 33,
>>>>>>> refs/remotes/origin/main
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_data = pd.read_csv(\"../aufgabe/training_dataset/recipes.csv\")\n",
    "threshold = 3.5\n",
    "\n",
    "# Column CookTime und Column PrepTime\n",
    "\"\"\"\n",
    "Outlier Detection:\n",
    "Außerhalb von 3,5*Standardabweichung -> Outlier\n",
    "\n",
    "=> Outlier werden entfernt\n",
    "\"\"\"\n",
    "# Name to lower\n",
    "recipes_data[\"Name\"] = recipes_data[\"Name\"].str.lower()\n",
    "\n",
    "# log transformation\n",
    "recipes_data[\"CookTime\"] = recipes_data[\"CookTime\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "recipes_data[\"PrepTime\"] = recipes_data[\"PrepTime\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "# print(recipes_data[\"PrepTime\"].describe())\n",
    "\n",
    "# plt.hist(recipes_data['PrepTime'], bins=10)\n",
    "# plt.title('Histogram of Cook Time')\n",
    "# plt.xlabel('Cook Time')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "std_CookTime = recipes_data['CookTime'].std()\n",
    "mean_CookTime = recipes_data['CookTime'].mean()\n",
    "upper_limit_CookTime = mean_CookTime + threshold * std_CookTime\n",
    "lower_limit_CookTime = mean_CookTime - threshold * std_CookTime\n",
    "\n",
    "\n",
    "std_PrepTime = recipes_data['PrepTime'].std()\n",
    "mean_PrepTime = recipes_data['PrepTime'].mean()\n",
    "upper_limit_PrepTime = mean_PrepTime + threshold * std_PrepTime\n",
    "lower_limit_PrepTime = mean_PrepTime - threshold * std_PrepTime\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"CookTime\"] >= lower_limit_CookTime) & (recipes_data['CookTime'] <= upper_limit_CookTime)]\n",
    "recipes_data = recipes_data[(recipes_data[\"PrepTime\"] >= lower_limit_PrepTime) & (recipes_data['PrepTime'] <= upper_limit_PrepTime)]\n",
    "print(\"Shape vorher: \", recipes_data.shape)\n",
    "\n",
    "\n",
    "# Column RecipeCategory\n",
    "recipes_data[\"RecipeCategory\"] = recipes_data[\"RecipeCategory\"].astype(\"category\")\n",
    "\n",
    "\n",
    "# Column RecipeIngredientQuantities und Column RecipeIngredientParts\n",
    "# zu liste von strings umwandeln\n",
    "recipes_data[\"RecipeIngredientQuantities\"] = recipes_data[\"RecipeIngredientQuantities\"].str.replace('character(0)', '').str.lstrip('\"c(\"').str.replace('\"', '').str.replace(\")\", \"\").str.replace('\\\\', '').str.split(\",\")\n",
    "recipes_data[\"RecipeIngredientParts\"] = recipes_data[\"RecipeIngredientParts\"].str.lower().replace('character(0)', '').str.lstrip('\"c(\"').str.replace('\"', '').str.replace(\")\", \"\").str.replace('\\\\', '').str.split(\",\")\n",
    "\n",
    "# Einteilung ob die Zutat in der Liste ist oder nicht\n",
    "# Fleisch\n",
    "value_meat = [\"chicken\", \"veal\", \"pork\", \"beef\", \"turkey\", \"ham\", \"bacon\", \"lamb\", \"duck\", \"goose\", \"rabbit\", \"venison\", \"quail\", \"pheasant\", \"alligator\", \"sausage\"]\n",
    "# Meeresfrüchte\n",
    "value_sea = [\"fish\", \"crab\", \"lobster\", \"shrimp\", \"prawn\", \"clam\", \"mussel\", \"scallop\", \"squid\", \"octopus\", \"anchovy\", \"sardine\", \"tuna\", \"salmon\", \"trout\", \"herring\", \"cod\", \"mackerel\", \"bass\", \"swordfish\", \"sturgeon\", \"walleye\", \"caviar\", \"crayfish\", \"cuttlefish\", \"sea cucumber\", \"sea snail\", \"sea bass\", \"sea bream\", \"sea trout\", \"seafood\", \"shellfish\"]\n",
    "# vegetarisch\n",
    "value_vegetarian = [\"tofu\", \"seitan\", \"tempeh\", \"plant-based\"]\n",
    "# vegan\n",
    "value_vegan = [\"vegan\"]\n",
    "\n",
    "def beinhaltet_substring(ingredient_list, category_list):\n",
    "    for ingredient in ingredient_list:\n",
    "        if any(cat in ingredient for cat in category_list):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "recipes_data[\"Meat\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_meat))\n",
    "recipes_data[\"Seafood\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_sea))\n",
    "recipes_data[\"Vegetarian\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_vegetarian))\n",
    "recipes_data[\"Vegan\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_vegan))\n",
    "\n",
    "\n",
    "# weitere Unterteilung, falls mehrere Kategorien anschlagen und falls alles 0, dann vegetarisch 1\n",
    "for index, row in recipes_data.iterrows():\n",
    "    if(beinhaltet_substring([row[\"Name\"]], value_vegan) == 1): \n",
    "        recipes_data.loc[index, [\"Vegan\"]] = 1\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 0\n",
    "        recipes_data.loc[index, [\"Seafood\"]] = 0\n",
    "        recipes_data.loc[index, [\"Meat\"]] = 0\n",
    "    elif(row[\"Meat\"] == 1 or row[\"Seafood\"] == 1):\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 0\n",
    "        recipes_data.loc[index, [\"Vegan\"]] = 0\n",
    "    elif(row[\"Vegetarian\"] == 0 and row[\"Vegan\"] == 0):\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 1\n",
    "\n",
    "\n",
    "# print(recipes_data[[\"RecipeId\", \"Meat\", \"Seafood\", \"Vegetarian\", \"Vegan\"]].head(20))\n",
    "# print(recipes_data[(recipes_data[\"Vegan\"] == 0) & (recipes_data[\"Vegetarian\"] == 0) & (recipes_data[\"Meat\"] == 0) & (recipes_data[\"Seafood\"] == 0)].value_counts())\n",
    "\n",
    "\n",
    "# Column Calories\n",
    "# Outlier weg\n",
    "recipes_data[\"Calories\"] = recipes_data[\"Calories\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "\n",
    "std_Calories = recipes_data['Calories'].std()\n",
    "mean_Calories = recipes_data['Calories'].mean()\n",
    "upper_limit_Calories = mean_Calories + threshold * std_Calories\n",
    "lower_limit_Calories = mean_Calories - threshold * std_Calories\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"Calories\"] >= lower_limit_Calories) & (recipes_data['Calories'] <= upper_limit_Calories)]\n",
    "\n",
    "\n",
    "# Column FatContent\n",
    "# Oulier weg\n",
    "\n",
    "recipes_data[\"FatContent\"] = recipes_data[\"FatContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_FatContent = recipes_data['FatContent'].std()\n",
    "mean_FatContent = recipes_data['FatContent'].mean()\n",
    "upper_limit_FatContent = mean_FatContent + threshold * std_FatContent\n",
    "lower_limit_FatContent = mean_FatContent - threshold * std_FatContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"FatContent\"] >= lower_limit_FatContent) & (recipes_data['FatContent'] <= upper_limit_FatContent)]\n",
    "\n",
    "\n",
    "# Column SaturatedFatContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SaturatedFatContent\"] = recipes_data[\"SaturatedFatContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_SaturatedFatContent = recipes_data['SaturatedFatContent'].std()\n",
    "mean_SaturatedFatContent = recipes_data['SaturatedFatContent'].mean()\n",
    "upper_limit_SaturatedFatContent = mean_SaturatedFatContent + threshold * std_SaturatedFatContent\n",
    "lower_limit_SaturatedFatContent = mean_SaturatedFatContent - threshold * std_SaturatedFatContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"SaturatedFatContent\"] >= lower_limit_SaturatedFatContent) & (recipes_data['SaturatedFatContent'] <= upper_limit_SaturatedFatContent)]\n",
    "\n",
    "\n",
    "# Column CholesterolContent\n",
    "# Outlier weg\n",
    "recipes_data[\"CholesterolContent\"] = recipes_data[\"CholesterolContent\"].apply(lambda x: np.log(x + 1) if x > 0 else x)\n",
    "\n",
    "std_CholesterolContent = recipes_data['CholesterolContent'].std()\n",
    "mean_CholesterolContent = recipes_data['CholesterolContent'].mean()\n",
    "upper_limit_CholesterolContent = mean_CholesterolContent + threshold * std_CholesterolContent\n",
    "lower_limit_CholesterolContent = mean_CholesterolContent - threshold * std_CholesterolContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"CholesterolContent\"] >= lower_limit_CholesterolContent) & (recipes_data['CholesterolContent'] <= upper_limit_CholesterolContent)]\n",
    "\n",
    "\n",
    "# Column SodiumContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SodiumContent\"] = recipes_data[\"SodiumContent\"].apply(lambda x: np.log(x + 1) if x > 0 else x)\n",
    "\n",
    "std_SodiumContent = recipes_data['SodiumContent'].std()\n",
    "mean_SodiumContent = recipes_data['SodiumContent'].mean()\n",
    "upper_limit_SodiumContent = mean_SodiumContent + threshold * std_SodiumContent\n",
    "lower_limit_SodiumContent = mean_SodiumContent - threshold * std_SodiumContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"SodiumContent\"] >= lower_limit_SodiumContent) & (recipes_data['SodiumContent'] <= upper_limit_SodiumContent)]\n",
    "\n",
    "\n",
    "# Column CarbohydrateContent\n",
    "# Outlier weg\n",
    "recipes_data[\"CarbohydrateContent\"] = recipes_data[\"CarbohydrateContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_CarbohydrateContent = recipes_data['CarbohydrateContent'].std()\n",
    "mean_CarbohydrateContent = recipes_data['CarbohydrateContent'].mean()\n",
    "upper_limit_CarbohydrateContent = mean_CarbohydrateContent + threshold * std_CarbohydrateContent\n",
    "lower_limit_CarbohydrateContent = mean_CarbohydrateContent - threshold * std_CarbohydrateContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"CarbohydrateContent\"] >= lower_limit_CarbohydrateContent) & (recipes_data['CarbohydrateContent'] <= upper_limit_CarbohydrateContent)]\n",
    "\n",
    "\n",
    "# Column FiberContent\n",
    "# Outlier weg\n",
    "recipes_data[\"FiberContent\"] = recipes_data[\"FiberContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_FiberContent = recipes_data['FiberContent'].std()\n",
    "mean_FiberContent = recipes_data['FiberContent'].mean()\n",
    "upper_limit_FiberContent = mean_FiberContent + threshold * std_FiberContent\n",
    "lower_limit_FiberContent = mean_FiberContent - threshold * std_FiberContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"FiberContent\"] >= lower_limit_FiberContent) & (recipes_data['FiberContent'] <= upper_limit_FiberContent)]\n",
    "\n",
    "\n",
    "# Column SugarContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SugarContent\"] = recipes_data['SugarContent'].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_SugarContent = recipes_data['SugarContent'].std()\n",
    "mean_SugarContent = recipes_data['SugarContent'].mean()\n",
    "upper_limit_SugarContent = mean_SugarContent + threshold * std_SugarContent\n",
    "lower_limit_SugarContent = mean_SugarContent - threshold * std_SugarContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"SugarContent\"] >= lower_limit_SugarContent) & (recipes_data['SugarContent'] <= upper_limit_SugarContent)]\n",
    "\n",
    "\n",
    "\n",
    "# Column ProteinContent\n",
    "# Outlier weg\n",
    "recipes_data[\"ProteinContent\"] = recipes_data[\"ProteinContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_ProteinContent = recipes_data['ProteinContent'].std()\n",
    "mean_ProteinContent = recipes_data['ProteinContent'].mean()\n",
    "upper_limit_ProteinContent = mean_ProteinContent + threshold * std_ProteinContent\n",
    "lower_limit_ProteinContent = mean_ProteinContent - threshold * std_ProteinContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"ProteinContent\"] >= lower_limit_ProteinContent) & (recipes_data['ProteinContent'] <= upper_limit_ProteinContent)]\n",
    "\n",
    "\n",
    "# Column RecipeServings & Column RecipeYield\n",
    "\"\"\"\n",
    "RecipeServings: Anzahl der Portionen, die das Rezept ergibt\n",
    "RecipeYield: Gibt an, wie viele Stücke man aus dem Rezept erhält. Ein Rezept ergibt zum Beispiel 1/2 Liter Suppe, was 2 Portionen entspricht.\n",
    "-> Versuch: RecipeYield zu standardisieren z.B. in Liter, Gramm, Stück, ... -> Problem: >2000 verschiedene Einheiten (zu viele) -> RecipeYield nicht verwenden\n",
    "-> Stattdessen auf RecipeServings zurückgreifen und fehlende Werte durch randomforest imputieren\n",
    "\n",
    "Wichtig: Da RecipeServings schlecht verteilt ist (z.b. meisten Werte zwischen 0 und 10, aber auch Werte >1000) werden die Modelle ungenau\n",
    "-> Lösung: Verwendung von Klassen: Einteilung in Serving-Size: small, medium, large, ...\n",
    "-> Wichtig: One-hot-encoding nutzen (da es sich um Kategorien handelt) -> 3 Spalten: small, medium, large\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Correlation Matrix von numerischen Werten\n",
    "# corr = recipes_data[[\"CookTime\", \"PrepTime\", \"Calories\", \"FatContent\", \"CarbohydrateContent\", \"FiberContent\", \"SugarContent\", \"ProteinContent\", \"RecipeServings\"]].corr(numeric_only=True)\n",
    "# print(corr)\n",
    "\n",
    "# RecipeYield\n",
    "# recipes_data[\"RecipeYield_Quantity\"] = recipes_data[\"RecipeYield\"].str.split(\" \").str[0]\n",
    "# recipes_data[\"RecipeYield_Unit\"] = recipes_data[\"RecipeYield\"].str.split(\" \").str[1]\n",
    "recipes_data.drop(columns=[\"RecipeYield\"], inplace=True)\n",
    "\n",
    "\n",
    "# RecipeServings\n",
    "# Logarithmieren von RecipeServings, um die Verteilung zu verbessern\n",
    "recipes_data[\"RecipeServings\"] = recipes_data[\"RecipeServings\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(recipes_data['RecipeServings'], bins=10)\n",
    "# plt.title('Histogram of Recipe Servings')\n",
    "# plt.xlabel('Recipe Servings')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# Outlier weg\n",
    "std_RecipeServings = recipes_data[recipes_data['RecipeServings'].notna()][\"RecipeServings\"].std()\n",
    "mean_RecipeServings = recipes_data[recipes_data['RecipeServings'].notna()][\"RecipeServings\"].mean()\n",
    "# print(recipes_data['RecipeServings'].describe())\n",
    "\"\"\"\n",
    "mean         1.770398\n",
    "std          0.781950\n",
    "min          0.000000\n",
    "25%          1.386294 -> 25% der Werte sind 1.386294 oder kleiner -> small\n",
    "50%          1.791759 -> 50% der Werte sind 1.791759 oder kleiner -> medium\n",
    "75%          2.079442 -> 75% der Werte sind 2.079442 oder kleiner -> large\n",
    "max          6.907755\n",
    "\"\"\"\n",
    "\n",
    "upper_limit_RecipeServings = mean_RecipeServings + threshold * std_RecipeServings\n",
    "lower_limit_RecipeServings = mean_RecipeServings - threshold * std_RecipeServings\n",
    "\n",
    "recipes_data = recipes_data[((recipes_data[\"RecipeServings\"] >= lower_limit_RecipeServings) & (recipes_data['RecipeServings'] <= upper_limit_RecipeServings)) | (recipes_data['RecipeServings'].isna())]\n",
    "print(\"Shape nach Kürzung der Outlier: \", recipes_data.shape)\n",
    "\n",
    "recipes_data_description = recipes_data.describe()\n",
    "\n",
    "# Conditions\n",
    "conditions = [\n",
    "    (recipes_data[\"RecipeServings\"] <= recipes_data_description.loc[\"25%\", \"RecipeServings\"]),\n",
    "    (recipes_data[\"RecipeServings\"] > recipes_data_description.loc[\"25%\", \"RecipeServings\"]) & (recipes_data[\"RecipeServings\"] <= recipes_data_description.loc[\"75%\", \"RecipeServings\"]),\n",
    "    (recipes_data[\"RecipeServings\"] > recipes_data_description.loc[\"75%\", \"RecipeServings\"])\n",
    "]\n",
    "# choices: small - 0, medium - 1, large - 2\n",
    "choices = [0, 1, 2]\n",
    "\n",
    "# Discretization von RecipeServings -> Klassen: small = 0, medium = 1, large = 2 -> one-hot-encoding\n",
    "recipes_data[\"ServingClass\"] = np.select(conditions, choices, default=np.nan)\n",
    "# Werte mit NA werden durch median 1 ersetzt.\n",
    "recipes_data.loc[recipes_data['RecipeServings'].isna(), \"ServingClass\"] = 1\n",
    "\n",
    "# In Csv\n",
    "# path_recipesData_marco = \"../cvs/Marco/recipes_data.csv\"\n",
    "# recipes_data.to_csv(path_recipesData_marco, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Versuch: Fehlende Daten zu imputieren, nicht geklappt!\n",
    "\"\"\"\n",
    "# fehlende Werte in RecipeServings durch random forest imputieren\n",
    "features = ['CookTime', 'PrepTime', 'Calories', 'FatContent', 'FiberContent', 'ProteinContent']\n",
    "\n",
    "# test_data with and without RecipeServings, training_data only with RecipeServings\n",
    "known_servings = recipes_data[recipes_data['ServingClass'].notna()]\n",
    "unknown_servings = recipes_data[recipes_data['ServingClass'].isna()]\n",
    "\n",
    "X = known_servings[features]\n",
    "y_known = known_servings[\"ServingClass\"]\n",
    "\n",
    "# print(X)\n",
    "# print(y_known)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_known, test_size=0.3, random_state=random_seed)\n",
    "# print('Training Features Shape:', X_train.shape)\n",
    "# print('Training Labels Shape:', y_train.shape)\n",
    "# print('Validation Features Shape:', X_val.shape)\n",
    "# print('Validation Labels Shape:', y_val.shape)\n",
    "\n",
    "\n",
    "# Model trainieren\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=random_seed)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluieren\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Bewertung des Modells\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "# calculate r squared\n",
    "r_squared = model.score(X_val, y_val)\n",
    "print('R Squared: ', r_squared)\n",
    "print('MSE Mean Squred Error: ', mse) \n",
    "\n",
    "\n",
    "# y_val together with y_pred \n",
    "y_val_pred = pd.DataFrame({'y_val': y_val, 'y_pred': y_pred})\n",
    "# Extract RecipeId for the validation set\n",
    "recipe_ids_val = recipes_data.loc[y_val.index, 'RecipeId']\n",
    "# Add RecipeId to the y_val_pred dataframe\n",
    "y_val_pred['RecipeId'] = recipe_ids_val\n",
    "\n",
    "# Display the dataframe\n",
    "print(y_val_pred.head(20))\n",
    "\n",
    "\n",
    "# Model anwenden um fehlende Daten zu analysieren\n",
    "# X_unknown = unknown_servings[features]\n",
    "# y_unknown = unknown_servings[\"RecipeServings\"]\n",
    "# y_unknown_pred = model.predict(X_unknown)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data Integration\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 36,
>>>>>>> refs/remotes/origin/main
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "diet_df:  271906\n",
      "recipes_df:  72073\n",
      "requests_df:  132993\n",
      "reviews_df:  97381\n",
      "97381\n"
     ]
    }
   ],
   "source": [
    "# Laden der Datensätze\n",
    "\n",
    "\n",
    "\"\"\"diet_df = pd.read_csv('../aufgabe/training_dataset/diet.csv')\n",
    "recipes_df = pd.read_csv('../aufgabe/training_dataset/recipes.csv')\n",
    "requests_df = pd.read_csv('../aufgabe/training_dataset/requests.csv')\n",
    "reviews_df = pd.read_csv('../aufgabe/training_dataset/reviews.csv')\"\"\"\n",
    "\n",
    "\n",
    "diet_df = users_diet \n",
    "recipes_df = recipes_data\n",
    "requests_df = requests_data\n",
    "reviews_df = reviews_data_test\n",
    "#reviews_df = reviews_data\n",
    "\n",
    "#print all lenghts\n",
    "print(\"diet_df: \", len(diet_df))\n",
    "print(\"recipes_df: \", len(recipes_df))\n",
    "print(\"requests_df: \", len(requests_df))\n",
    "print(\"reviews_df: \", len(reviews_df))\n",
    "\n",
    "# Anzeigen der ersten Reihen jedes Dataframes, um ihre Struktur zu verstehen\n",
    "diet_df.head(), recipes_df.head(), requests_df.head(), reviews_df.head()\n",
    "\n",
=======
      "Shape davor (97381, 34)\n",
      "Shape danach (93057, 34)\n",
      "final_table.csv\n",
      "count    93057.000000\n",
      "mean         0.868736\n",
      "std          0.646531\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          2.000000\n",
      "Name: ServingClass, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAq9klEQVR4nO3df3RU9Z3/8VcSmAm/ZiK/ErKEH5YKBPmxBAhjtUqNjDZ6ZIUtsJSmCLpwAkdI5dcuG9D2HCi6BVwCtPVH2F2RH9uCK5EgDSacSgQNZAtUOGhjgxsnATWZECGBzP3+4Td3GRMgExKTfHw+zrlHcj/v+5nPZz4Z5+Xl3muYZVmWAAAADBPe2gMAAABoCYQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICROrT2AFpTIBBQSUmJunXrprCwsNYeDgAAaATLslRZWanY2FiFh1//fM23OuSUlJQoLi6utYcBAACa4Ny5c+rbt+9127/VIadbt26SvnqTXC5XK48GAAA0ht/vV1xcnP09fj3f6pBT91dULpeLkAMAQDtzs0tNuPAYAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEgdWnsAABpvwLKs1h4CDPXxmuTWHgLQ7DiTAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRQgo5q1atUlhYWNA2ZMgQu/3y5ctKTU1Vjx491LVrV02ePFmlpaVBfRQXFys5OVmdO3dW7969tXjxYl29ejWoJjc3V6NHj5bT6dSgQYOUmZlZbywZGRkaMGCAIiMjlZiYqKNHj4YyFQAAYLiQz+QMGzZMn376qb398Y9/tNsWLVqkN954Q7t27VJeXp5KSkr02GOP2e21tbVKTk5WTU2NDh8+rK1btyozM1Pp6el2TVFRkZKTkzVhwgQVFhZq4cKFmjNnjvbv32/X7NixQ2lpaVq5cqWOHTumkSNHyuv1qqysrKnvAwAAMEyYZVlWY4tXrVqlPXv2qLCwsF5bRUWFevXqpW3btmnKlCmSpNOnT2vo0KHKz8/X+PHjtW/fPj388MMqKSlRdHS0JGnLli1aunSpzp8/L4fDoaVLlyorK0snT560+542bZrKy8uVnZ0tSUpMTNTYsWO1ceNGSVIgEFBcXJwWLFigZcuWNXryfr9fbrdbFRUVcrlcjT4OaC0DlmW19hBgqI/XJLf2EIBGa+z3d8hncs6ePavY2FjdfvvtmjFjhoqLiyVJBQUFunLlipKSkuzaIUOGqF+/fsrPz5ck5efna/jw4XbAkSSv1yu/369Tp07ZNdf2UVdT10dNTY0KCgqCasLDw5WUlGTXXE91dbX8fn/QBgAAzBRSyElMTFRmZqays7O1efNmFRUV6Z577lFlZaV8Pp8cDoeioqKCjomOjpbP55Mk+Xy+oIBT117XdqMav9+vS5cu6cKFC6qtrW2wpq6P61m9erXcbre9xcXFhTJ9AADQjnQIpfihhx6y/zxixAglJiaqf//+2rlzpzp16tTsg2tuy5cvV1pamv2z3+8n6AAAYKhbuoU8KipKd9xxhz788EPFxMSopqZG5eXlQTWlpaWKiYmRJMXExNS726ru55vVuFwuderUST179lRERESDNXV9XI/T6ZTL5QraAACAmW4p5Fy8eFEfffSR+vTpo4SEBHXs2FE5OTl2+5kzZ1RcXCyPxyNJ8ng8OnHiRNBdUAcOHJDL5VJ8fLxdc20fdTV1fTgcDiUkJATVBAIB5eTk2DUAAAAhhZynn35aeXl5+vjjj3X48GH93d/9nSIiIjR9+nS53W7Nnj1baWlpevvtt1VQUKBZs2bJ4/Fo/PjxkqSJEycqPj5eM2fO1P/8z/9o//79WrFihVJTU+V0OiVJc+fO1V/+8hctWbJEp0+f1qZNm7Rz504tWrTIHkdaWpp++9vfauvWrfrggw80b948VVVVadasWc341gAAgPYspGtyPvnkE02fPl2fffaZevXqpbvvvlvvvvuuevXqJUlat26dwsPDNXnyZFVXV8vr9WrTpk328REREdq7d6/mzZsnj8ejLl26KCUlRc8++6xdM3DgQGVlZWnRokXasGGD+vbtqxdffFFer9eumTp1qs6fP6/09HT5fD6NGjVK2dnZ9S5GBgAA314hPSfHNDwnB+0Nz8lBS+E5OWhPWuw5OQAAAO0BIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYKRbCjlr1qxRWFiYFi5caO+7fPmyUlNT1aNHD3Xt2lWTJ09WaWlp0HHFxcVKTk5W586d1bt3by1evFhXr14NqsnNzdXo0aPldDo1aNAgZWZm1nv9jIwMDRgwQJGRkUpMTNTRo0dvZToAAMAgTQ457733nn79619rxIgRQfsXLVqkN954Q7t27VJeXp5KSkr02GOP2e21tbVKTk5WTU2NDh8+rK1btyozM1Pp6el2TVFRkZKTkzVhwgQVFhZq4cKFmjNnjvbv32/X7NixQ2lpaVq5cqWOHTumkSNHyuv1qqysrKlTAgAABgmzLMsK9aCLFy9q9OjR2rRpk37xi19o1KhRWr9+vSoqKtSrVy9t27ZNU6ZMkSSdPn1aQ4cOVX5+vsaPH699+/bp4YcfVklJiaKjoyVJW7Zs0dKlS3X+/Hk5HA4tXbpUWVlZOnnypP2a06ZNU3l5ubKzsyVJiYmJGjt2rDZu3ChJCgQCiouL04IFC7Rs2bJGzcPv98vtdquiokIulyvUtwH4xg1YltXaQ4ChPl6T3NpDABqtsd/fTTqTk5qaquTkZCUlJQXtLygo0JUrV4L2DxkyRP369VN+fr4kKT8/X8OHD7cDjiR5vV75/X6dOnXKrvl6316v1+6jpqZGBQUFQTXh4eFKSkqyaxpSXV0tv98ftAEAADN1CPWA7du369ixY3rvvffqtfl8PjkcDkVFRQXtj46Ols/ns2uuDTh17XVtN6rx+/26dOmSvvjiC9XW1jZYc/r06euOffXq1XrmmWcaN1EAANCuhXQm59y5c3rqqaf06quvKjIysqXG1GKWL1+uiooKezt37lxrDwkAALSQkEJOQUGBysrKNHr0aHXo0EEdOnRQXl6eXnjhBXXo0EHR0dGqqalReXl50HGlpaWKiYmRJMXExNS726ru55vVuFwuderUST179lRERESDNXV9NMTpdMrlcgVtAADATCGFnPvvv18nTpxQYWGhvY0ZM0YzZsyw/9yxY0fl5OTYx5w5c0bFxcXyeDySJI/HoxMnTgTdBXXgwAG5XC7Fx8fbNdf2UVdT14fD4VBCQkJQTSAQUE5Ojl0DAAC+3UK6Jqdbt2668847g/Z16dJFPXr0sPfPnj1baWlp6t69u1wulxYsWCCPx6Px48dLkiZOnKj4+HjNnDlTa9eulc/n04oVK5Samiqn0ylJmjt3rjZu3KglS5bo8ccf18GDB7Vz505lZf3fnSVpaWlKSUnRmDFjNG7cOK1fv15VVVWaNWvWLb0hAADADCFfeHwz69atU3h4uCZPnqzq6mp5vV5t2rTJbo+IiNDevXs1b948eTwedenSRSkpKXr22WftmoEDByorK0uLFi3Shg0b1LdvX7344ovyer12zdSpU3X+/Hmlp6fL5/Np1KhRys7OrncxMgAA+HZq0nNyTMFzctDe8JwctBSek4P2pEWfkwMAANDWEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYKKeRs3rxZI0aMkMvlksvlksfj0b59++z2y5cvKzU1VT169FDXrl01efJklZaWBvVRXFys5ORkde7cWb1799bixYt19erVoJrc3FyNHj1aTqdTgwYNUmZmZr2xZGRkaMCAAYqMjFRiYqKOHj0aylQAAIDhQgo5ffv21Zo1a1RQUKD3339fP/jBD/Too4/q1KlTkqRFixbpjTfe0K5du5SXl6eSkhI99thj9vG1tbVKTk5WTU2NDh8+rK1btyozM1Pp6el2TVFRkZKTkzVhwgQVFhZq4cKFmjNnjvbv32/X7NixQ2lpaVq5cqWOHTumkSNHyuv1qqys7FbfDwAAYIgwy7KsW+mge/fueu655zRlyhT16tVL27Zt05QpUyRJp0+f1tChQ5Wfn6/x48dr3759evjhh1VSUqLo6GhJ0pYtW7R06VKdP39eDodDS5cuVVZWlk6ePGm/xrRp01ReXq7s7GxJUmJiosaOHauNGzdKkgKBgOLi4rRgwQItW7as0WP3+/1yu92qqKiQy+W6lbcB+EYMWJbV2kOAoT5ek9zaQwAarbHf302+Jqe2tlbbt29XVVWVPB6PCgoKdOXKFSUlJdk1Q4YMUb9+/ZSfny9Jys/P1/Dhw+2AI0ler1d+v98+G5Sfnx/UR11NXR81NTUqKCgIqgkPD1dSUpJdcz3V1dXy+/1BGwAAMFPIIefEiRPq2rWrnE6n5s6dq927dys+Pl4+n08Oh0NRUVFB9dHR0fL5fJIkn88XFHDq2uvablTj9/t16dIlXbhwQbW1tQ3W1PVxPatXr5bb7ba3uLi4UKcPAADaiZBDzuDBg1VYWKgjR45o3rx5SklJ0Z///OeWGFuzW758uSoqKuzt3LlzrT0kAADQQjqEeoDD4dCgQYMkSQkJCXrvvfe0YcMGTZ06VTU1NSovLw86m1NaWqqYmBhJUkxMTL27oOruvrq25ut3ZJWWlsrlcqlTp06KiIhQREREgzV1fVyP0+mU0+kMdcoAAKAduuXn5AQCAVVXVyshIUEdO3ZUTk6O3XbmzBkVFxfL4/FIkjwej06cOBF0F9SBAwfkcrkUHx9v11zbR11NXR8Oh0MJCQlBNYFAQDk5OXYNAABASGdyli9froceekj9+vVTZWWltm3bptzcXO3fv19ut1uzZ89WWlqaunfvLpfLpQULFsjj8Wj8+PGSpIkTJyo+Pl4zZ87U2rVr5fP5tGLFCqWmptpnWObOnauNGzdqyZIlevzxx3Xw4EHt3LlTWVn/d1dJWlqaUlJSNGbMGI0bN07r169XVVWVZs2a1YxvDQAAaM9CCjllZWX6yU9+ok8//VRut1sjRozQ/v379cADD0iS1q1bp/DwcE2ePFnV1dXyer3atGmTfXxERIT27t2refPmyePxqEuXLkpJSdGzzz5r1wwcOFBZWVlatGiRNmzYoL59++rFF1+U1+u1a6ZOnarz588rPT1dPp9Po0aNUnZ2dr2LkQEAwLfXLT8npz3jOTlob3hODloKz8lBe9Liz8kBAABoywg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEgdWnsAphqwLKu1hwAAwLcaZ3IAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGCmkkLN69WqNHTtW3bp1U+/evTVp0iSdOXMmqOby5ctKTU1Vjx491LVrV02ePFmlpaVBNcXFxUpOTlbnzp3Vu3dvLV68WFevXg2qyc3N1ejRo+V0OjVo0CBlZmbWG09GRoYGDBigyMhIJSYm6ujRo6FMBwAAGCykkJOXl6fU1FS9++67OnDggK5cuaKJEyeqqqrKrlm0aJHeeOMN7dq1S3l5eSopKdFjjz1mt9fW1io5OVk1NTU6fPiwtm7dqszMTKWnp9s1RUVFSk5O1oQJE1RYWKiFCxdqzpw52r9/v12zY8cOpaWlaeXKlTp27JhGjhwpr9ersrKyW3k/AACAIcIsy7KaevD58+fVu3dv5eXl6fvf/74qKirUq1cvbdu2TVOmTJEknT59WkOHDlV+fr7Gjx+vffv26eGHH1ZJSYmio6MlSVu2bNHSpUt1/vx5ORwOLV26VFlZWTp58qT9WtOmTVN5ebmys7MlSYmJiRo7dqw2btwoSQoEAoqLi9OCBQu0bNmyRo3f7/fL7XaroqJCLperqW9DgwYsy2rW/gCgJX28Jrm1hwA0WmO/v2/pmpyKigpJUvfu3SVJBQUFunLlipKSkuyaIUOGqF+/fsrPz5ck5efna/jw4XbAkSSv1yu/369Tp07ZNdf2UVdT10dNTY0KCgqCasLDw5WUlGTXNKS6ulp+vz9oAwAAZmpyyAkEAlq4cKG+973v6c4775Qk+Xw+ORwORUVFBdVGR0fL5/PZNdcGnLr2urYb1fj9fl26dEkXLlxQbW1tgzV1fTRk9erVcrvd9hYXFxf6xAEAQLvQ5JCTmpqqkydPavv27c05nha1fPlyVVRU2Nu5c+dae0gAAKCFdGjKQfPnz9fevXt16NAh9e3b194fExOjmpoalZeXB53NKS0tVUxMjF3z9bug6u6+urbm63dklZaWyuVyqVOnToqIiFBERESDNXV9NMTpdMrpdIY+YQAA0O6EdCbHsizNnz9fu3fv1sGDBzVw4MCg9oSEBHXs2FE5OTn2vjNnzqi4uFgej0eS5PF4dOLEiaC7oA4cOCCXy6X4+Hi75to+6mrq+nA4HEpISAiqCQQCysnJsWsAAMC3W0hnclJTU7Vt2za9/vrr6tatm339i9vtVqdOneR2uzV79mylpaWpe/fucrlcWrBggTwej8aPHy9JmjhxouLj4zVz5kytXbtWPp9PK1asUGpqqn2WZe7cudq4caOWLFmixx9/XAcPHtTOnTuVlfV/dyylpaUpJSVFY8aM0bhx47R+/XpVVVVp1qxZzfXeAACAdiykkLN582ZJ0n333Re0/5VXXtFPf/pTSdK6desUHh6uyZMnq7q6Wl6vV5s2bbJrIyIitHfvXs2bN08ej0ddunRRSkqKnn32Wbtm4MCBysrK0qJFi7Rhwwb17dtXL774orxer10zdepUnT9/Xunp6fL5fBo1apSys7PrXYwMAAC+nW7pOTntHc/JAYCv8JwctCffyHNyAAAA2ipCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgpA6tPQAAQOsbsCyrtYcAA328JrlVX58zOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYKOeQcOnRIjzzyiGJjYxUWFqY9e/YEtVuWpfT0dPXp00edOnVSUlKSzp49G1Tz+eefa8aMGXK5XIqKitLs2bN18eLFoJo//elPuueeexQZGam4uDitXbu23lh27dqlIUOGKDIyUsOHD9ebb74Z6nQAAIChQg45VVVVGjlypDIyMhpsX7t2rV544QVt2bJFR44cUZcuXeT1enX58mW7ZsaMGTp16pQOHDigvXv36tChQ3ryySftdr/fr4kTJ6p///4qKCjQc889p1WrVuk3v/mNXXP48GFNnz5ds2fP1vHjxzVp0iRNmjRJJ0+eDHVKAADAQGGWZVlNPjgsTLt379akSZMkfXUWJzY2Vj/72c/09NNPS5IqKioUHR2tzMxMTZs2TR988IHi4+P13nvvacyYMZKk7Oxs/fCHP9Qnn3yi2NhYbd68Wf/8z/8sn88nh8MhSVq2bJn27Nmj06dPS5KmTp2qqqoq7d271x7P+PHjNWrUKG3ZsqVR4/f7/XK73aqoqJDL5Wrq29CgAcuymrU/AADam4/XJLdIv439/m7Wa3KKiork8/mUlJRk73O73UpMTFR+fr4kKT8/X1FRUXbAkaSkpCSFh4fryJEjds33v/99O+BIktfr1ZkzZ/TFF1/YNde+Tl1N3es0pLq6Wn6/P2gDAABmataQ4/P5JEnR0dFB+6Ojo+02n8+n3r17B7V36NBB3bt3D6ppqI9rX+N6NXXtDVm9erXcbre9xcXFhTpFAADQTnyr7q5avny5Kioq7O3cuXOtPSQAANBCmjXkxMTESJJKS0uD9peWltptMTExKisrC2q/evWqPv/886Cahvq49jWuV1PX3hCn0ymXyxW0AQAAMzVryBk4cKBiYmKUk5Nj7/P7/Tpy5Ig8Ho8kyePxqLy8XAUFBXbNwYMHFQgElJiYaNccOnRIV65csWsOHDigwYMH67bbbrNrrn2dupq61wEAAN9uIYecixcvqrCwUIWFhZK+uti4sLBQxcXFCgsL08KFC/WLX/xC//3f/60TJ07oJz/5iWJjY+07sIYOHaoHH3xQTzzxhI4ePap33nlH8+fP17Rp0xQbGytJ+od/+Ac5HA7Nnj1bp06d0o4dO7RhwwalpaXZ43jqqaeUnZ2tf/3Xf9Xp06e1atUqvf/++5o/f/6tvysAAKDd6xDqAe+//74mTJhg/1wXPFJSUpSZmaklS5aoqqpKTz75pMrLy3X33XcrOztbkZGR9jGvvvqq5s+fr/vvv1/h4eGaPHmyXnjhBbvd7XbrrbfeUmpqqhISEtSzZ0+lp6cHPUvnrrvu0rZt27RixQr90z/9k7773e9qz549uvPOO5v0RgAAALPc0nNy2juekwMAQMsx6jk5AAAAbQUhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgJEIOAAAwEiEHAAAYiZADAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJEIOQAAwEiEHAAAYCRCDgAAMBIhBwAAGImQAwAAjETIAQAARiLkAAAAIxFyAACAkQg5AADASIQcAABgpHYfcjIyMjRgwABFRkYqMTFRR48ebe0hAQCANqBdh5wdO3YoLS1NK1eu1LFjxzRy5Eh5vV6VlZW19tAAAEAra9ch51e/+pWeeOIJzZo1S/Hx8dqyZYs6d+6sl19+ubWHBgAAWlmH1h5AU9XU1KigoEDLly+394WHhyspKUn5+fkNHlNdXa3q6mr754qKCkmS3+9v9vEFqr9s9j4BAGhPWuL79dp+Lcu6YV27DTkXLlxQbW2toqOjg/ZHR0fr9OnTDR6zevVqPfPMM/X2x8XFtcgYAQD4NnOvb9n+Kysr5Xa7r9vebkNOUyxfvlxpaWn2z4FAQJ9//rl69OihsLCwZnsdv9+vuLg4nTt3Ti6Xq9n6bUtMnyPza/9MnyPza/9Mn2NLzs+yLFVWVio2NvaGde025PTs2VMREREqLS0N2l9aWqqYmJgGj3E6nXI6nUH7oqKiWmqIcrlcRv7iXsv0OTK/9s/0OTK/9s/0ObbU/G50BqdOu73w2OFwKCEhQTk5Ofa+QCCgnJwceTyeVhwZAABoC9rtmRxJSktLU0pKisaMGaNx48Zp/fr1qqqq0qxZs1p7aAAAoJW165AzdepUnT9/Xunp6fL5fBo1apSys7PrXYz8TXM6nVq5cmW9vxozielzZH7tn+lzZH7tn+lzbAvzC7Nudv8VAABAO9Rur8kBAAC4EUIOAAAwEiEHAAAYiZADAACMRMhppIyMDA0YMECRkZFKTEzU0aNHb1i/a9cuDRkyRJGRkRo+fLjefPPNoHbLspSenq4+ffqoU6dOSkpK0tmzZ1tyCjcUyvx++9vf6p577tFtt92m2267TUlJSfXqf/rTnyosLCxoe/DBB1t6GtcVyvwyMzPrjT0yMjKopq2tnxTaHO+77756cwwLC1NycrJd05bW8NChQ3rkkUcUGxursLAw7dmz56bH5ObmavTo0XI6nRo0aJAyMzPr1YT6uW4poc7v97//vR544AH16tVLLpdLHo9H+/fvD6pZtWpVvfUbMmRIC87ixkKdY25uboO/oz6fL6iuva5hQ5+vsLAwDRs2zK5pS2u4evVqjR07Vt26dVPv3r01adIknTlz5qbHtfZ3ISGnEXbs2KG0tDStXLlSx44d08iRI+X1elVWVtZg/eHDhzV9+nTNnj1bx48f16RJkzRp0iSdPHnSrlm7dq1eeOEFbdmyRUeOHFGXLl3k9Xp1+fLlb2patlDnl5ubq+nTp+vtt99Wfn6+4uLiNHHiRP3v//5vUN2DDz6oTz/91N5ee+21b2I69YQ6P+mrJ3ReO/a//vWvQe1taf2k0Of4+9//Pmh+J0+eVEREhP7+7/8+qK6trGFVVZVGjhypjIyMRtUXFRUpOTlZEyZMUGFhoRYuXKg5c+YEBYGm/F60lFDnd+jQIT3wwAN68803VVBQoAkTJuiRRx7R8ePHg+qGDRsWtH5//OMfW2L4jRLqHOucOXMmaA69e/e229rzGm7YsCFoXufOnVP37t3rfQbbyhrm5eUpNTVV7777rg4cOKArV65o4sSJqqqquu4xbeK70MJNjRs3zkpNTbV/rq2ttWJjY63Vq1c3WP+jH/3ISk5ODtqXmJho/eM//qNlWZYVCASsmJgY67nnnrPby8vLLafTab322mstMIMbC3V+X3f16lWrW7du1tatW+19KSkp1qOPPtrcQ22SUOf3yiuvWG63+7r9tbX1s6xbX8N169ZZ3bp1sy5evGjva0treC1J1u7du29Ys2TJEmvYsGFB+6ZOnWp5vV7751t9z1pKY+bXkPj4eOuZZ56xf165cqU1cuTI5htYM2rMHN9++21LkvXFF19ct8akNdy9e7cVFhZmffzxx/a+tryGZWVlliQrLy/vujVt4buQMzk3UVNTo4KCAiUlJdn7wsPDlZSUpPz8/AaPyc/PD6qXJK/Xa9cXFRXJ5/MF1bjdbiUmJl63z5bSlPl93ZdffqkrV66oe/fuQftzc3PVu3dvDR48WPPmzdNnn33WrGNvjKbO7+LFi+rfv7/i4uL06KOP6tSpU3ZbW1o/qXnW8KWXXtK0adPUpUuXoP1tYQ2b4mafweZ4z9qSQCCgysrKep/Bs2fPKjY2VrfffrtmzJih4uLiVhph040aNUp9+vTRAw88oHfeecfeb9oavvTSS0pKSlL//v2D9rfVNayoqJCker9z12oL34WEnJu4cOGCamtr6z1FOTo6ut7fDdfx+Xw3rK/7Zyh9tpSmzO/rli5dqtjY2KBf1AcffFD//u//rpycHP3yl79UXl6eHnroIdXW1jbr+G+mKfMbPHiwXn75Zb3++uv6z//8TwUCAd1111365JNPJLWt9ZNufQ2PHj2qkydPas6cOUH728oaNsX1PoN+v1+XLl1qlt/7tuT555/XxYsX9aMf/cjel5iYqMzMTGVnZ2vz5s0qKirSPffco8rKylYcaeP16dNHW7Zs0e9+9zv97ne/U1xcnO677z4dO3ZMUvP8u6utKCkp0b59++p9BtvqGgYCAS1cuFDf+973dOedd163ri18F7br/60DWt+aNWu0fft25ebmBl2cO23aNPvPw4cP14gRI/Sd73xHubm5uv/++1tjqI3m8XiC/ievd911l4YOHapf//rX+vnPf96KI2sZL730koYPH65x48YF7W/Pa/htsm3bNj3zzDN6/fXXg65Xeeihh+w/jxgxQomJierfv7927typ2bNnt8ZQQzJ48GANHjzY/vmuu+7SRx99pHXr1uk//uM/WnFkzW/r1q2KiorSpEmTgva31TVMTU3VyZMnW/Uar8biTM5N9OzZUxERESotLQ3aX1paqpiYmAaPiYmJuWF93T9D6bOlNGV+dZ5//nmtWbNGb731lkaMGHHD2ttvv109e/bUhx9+eMtjDsWtzK9Ox44d9bd/+7f22NvS+km3Nseqqipt3769Uf/CbK01bIrrfQZdLpc6derULL8XbcH27ds1Z84c7dy5s95fC3xdVFSU7rjjjnaxftczbtw4e/ymrKFlWXr55Zc1c+ZMORyOG9a2hTWcP3++9u7dq7ffflt9+/a9YW1b+C4k5NyEw+FQQkKCcnJy7H2BQEA5OTlB/7V/LY/HE1QvSQcOHLDrBw4cqJiYmKAav9+vI0eOXLfPltKU+UlfXRH/85//XNnZ2RozZsxNX+eTTz7RZ599pj59+jTLuBurqfO7Vm1trU6cOGGPvS2tn3Rrc9y1a5eqq6v14x//+Kav01pr2BQ3+ww2x+9Fa3vttdc0a9Ysvfbaa0G3/l/PxYsX9dFHH7WL9buewsJCe/wmrKH01V1LH374YaP+Q6M119CyLM2fP1+7d+/WwYMHNXDgwJse0ya+C5vl8mXDbd++3XI6nVZmZqb15z//2XryySetqKgoy+fzWZZlWTNnzrSWLVtm17/zzjtWhw4drOeff9764IMPrJUrV1odO3a0Tpw4YdesWbPGioqKsl5//XXrT3/6k/Xoo49aAwcOtC5dutTm57dmzRrL4XBY//Vf/2V9+umn9lZZWWlZlmVVVlZaTz/9tJWfn28VFRVZf/jDH6zRo0db3/3ud63Lly+3+fk988wz1v79+62PPvrIKigosKZNm2ZFRkZap06dsmva0vpZVuhzrHP33XdbU6dOrbe/ra1hZWWldfz4cev48eOWJOtXv/qVdfz4ceuvf/2rZVmWtWzZMmvmzJl2/V/+8herc+fO1uLFi60PPvjAysjIsCIiIqzs7Gy75mbvWVue36uvvmp16NDBysjICPoMlpeX2zU/+9nPrNzcXKuoqMh65513rKSkJKtnz55WWVnZNz4/ywp9juvWrbP27NljnT171jpx4oT11FNPWeHh4dYf/vAHu6Y9r2GdH//4x1ZiYmKDfbalNZw3b57ldrut3NzcoN+5L7/80q5pi9+FhJxG+rd/+zerX79+lsPhsMaNG2e9++67dtu9995rpaSkBNXv3LnTuuOOOyyHw2ENGzbMysrKCmoPBALWv/zLv1jR0dGW0+m07r//fuvMmTPfxFQaFMr8+vfvb0mqt61cudKyLMv68ssvrYkTJ1q9evWyOnbsaPXv39964oknWuVfPHVCmd/ChQvt2ujoaOuHP/yhdezYsaD+2tr6WVbov6OnT5+2JFlvvfVWvb7a2hrW3U789a1uTikpKda9995b75hRo0ZZDofDuv32261XXnmlXr83es++SaHO7957771hvWV9dct8nz59LIfDYf3N3/yNNXXqVOvDDz/8Zid2jVDn+Mtf/tL6zne+Y0VGRlrdu3e37rvvPuvgwYP1+m2va2hZX90u3alTJ+s3v/lNg322pTVsaG6Sgj5XbfG7MOz/Dx4AAMAoXJMDAACMRMgBAABGIuQAAAAjEXIAAICRCDkAAMBIhBwAAGAkQg4AADASIQcAABiJkAMAAIxEyAEAAEYi5AAAACMRcgAAgJH+H3sYed3eLB/NAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
>>>>>>> refs/remotes/origin/main
    "# Verbinden von reviews_df mit diet_df über AuthorId\n",
    "combined_df1 = pd.merge(reviews_data, users_diet_data, on='AuthorId', how='left')\n",
    "\n",
    "# Verbinden von combined_df1 mit requests_df über AuthorId und RecipeId\n",
    "combined_df2 = pd.merge(combined_df1, requests_data, on=['AuthorId', 'RecipeId'], how='left')\n",
    "\n",
    "# Verbinden von combined_df2 mit recipes_df über RecipeId\n",
    "final_combined_df = pd.merge(combined_df2, recipes_data, on='RecipeId', how='left')\n",
    "\n",
    "\n",
    "# Clean data without nan values - z.B. falls Rezept gelöscht wurde usw.\n",
    "print(\"Shape davor\", final_combined_df.shape)\n",
    "final_combined_df = final_combined_df[final_combined_df[\"Time\"].notna()]\n",
    "final_combined_df = final_combined_df[final_combined_df[\"Name\"].notna()]\n",
    "print(\"Shape danach\", final_combined_df.shape)\n",
    "\n",
    "\n",
    "\"\"\"csv_file_path = 'final_table.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "final_combined_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Returning the file path for download\n",
<<<<<<< HEAD
    "print (csv_file_path)\"\"\"\n",
    "\n",
    "length_of_dataframe = len(final_combined_df)\n",
    "print(length_of_dataframe)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
=======
    "print(csv_file_path)\n",
    "\n",
    "# print(final_combined_df[\"ServingClass\"].describe())\n",
    "# plt.hist(final_combined_df[\"ServingClass\"], bins=3)\n",
    "# plt.show()\n",
>>>>>>> refs/remotes/origin/main
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Frytz Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test I - Random forrest classiifier (20 sec execution time, 67 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model predicted the outcomes as follows:\\n\\n\\n- Accuracy: Approximately 89.84%\\n- **Like (1)** was predicted **1,147** times.\\n- **Not Like (0)** was predicted **16,529** times. \\n\\nThis distribution reflects the model\\'s tendency to predict more instances of \"Not Like\" compared to \"Like.\\n\\nBalanced Accuracy (BAC): Approximately 66.86%\\nRecall: Approximately 35.84%\\nSpecificity: Approximately 97.87%\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "filtered_df = final_combined_df.dropna(subset=['Name', 'Time', 'Diet'])\n",
    "filtered_df = filtered_df.drop_duplicates(subset=['AuthorId', 'RecipeId'])\n",
    "filtered_df = filtered_df.drop(columns=['RecipeCategory', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 'RecipeServings', 'ServingClass', 'Name'])\n",
    "\n",
    "\n",
    "diet_numerical = pd.get_dummies(filtered_df['Diet'], prefix='Diet')\n",
    "filtered_df_numerical = pd.concat([filtered_df.drop('Diet', axis=1), diet_numerical], axis=1)\n",
    "\n",
    "bool_columns = filtered_df_numerical.select_dtypes(include=['bool']).columns\n",
    "filtered_df_numerical[bool_columns] = filtered_df_numerical[bool_columns].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Selecting features and target variable\n",
    "X = filtered_df_numerical.drop(columns=['Like', 'AuthorId', 'RecipeId'])\n",
    "y = filtered_df_numerical['Like']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "\n",
    "# Creating a Random Forest Classifier model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fitting the model with the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "prediction_counts = Counter(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "accuracy, report, prediction_counts\n",
    "\n",
    "\n",
    "\"\"\"The model predicted the outcomes as follows:\n",
    "\n",
    "\n",
    "- Accuracy: Approximately 89.84%\n",
    "- **Like (1)** was predicted **1,147** times.\n",
    "- **Not Like (0)** was predicted **16,529** times. \n",
    "\n",
    "This distribution reflects the model's tendency to predict more instances of \"Not Like\" compared to \"Like.\n",
    "\n",
    "Balanced Accuracy (BAC): Approximately 66.86%\n",
    "Recall: Approximately 35.84%\n",
    "Specificity: Approximately 97.87%\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test II - Gradient Boosting Classifier (5min execution time, 71%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Setting up GridSearchCV to find the best parameters\u001b[39;00m\n\u001b[1;32m     38\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(gb_model, param_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Best model from grid search\u001b[39;00m\n\u001b[1;32m     42\u001b[0m best_gb_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "# Preprocessing steps\n",
    "filtered_df = final_combined_df.dropna(subset=['Name', 'Time', 'Diet'])\n",
    "filtered_df = filtered_df.drop_duplicates(subset=['AuthorId', 'RecipeId'])\n",
    "filtered_df = filtered_df.drop(columns=['RecipeCategory', 'RecipeIngredientQuantities', \n",
    "                                        'RecipeIngredientParts', 'RecipeServings', \n",
    "                                        'ServingClass', 'Name'])\n",
    "\n",
    "diet_numerical = pd.get_dummies(filtered_df['Diet'], prefix='Diet')\n",
    "filtered_df_numerical = pd.concat([filtered_df.drop('Diet', axis=1), diet_numerical], axis=1)\n",
    "bool_columns = filtered_df_numerical.select_dtypes(include=['bool']).columns\n",
    "filtered_df_numerical[bool_columns] = filtered_df_numerical[bool_columns].astype(int)\n",
    "\n",
    "# Selecting features and target variable\n",
    "X = filtered_df_numerical.drop(columns=['Like', 'AuthorId', 'RecipeId'])\n",
    "y = filtered_df_numerical['Like']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "\n",
    "# Gradient Boosting Classifier model\n",
    "gb_model = GradientBoostingClassifier(random_state=2024)\n",
    "\n",
    "# Parameters for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "}\n",
    "\n",
    "# Setting up GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(gb_model, param_grid, scoring='balanced_accuracy', cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model from grid search\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "\n",
    "# Making predictions on the test set using the best model\n",
    "y_pred_gb = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "balanced_accuracy_gb = balanced_accuracy_score(y_test, y_pred_gb)\n",
    "report_gb = classification_report(y_test, y_pred_gb)\n",
    "prediction_counts_gb = Counter(y_pred_gb)\n",
    "\n",
    "print('Gradient Boosting Classifier:')\n",
    "print('Accuracy:', accuracy_gb)\n",
    "print('Balanced Accuracy:', balanced_accuracy_gb)\n",
    "print('Classification Report:\\n', report_gb)\n",
    "print('Prediction Counts:', prediction_counts_gb)\n",
    "\n",
    "\n",
    "\"\"\"Gradient Boosting Classifier:\n",
    "Accuracy: 0.9057479067662367\n",
    "Recall: Approximately 0.44\n",
    "Balanced Accuracy: 0.7079003100389369\n",
    "- **Like (1)** was predicted **1393** times.\n",
    "- **Not Like (0)** was predicted **16283** times. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.98      0.95     15379\n",
    "           1       0.73      0.44      0.55      2297\n",
    "\n",
    "    accuracy                           0.91     17676\n",
    "   macro avg       0.82      0.71      0.75     17676\n",
    "weighted avg       0.90      0.91      0.90     17676\n",
    "\n",
    "Prediction Counts: Counter({0: 16283, 1: 1393})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test III - XGBoost model (1 sec execution time, 72,4%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy Score for XGBoost: 0.7242974727378159\n",
      "Classification Report for XGBoost:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.97      0.95     15388\n",
      "        True       0.68      0.48      0.56      2288\n",
      "\n",
      "    accuracy                           0.90     17676\n",
      "   macro avg       0.80      0.72      0.75     17676\n",
      "weighted avg       0.89      0.90      0.90     17676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "# Assuming final_combined_df is your dataframe after loading the data\n",
    "# final_combined_df = pd.read_csv('path_to_your_csv.csv')  # Uncomment and set path to your CSV\n",
    "\n",
<<<<<<< HEAD
    "# Data preprocessing\n",
    "filtered_df = final_combined_df.dropna(subset=['Name', 'Time', 'Diet'])\n",
    "filtered_df = filtered_df.drop_duplicates(subset=['AuthorId', 'RecipeId'])\n",
    "filtered_df = filtered_df.drop(columns=['RecipeCategory', 'RecipeIngredientQuantities', \n",
    "                                        'RecipeIngredientParts', 'RecipeServings', \n",
    "                                        'ServingClass', 'Name'])\n",
    "\n",
    "\n",
    "# Converting 'HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber' to integers\n",
    "bool_cols_to_convert = ['HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber']\n",
    "filtered_df[bool_cols_to_convert] = filtered_df[bool_cols_to_convert].apply(lambda col: col.astype('bool').astype('int'))\n",
=======
    "# Diet in numerischen Wert umwandeln -> 1 = Vegetarian, 2 = Vegan, 3 = Omnivore\n",
    "final_combined_df[\"Diet\"] = final_combined_df[\"Diet\"].apply(lambda x: 1 if x == \"Vegetarian\" else (2 if x == \"Vegan\" else 3))\n",
    "\n",
    "print(\"Werte: \", final_combined_df[\"Like\"].value_counts())\n",
    "\n",
    "bool_columns = ['HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber', 'Meat', 'Seafood', 'Vegetarian', 'Vegan']\n",
    "for col in bool_columns:\n",
    "    final_combined_df[col] = final_combined_df[col].astype(int)\n",
    "\n",
    "\n",
    "X = final_combined_df[features]\n",
    "y = final_combined_df['Like']\n",
>>>>>>> refs/remotes/origin/main
    "\n",
    "# Select features and target\n",
    "X = filtered_df.drop(columns=['Like', 'AuthorId', 'RecipeId'])\n",
    "y = filtered_df['Like']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assuming 'Diet' is the only categorical column that needs to be converted\n",
    "# If 'Diet' is already encoded as a category with proper codes, you can directly convert it to int\n",
    "X_train['Diet'] = X_train['Diet'].cat.codes\n",
    "X_test['Diet'] = X_test['Diet'].cat.codes\n",
    "\n",
    "# If there are other categorical columns, convert them in the same way or use pd.get_dummies()\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Now that the model is fitted, you can predict on testing data\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Continue with the evaluation using the predictions\n",
    "bac_xgb = balanced_accuracy_score(y_test, y_pred_xgb)\n",
    "report_xgb = classification_report(y_test, y_pred_xgb)\n",
    "\n",
    "print('Balanced Accuracy Score for XGBoost:', bac_xgb)\n",
    "print('Classification Report for XGBoost:\\n', report_xgb)\n",
    "\n",
    "\n",
    "\"\"\"**XGBoost Model Performance Summary**\n",
    "\n",
    "The model predicted the outcomes as follows:\n",
    "\n",
    "- **Accuracy**: Approximately 90%\n",
    "- **Like (1)** was predicted **1,147** times.\n",
    "- **Not Like (0)** was predicted **16,529** times.\n",
    "\n",
    "This distribution reflects the model's tendency to predict more instances of \"Not Like\" compared to \"Like.\"\n",
    "\n",
    "- **Balanced Accuracy (BAC)**: Approximately 72.43%\n",
    "- **Recall**: Approximately 48%\n",
    "- **Specificity**: Approximately 97%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test IV - XGBoost model + hyperparameter (4 min execution time, 72,7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Perform grid search with cross-validation to find the best hyperparameters\u001b[39;00m\n\u001b[1;32m     51\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mxgb_model, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Get the best model from the grid search\u001b[39;00m\n\u001b[1;32m     55\u001b[0m best_xgb_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "# Assuming final_combined_df is your dataframe after loading the data\n",
    "# final_combined_df = pd.read_csv('path_to_your_csv.csv')  # Uncomment and set path to your CSV\n",
    "\n",
    "# Data preprocessing\n",
    "filtered_df = final_combined_df.dropna(subset=['Name', 'Time', 'Diet'])\n",
    "filtered_df = filtered_df.drop_duplicates(subset=['AuthorId', 'RecipeId'])\n",
    "filtered_df = filtered_df.drop(columns=['RecipeCategory', 'RecipeIngredientQuantities', \n",
    "                                        'RecipeIngredientParts', 'RecipeServings', \n",
    "                                        'ServingClass', 'Name'])\n",
    "\n",
    "\n",
    "# Converting 'HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber' to integers\n",
    "bool_cols_to_convert = ['HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber']\n",
    "filtered_df[bool_cols_to_convert] = filtered_df[bool_cols_to_convert].apply(lambda col: col.astype('bool').astype('int'))\n",
    "\n",
    "# Select features and target\n",
    "X = filtered_df.drop(columns=['Like', 'AuthorId', 'RecipeId'])\n",
    "y = filtered_df['Like']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assuming 'Diet' is the only categorical column that needs to be converted\n",
    "# If 'Diet' is already encoded as a category with proper codes, you can directly convert it to int\n",
    "X_train['Diet'] = X_train['Diet'].cat.codes\n",
    "X_test['Diet'] = X_test['Diet'].cat.codes\n",
    "\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform grid search with cross-validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='balanced_accuracy', cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on testing data with the best model\n",
    "y_pred_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "bac_xgb = balanced_accuracy_score(y_test, y_pred_xgb)\n",
    "report_xgb = classification_report(y_test, y_pred_xgb)\n",
    "\n",
    "print('Balanced Accuracy Score for XGBoost (after hyperparameter tuning):', bac_xgb)\n",
    "print('Classification Report for XGBoost (after hyperparameter tuning):\\n', report_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test V - Support Vector Machine (14 min execution time, 53,7%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy Score for SVM Classifier: 0.535268369254299\n",
      "Classification Report for SVM Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93     15379\n",
      "           1       0.38      0.09      0.15      2297\n",
      "\n",
      "    accuracy                           0.86     17676\n",
      "   macro avg       0.63      0.54      0.54     17676\n",
      "weighted avg       0.81      0.86      0.82     17676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming final_combined_df is already defined and loaded\n",
    "filtered_df = final_combined_df.dropna(subset=['Name', 'Time', 'Diet'])\n",
    "filtered_df = filtered_df.drop_duplicates(subset=['AuthorId', 'RecipeId'])\n",
    "filtered_df = filtered_df.drop(columns=['RecipeCategory', 'RecipeIngredientQuantities', 'RecipeIngredientParts', 'RecipeServings', 'ServingClass', 'Name'])\n",
    "\n",
    "\n",
    "diet_numerical = pd.get_dummies(filtered_df['Diet'], prefix='Diet')\n",
    "filtered_df_numerical = pd.concat([filtered_df.drop('Diet', axis=1), diet_numerical], axis=1)\n",
    "\n",
    "bool_columns = filtered_df_numerical.select_dtypes(include=['bool']).columns\n",
    "filtered_df_numerical[bool_columns] = filtered_df_numerical[bool_columns].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Selecting features and target variable\n",
    "X = filtered_df_numerical.drop(columns=['Like', 'AuthorId', 'RecipeId'])\n",
    "y = filtered_df_numerical['Like']\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "\n",
    "# Impute missing values with mean values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Check for NaN values after imputation\n",
    "if np.isnan(X_train_imputed).any() or np.isnan(X_test_imputed).any():\n",
    "    raise ValueError(\"NaN values present after imputation.\")\n",
    "\n",
    "# Initialize Support Vector Machine (SVM) classifier\n",
    "svm_model = SVC(kernel='linear', C=1, random_state=42)\n",
    "\n",
    "# Fit the model on training data\n",
    "svm_model.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Predict on testing data\n",
    "y_pred_svm = svm_model.predict(X_test_imputed)\n",
    "\n",
    "# Evaluate the model\n",
    "bac_svm = balanced_accuracy_score(y_test, y_pred_svm)\n",
    "report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "# Print the results\n",
    "print('Balanced Accuracy Score for SVM Classifier:', bac_svm)\n",
    "print('Classification Report for SVM Classifier:\\n', report_svm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac-cup-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
