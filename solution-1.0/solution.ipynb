{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score, confusion_matrix\n",
    "\n",
    "random_seed = 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data-Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. diet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuthorId      object\n",
       "Diet        category\n",
       "Age            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_diet_data = pd.read_csv('../aufgabe/training_dataset/diet.csv')\n",
    "# print(users_diet.head())\n",
    "# users_diet.shape\n",
    "# users_diet.dtypes\n",
    "\n",
    "# Nullwerte\n",
    "# users_diet.isnull().sum()\n",
    "\n",
    "# 1. Spalte \"Diet\"\n",
    "# -> ein Nullwert drin \n",
    "#users_diet[users_diet[\"Diet\"].isna()]\n",
    "\n",
    "# Author 646062A ohne Wert für Diet -> Weg mit dem Hund\n",
    "users_diet_data = users_diet_data.drop(users_diet_data[users_diet_data[\"Diet\"].isna()].index).reset_index(drop=True)\n",
    "\n",
    "# Weitere missing values finden: z.B. <empty field>, \"0\", \".\", \"999\", \"NA\" ...\n",
    "# users_diet[(users_diet[\"Diet\"] == \"\") | (users_diet[\"Diet\"] == \".\") | (users_diet[\"Diet\"] == \"999\")]\n",
    "\n",
    "\n",
    "# 2. Spalte \"Age\"\n",
    "# users_diet[users_diet[\"Age\"] > 100]\n",
    "# users_diet[users_diet[\"Age\"] < 5]\n",
    "# print(\"Range Alter\", users_diet[\"Age\"].min(), users_diet[\"Age\"].max())\n",
    "\n",
    "# 3. Spalte \"AuthorId\"\n",
    "# print(\"Einzigartige IDs: \", users_diet[\"AuthorId\"].nunique(), users_diet.shape[0])\n",
    "\n",
    "\n",
    "# Datentyp bei \"Diet\" zu Category ändern\n",
    "users_diet_data[\"Diet\"] = users_diet_data['Diet'].astype(\"category\")\n",
    "users_diet_data.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      AuthorId  RecipeId  Rating Like  TestSetId\n",
      "0     2492191A     33671     2.0  NaN        1.0\n",
      "1  2002019979A     92647     2.0  NaN        2.0\n",
      "3  2001625557E    108231     2.0  NaN        4.0\n",
      "6      588901B     87380     2.0  NaN        7.0\n",
      "7     1038235B      9475     2.0  NaN        8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/z37d__mj75g9k_jcd30hbw0c0000gn/T/ipykernel_67493/2431628837.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  reviews_data = pd.read_csv(\"../aufgabe/training_dataset/reviews.csv\")\n"
     ]
    }
   ],
   "source": [
    "reviews_data = pd.read_csv(\"../aufgabe/training_dataset/reviews.csv\")\n",
    "\n",
    "# reviews_data_test.dtypes\n",
    "reviews_userToPredict_data = reviews_data[reviews_data[\"Rating\"].notna()]\n",
    "print(reviews_userToPredict_data.head())\n",
    "\n",
    "# Test-Daten -> TestSetId != NaN, die anderen nicht für Modelle verwenden\n",
    "reviews_data = reviews_data[reviews_data[\"TestSetId\"].isna()].reset_index(drop=True)\n",
    "reviews_data.loc[reviews_data['Rating'].isna(), \"Rating\"] = 999\n",
    "reviews_data[\"Rating\"] = reviews_data[\"Rating\"].astype(\"category\")\n",
    "\n",
    "# Like column zu boolean\n",
    "reviews_data[\"Like\"] = reviews_data[\"Like\"].astype(\"bool\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. requests.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_data = pd.read_csv(\"../aufgabe/training_dataset/requests.csv\")\n",
    "requests_data.head()\n",
    "\n",
    "# Column: Time\n",
    "# Runden\n",
    "\"\"\" \n",
    "Column Time -> runden, da Nachkommastellen bei Kochzeit irrelevant\n",
    "Beschreibung: The duration a recipe should take at most (including the time reserved\n",
    "for the preparation and cooking).\n",
    "\"\"\"\n",
    "requests_data[\"Time\"] = requests_data[\"Time\"].round(1)\n",
    "\n",
    "# Teilweise negative Werte -> 0\n",
    "requests_data.loc[requests_data[\"Time\"] <= 0, \"Time\"] = 0\n",
    "\"\"\"\n",
    "Test, ob negative Werte immer False als \"Like\" haben -> stimmt aber nicht siehe Code:\n",
    "Requests mit [AuthorId, RecipeId] time <= 0 mit reviews [AuthorId, RecipeId] joinen und dort like checken\n",
    "joined_data = requests_data.merge(reviews_data_test, on=[\"AuthorId\", \"RecipeId\"], how=\"left\")\n",
    "joined_data = joined_data[joined_data[\"Time\"] <= 0]\n",
    "joined_data = joined_data[~joined_data[\"Like\"].isna()]\n",
    "\"\"\"\n",
    "\n",
    "# Column: HighCalories\n",
    "requests_data[\"HighCalories\"] = requests_data[\"HighCalories\"].astype(\"bool\")\n",
    "\n",
    "# Column: HighProtein\n",
    "\"\"\"\n",
    "2 Werte: Indifferent und Yes\n",
    "Daraus wird boolean indifferent = False und Yes = True\n",
    "\"\"\"\n",
    "requests_data.loc[requests_data[\"HighProtein\"] == \"Indifferent\", \"HighProtein\"] = 0\n",
    "requests_data.loc[requests_data[\"HighProtein\"] == \"Yes\", \"HighProtein\"] = 1\n",
    "requests_data[\"HighProtein\"] = requests_data[\"HighProtein\"].astype(\"bool\")\n",
    "\n",
    "# Column: LowFat\n",
    "requests_data[\"LowFat\"] = requests_data[\"LowFat\"].astype(\"bool\")\n",
    "\n",
    "# Column: LowSugar\n",
    "\"\"\"\n",
    "2 Werte: Indifferent und 0. Interpretation: 0 -> user braucht kein low-sugar Inhalt, Indifferent -> User ist es egal\n",
    "Daraus wird boolean 0 = False und indifferent = True\n",
    "\"\"\"\n",
    "requests_data.loc[requests_data[\"LowSugar\"] == \"0\", \"LowSugar\"] = 0\n",
    "requests_data.loc[requests_data[\"LowSugar\"] == \"Indifferent\", \"LowSugar\"] = 1\n",
    "requests_data[\"LowSugar\"] = requests_data[\"LowSugar\"].astype(\"bool\")\n",
    "\n",
    "# Column HighFiber\n",
    "requests_data[\"HighFiber\"] = requests_data[\"HighFiber\"].astype(\"bool\")\n",
    "\n",
    "# requests_data[\"HighFiber\"].unique()\n",
    "# requests_data.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. recipes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape vorher:  (73253, 18)\n",
      "Shape nach Kürzung der Outlier:  (72073, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# fehlende Werte in RecipeServings durch random forest imputieren\\nfeatures = [\\'CookTime\\', \\'PrepTime\\', \\'Calories\\', \\'FatContent\\', \\'FiberContent\\', \\'ProteinContent\\']\\n\\n# test_data with and without RecipeServings, training_data only with RecipeServings\\nknown_servings = recipes_data[recipes_data[\\'ServingClass\\'].notna()]\\nunknown_servings = recipes_data[recipes_data[\\'ServingClass\\'].isna()]\\n\\nX = known_servings[features]\\ny_known = known_servings[\"ServingClass\"]\\n\\n# print(X)\\n# print(y_known)\\nX_train, X_val, y_train, y_val = train_test_split(X, y_known, test_size=0.3, random_state=random_seed)\\n# print(\\'Training Features Shape:\\', X_train.shape)\\n# print(\\'Training Labels Shape:\\', y_train.shape)\\n# print(\\'Validation Features Shape:\\', X_val.shape)\\n# print(\\'Validation Labels Shape:\\', y_val.shape)\\n\\n\\n# Model trainieren\\nmodel = RandomForestClassifier(n_estimators=200, random_state=random_seed)\\nmodel.fit(X_train, y_train)\\n\\n# Model evaluieren\\ny_pred = model.predict(X_val)\\n\\n# Bewertung des Modells\\nmse = mean_squared_error(y_val, y_pred)\\n# calculate r squared\\nr_squared = model.score(X_val, y_val)\\nprint(\\'R Squared: \\', r_squared)\\nprint(\\'MSE Mean Squred Error: \\', mse) \\n\\n\\n# y_val together with y_pred \\ny_val_pred = pd.DataFrame({\\'y_val\\': y_val, \\'y_pred\\': y_pred})\\n# Extract RecipeId for the validation set\\nrecipe_ids_val = recipes_data.loc[y_val.index, \\'RecipeId\\']\\n# Add RecipeId to the y_val_pred dataframe\\ny_val_pred[\\'RecipeId\\'] = recipe_ids_val\\n\\n# Display the dataframe\\nprint(y_val_pred.head(20))\\n\\n\\n# Model anwenden um fehlende Daten zu analysieren\\n# X_unknown = unknown_servings[features]\\n# y_unknown = unknown_servings[\"RecipeServings\"]\\n# y_unknown_pred = model.predict(X_unknown)\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_data = pd.read_csv(\"../aufgabe/training_dataset/recipes.csv\")\n",
    "threshold = 3.5\n",
    "\n",
    "# Column CookTime und Column PrepTime\n",
    "\"\"\"\n",
    "Outlier Detection:\n",
    "Außerhalb von 3,5*Standardabweichung -> Outlier\n",
    "\n",
    "=> Outlier werden entfernt\n",
    "\"\"\"\n",
    "# Name to lower\n",
    "recipes_data[\"Name\"] = recipes_data[\"Name\"].str.lower()\n",
    "\n",
    "# log transformation\n",
    "recipes_data[\"CookTime\"] = recipes_data[\"CookTime\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "recipes_data[\"PrepTime\"] = recipes_data[\"PrepTime\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "# print(recipes_data[\"PrepTime\"].describe())\n",
    "\n",
    "# plt.hist(recipes_data['PrepTime'], bins=10)\n",
    "# plt.title('Histogram of Cook Time')\n",
    "# plt.xlabel('Cook Time')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "std_CookTime = recipes_data['CookTime'].std()\n",
    "mean_CookTime = recipes_data['CookTime'].mean()\n",
    "upper_limit_CookTime = mean_CookTime + threshold * std_CookTime\n",
    "lower_limit_CookTime = mean_CookTime - threshold * std_CookTime\n",
    "\n",
    "\n",
    "std_PrepTime = recipes_data['PrepTime'].std()\n",
    "mean_PrepTime = recipes_data['PrepTime'].mean()\n",
    "upper_limit_PrepTime = mean_PrepTime + threshold * std_PrepTime\n",
    "lower_limit_PrepTime = mean_PrepTime - threshold * std_PrepTime\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"CookTime\"] >= lower_limit_CookTime) & (recipes_data['CookTime'] <= upper_limit_CookTime)]\n",
    "recipes_data = recipes_data[(recipes_data[\"PrepTime\"] >= lower_limit_PrepTime) & (recipes_data['PrepTime'] <= upper_limit_PrepTime)]\n",
    "print(\"Shape vorher: \", recipes_data.shape)\n",
    "\n",
    "\n",
    "# Column RecipeCategory\n",
    "recipes_data[\"RecipeCategory\"] = recipes_data[\"RecipeCategory\"].astype(\"category\")\n",
    "\n",
    "\n",
    "# Column RecipeIngredientQuantities und Column RecipeIngredientParts\n",
    "# zu liste von strings umwandeln\n",
    "recipes_data[\"RecipeIngredientQuantities\"] = recipes_data[\"RecipeIngredientQuantities\"].str.replace('character(0)', '').str.lstrip('\"c(\"').str.replace('\"', '').str.replace(\")\", \"\").str.replace('\\\\', '').str.split(\",\")\n",
    "recipes_data[\"RecipeIngredientParts\"] = recipes_data[\"RecipeIngredientParts\"].str.lower().replace('character(0)', '').str.lstrip('\"c(\"').str.replace('\"', '').str.replace(\")\", \"\").str.replace('\\\\', '').str.split(\",\")\n",
    "\n",
    "# Einteilung ob die Zutat in der Liste ist oder nicht\n",
    "# Fleisch\n",
    "value_meat = [\"chicken\", \"veal\", \"pork\", \"beef\", \"turkey\", \"ham\", \"bacon\", \"lamb\", \"duck\", \"goose\", \"rabbit\", \"venison\", \"quail\", \"pheasant\", \"alligator\", \"sausage\"]\n",
    "# Meeresfrüchte\n",
    "value_sea = [\"fish\", \"crab\", \"lobster\", \"shrimp\", \"prawn\", \"clam\", \"mussel\", \"scallop\", \"squid\", \"octopus\", \"anchovy\", \"sardine\", \"tuna\", \"salmon\", \"trout\", \"herring\", \"cod\", \"mackerel\", \"bass\", \"swordfish\", \"sturgeon\", \"walleye\", \"caviar\", \"crayfish\", \"cuttlefish\", \"sea cucumber\", \"sea snail\", \"sea bass\", \"sea bream\", \"sea trout\", \"seafood\", \"shellfish\"]\n",
    "# vegetarisch\n",
    "value_vegetarian = [\"tofu\", \"seitan\", \"tempeh\", \"plant-based\"]\n",
    "# vegan\n",
    "value_vegan = [\"vegan\"]\n",
    "\n",
    "def beinhaltet_substring(ingredient_list, category_list):\n",
    "    for ingredient in ingredient_list:\n",
    "        if any(cat in ingredient for cat in category_list):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "recipes_data[\"Meat\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_meat))\n",
    "recipes_data[\"Seafood\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_sea))\n",
    "recipes_data[\"Vegetarian\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_vegetarian))\n",
    "recipes_data[\"Vegan\"] = recipes_data[\"RecipeIngredientParts\"].apply(lambda x: beinhaltet_substring(x, value_vegan))\n",
    "\n",
    "\n",
    "# weitere Unterteilung, falls mehrere Kategorien anschlagen und falls alles 0, dann vegetarisch 1\n",
    "for index, row in recipes_data.iterrows():\n",
    "    if(beinhaltet_substring([row[\"Name\"]], value_vegan) == 1): \n",
    "        recipes_data.loc[index, [\"Vegan\"]] = 1\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 0\n",
    "        recipes_data.loc[index, [\"Seafood\"]] = 0\n",
    "        recipes_data.loc[index, [\"Meat\"]] = 0\n",
    "    elif(row[\"Meat\"] == 1 or row[\"Seafood\"] == 1):\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 0\n",
    "        recipes_data.loc[index, [\"Vegan\"]] = 0\n",
    "    elif(row[\"Vegetarian\"] == 0 and row[\"Vegan\"] == 0):\n",
    "        recipes_data.loc[index, [\"Vegetarian\"]] = 1\n",
    "\n",
    "\n",
    "# print(recipes_data[[\"RecipeId\", \"Meat\", \"Seafood\", \"Vegetarian\", \"Vegan\"]].head(20))\n",
    "# print(recipes_data[(recipes_data[\"Vegan\"] == 0) & (recipes_data[\"Vegetarian\"] == 0) & (recipes_data[\"Meat\"] == 0) & (recipes_data[\"Seafood\"] == 0)].value_counts())\n",
    "\n",
    "\n",
    "# Column Calories\n",
    "# Outlier weg\n",
    "recipes_data[\"Calories\"] = recipes_data[\"Calories\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "\n",
    "std_Calories = recipes_data['Calories'].std()\n",
    "mean_Calories = recipes_data['Calories'].mean()\n",
    "upper_limit_Calories = mean_Calories + threshold * std_Calories\n",
    "lower_limit_Calories = mean_Calories - threshold * std_Calories\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"Calories\"] >= lower_limit_Calories) & (recipes_data['Calories'] <= upper_limit_Calories)]\n",
    "\n",
    "\n",
    "# Column FatContent\n",
    "# Oulier weg\n",
    "\n",
    "recipes_data[\"FatContent\"] = recipes_data[\"FatContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_FatContent = recipes_data['FatContent'].std()\n",
    "mean_FatContent = recipes_data['FatContent'].mean()\n",
    "upper_limit_FatContent = mean_FatContent + threshold * std_FatContent\n",
    "lower_limit_FatContent = mean_FatContent - threshold * std_FatContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"FatContent\"] >= lower_limit_FatContent) & (recipes_data['FatContent'] <= upper_limit_FatContent)]\n",
    "\n",
    "\n",
    "# Column SaturatedFatContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SaturatedFatContent\"] = recipes_data[\"SaturatedFatContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_SaturatedFatContent = recipes_data['SaturatedFatContent'].std()\n",
    "mean_SaturatedFatContent = recipes_data['SaturatedFatContent'].mean()\n",
    "upper_limit_SaturatedFatContent = mean_SaturatedFatContent + threshold * std_SaturatedFatContent\n",
    "lower_limit_SaturatedFatContent = mean_SaturatedFatContent - threshold * std_SaturatedFatContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"SaturatedFatContent\"] >= lower_limit_SaturatedFatContent) & (recipes_data['SaturatedFatContent'] <= upper_limit_SaturatedFatContent)]\n",
    "\n",
    "\n",
    "# Column CholesterolContent\n",
    "# Outlier weg\n",
    "recipes_data[\"CholesterolContent\"] = recipes_data[\"CholesterolContent\"].apply(lambda x: np.log(x + 1) if x > 0 else x)\n",
    "\n",
    "std_CholesterolContent = recipes_data['CholesterolContent'].std()\n",
    "mean_CholesterolContent = recipes_data['CholesterolContent'].mean()\n",
    "upper_limit_CholesterolContent = mean_CholesterolContent + threshold * std_CholesterolContent\n",
    "lower_limit_CholesterolContent = mean_CholesterolContent - threshold * std_CholesterolContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"CholesterolContent\"] >= lower_limit_CholesterolContent) & (recipes_data['CholesterolContent'] <= upper_limit_CholesterolContent)]\n",
    "\n",
    "\n",
    "# Column SodiumContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SodiumContent\"] = recipes_data[\"SodiumContent\"].apply(lambda x: np.log(x + 1) if x > 0 else x)\n",
    "\n",
    "std_SodiumContent = recipes_data['SodiumContent'].std()\n",
    "mean_SodiumContent = recipes_data['SodiumContent'].mean()\n",
    "upper_limit_SodiumContent = mean_SodiumContent + threshold * std_SodiumContent\n",
    "lower_limit_SodiumContent = mean_SodiumContent - threshold * std_SodiumContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"SodiumContent\"] >= lower_limit_SodiumContent) & (recipes_data['SodiumContent'] <= upper_limit_SodiumContent)]\n",
    "\n",
    "\n",
    "# Column CarbohydrateContent\n",
    "# Outlier weg\n",
    "recipes_data[\"CarbohydrateContent\"] = recipes_data[\"CarbohydrateContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_CarbohydrateContent = recipes_data['CarbohydrateContent'].std()\n",
    "mean_CarbohydrateContent = recipes_data['CarbohydrateContent'].mean()\n",
    "upper_limit_CarbohydrateContent = mean_CarbohydrateContent + threshold * std_CarbohydrateContent\n",
    "lower_limit_CarbohydrateContent = mean_CarbohydrateContent - threshold * std_CarbohydrateContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"CarbohydrateContent\"] >= lower_limit_CarbohydrateContent) & (recipes_data['CarbohydrateContent'] <= upper_limit_CarbohydrateContent)]\n",
    "\n",
    "\n",
    "# Column FiberContent\n",
    "# Outlier weg\n",
    "recipes_data[\"FiberContent\"] = recipes_data[\"FiberContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_FiberContent = recipes_data['FiberContent'].std()\n",
    "mean_FiberContent = recipes_data['FiberContent'].mean()\n",
    "upper_limit_FiberContent = mean_FiberContent + threshold * std_FiberContent\n",
    "lower_limit_FiberContent = mean_FiberContent - threshold * std_FiberContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"FiberContent\"] >= lower_limit_FiberContent) & (recipes_data['FiberContent'] <= upper_limit_FiberContent)]\n",
    "\n",
    "\n",
    "# Column SugarContent\n",
    "# Outlier weg\n",
    "recipes_data[\"SugarContent\"] = recipes_data['SugarContent'].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_SugarContent = recipes_data['SugarContent'].std()\n",
    "mean_SugarContent = recipes_data['SugarContent'].mean()\n",
    "upper_limit_SugarContent = mean_SugarContent + threshold * std_SugarContent\n",
    "lower_limit_SugarContent = mean_SugarContent - threshold * std_SugarContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"SugarContent\"] >= lower_limit_SugarContent) & (recipes_data['SugarContent'] <= upper_limit_SugarContent)]\n",
    "\n",
    "\n",
    "\n",
    "# Column ProteinContent\n",
    "# Outlier weg\n",
    "recipes_data[\"ProteinContent\"] = recipes_data[\"ProteinContent\"].apply(lambda x: np.log(x+1) if x > 0 else x)\n",
    "\n",
    "std_ProteinContent = recipes_data['ProteinContent'].std()\n",
    "mean_ProteinContent = recipes_data['ProteinContent'].mean()\n",
    "upper_limit_ProteinContent = mean_ProteinContent + threshold * std_ProteinContent\n",
    "lower_limit_ProteinContent = mean_ProteinContent - threshold * std_ProteinContent\n",
    "\n",
    "recipes_data = recipes_data[(recipes_data[\"ProteinContent\"] >= lower_limit_ProteinContent) & (recipes_data['ProteinContent'] <= upper_limit_ProteinContent)]\n",
    "\n",
    "\n",
    "# Column RecipeServings & Column RecipeYield\n",
    "\"\"\"\n",
    "RecipeServings: Anzahl der Portionen, die das Rezept ergibt\n",
    "RecipeYield: Gibt an, wie viele Stücke man aus dem Rezept erhält. Ein Rezept ergibt zum Beispiel 1/2 Liter Suppe, was 2 Portionen entspricht.\n",
    "-> Versuch: RecipeYield zu standardisieren z.B. in Liter, Gramm, Stück, ... -> Problem: >2000 verschiedene Einheiten (zu viele) -> RecipeYield nicht verwenden\n",
    "-> Stattdessen auf RecipeServings zurückgreifen und fehlende Werte durch randomforest imputieren\n",
    "\n",
    "Wichtig: Da RecipeServings schlecht verteilt ist (z.b. meisten Werte zwischen 0 und 10, aber auch Werte >1000) werden die Modelle ungenau\n",
    "-> Lösung: Verwendung von Klassen: Einteilung in Serving-Size: small, medium, large, ...\n",
    "-> Wichtig: One-hot-encoding nutzen (da es sich um Kategorien handelt) -> 3 Spalten: small, medium, large\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Correlation Matrix von numerischen Werten\n",
    "# corr = recipes_data[[\"CookTime\", \"PrepTime\", \"Calories\", \"FatContent\", \"CarbohydrateContent\", \"FiberContent\", \"SugarContent\", \"ProteinContent\", \"RecipeServings\"]].corr(numeric_only=True)\n",
    "# print(corr)\n",
    "\n",
    "# RecipeYield\n",
    "# recipes_data[\"RecipeYield_Quantity\"] = recipes_data[\"RecipeYield\"].str.split(\" \").str[0]\n",
    "# recipes_data[\"RecipeYield_Unit\"] = recipes_data[\"RecipeYield\"].str.split(\" \").str[1]\n",
    "recipes_data.drop(columns=[\"RecipeYield\"], inplace=True)\n",
    "\n",
    "\n",
    "# RecipeServings\n",
    "# Logarithmieren von RecipeServings, um die Verteilung zu verbessern\n",
    "recipes_data[\"RecipeServings\"] = recipes_data[\"RecipeServings\"].apply(lambda x: np.log(x) if x > 0 else x)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(recipes_data['RecipeServings'], bins=10)\n",
    "# plt.title('Histogram of Recipe Servings')\n",
    "# plt.xlabel('Recipe Servings')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# Outlier weg\n",
    "std_RecipeServings = recipes_data[recipes_data['RecipeServings'].notna()][\"RecipeServings\"].std()\n",
    "mean_RecipeServings = recipes_data[recipes_data['RecipeServings'].notna()][\"RecipeServings\"].mean()\n",
    "# print(recipes_data['RecipeServings'].describe())\n",
    "\"\"\"\n",
    "mean         1.770398\n",
    "std          0.781950\n",
    "min          0.000000\n",
    "25%          1.386294 -> 25% der Werte sind 1.386294 oder kleiner -> small\n",
    "50%          1.791759 -> 50% der Werte sind 1.791759 oder kleiner -> medium\n",
    "75%          2.079442 -> 75% der Werte sind 2.079442 oder kleiner -> large\n",
    "max          6.907755\n",
    "\"\"\"\n",
    "\n",
    "upper_limit_RecipeServings = mean_RecipeServings + threshold * std_RecipeServings\n",
    "lower_limit_RecipeServings = mean_RecipeServings - threshold * std_RecipeServings\n",
    "\n",
    "recipes_data = recipes_data[((recipes_data[\"RecipeServings\"] >= lower_limit_RecipeServings) & (recipes_data['RecipeServings'] <= upper_limit_RecipeServings)) | (recipes_data['RecipeServings'].isna())]\n",
    "print(\"Shape nach Kürzung der Outlier: \", recipes_data.shape)\n",
    "\n",
    "recipes_data_description = recipes_data.describe()\n",
    "\n",
    "# Conditions\n",
    "conditions = [\n",
    "    (recipes_data[\"RecipeServings\"] <= recipes_data_description.loc[\"25%\", \"RecipeServings\"]),\n",
    "    (recipes_data[\"RecipeServings\"] > recipes_data_description.loc[\"25%\", \"RecipeServings\"]) & (recipes_data[\"RecipeServings\"] <= recipes_data_description.loc[\"75%\", \"RecipeServings\"]),\n",
    "    (recipes_data[\"RecipeServings\"] > recipes_data_description.loc[\"75%\", \"RecipeServings\"])\n",
    "]\n",
    "# choices: small - 0, medium - 1, large - 2\n",
    "choices = [\"Small\", \"Medium\", \"Large\"]\n",
    "\n",
    "# Discretization von RecipeServings -> Klassen: small = 0, medium = 1, large = 2 -> one-hot-encoding\n",
    "recipes_data[\"ServingClass\"] = np.select(conditions, choices, default=np.nan)\n",
    "# Werte mit NA werden durch median 1 ersetzt.\n",
    "recipes_data.loc[recipes_data['RecipeServings'].isna(), \"ServingClass\"] = \"Medium\"\n",
    "\n",
    "# In Csv\n",
    "# path_recipesData_marco = \"../cvs/Marco/recipes_data.csv\"\n",
    "# recipes_data.to_csv(path_recipesData_marco, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Versuch: Fehlende Daten zu imputieren, nicht geklappt!\n",
    "\"\"\"\n",
    "# fehlende Werte in RecipeServings durch random forest imputieren\n",
    "features = ['CookTime', 'PrepTime', 'Calories', 'FatContent', 'FiberContent', 'ProteinContent']\n",
    "\n",
    "# test_data with and without RecipeServings, training_data only with RecipeServings\n",
    "known_servings = recipes_data[recipes_data['ServingClass'].notna()]\n",
    "unknown_servings = recipes_data[recipes_data['ServingClass'].isna()]\n",
    "\n",
    "X = known_servings[features]\n",
    "y_known = known_servings[\"ServingClass\"]\n",
    "\n",
    "# print(X)\n",
    "# print(y_known)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_known, test_size=0.3, random_state=random_seed)\n",
    "# print('Training Features Shape:', X_train.shape)\n",
    "# print('Training Labels Shape:', y_train.shape)\n",
    "# print('Validation Features Shape:', X_val.shape)\n",
    "# print('Validation Labels Shape:', y_val.shape)\n",
    "\n",
    "\n",
    "# Model trainieren\n",
    "model = RandomForestClassifier(n_estimators=200, random_state=random_seed)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluieren\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Bewertung des Modells\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "# calculate r squared\n",
    "r_squared = model.score(X_val, y_val)\n",
    "print('R Squared: ', r_squared)\n",
    "print('MSE Mean Squred Error: ', mse) \n",
    "\n",
    "\n",
    "# y_val together with y_pred \n",
    "y_val_pred = pd.DataFrame({'y_val': y_val, 'y_pred': y_pred})\n",
    "# Extract RecipeId for the validation set\n",
    "recipe_ids_val = recipes_data.loc[y_val.index, 'RecipeId']\n",
    "# Add RecipeId to the y_val_pred dataframe\n",
    "y_val_pred['RecipeId'] = recipe_ids_val\n",
    "\n",
    "# Display the dataframe\n",
    "print(y_val_pred.head(20))\n",
    "\n",
    "\n",
    "# Model anwenden um fehlende Daten zu analysieren\n",
    "# X_unknown = unknown_servings[features]\n",
    "# y_unknown = unknown_servings[\"RecipeServings\"]\n",
    "# y_unknown_pred = model.predict(X_unknown)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape davor (97381, 34)\n",
      "Shape danach (93057, 34)\n",
      "final_table.csv\n"
     ]
    }
   ],
   "source": [
    "# Verbinden von reviews_df mit diet_df über AuthorId\n",
    "combined_df1 = pd.merge(reviews_data, users_diet_data, on='AuthorId', how='left')\n",
    "\n",
    "# Verbinden von combined_df1 mit requests_df über AuthorId und RecipeId\n",
    "combined_df2 = pd.merge(combined_df1, requests_data, on=['AuthorId', 'RecipeId'], how='left')\n",
    "\n",
    "# Verbinden von combined_df2 mit recipes_df über RecipeId\n",
    "final_combined_df = pd.merge(combined_df2, recipes_data, on='RecipeId', how='left')\n",
    "\n",
    "\n",
    "# Clean data without nan values - z.B. falls Rezept gelöscht wurde usw.\n",
    "print(\"Shape davor\", final_combined_df.shape)\n",
    "final_combined_df = final_combined_df[final_combined_df[\"Time\"].notna()]\n",
    "final_combined_df = final_combined_df[final_combined_df[\"Name\"].notna()]\n",
    "print(\"Shape danach\", final_combined_df.shape)\n",
    "\n",
    "\n",
    "csv_file_path = 'final_table.csv'\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "final_combined_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Returning the file path for download\n",
    "print(csv_file_path)\n",
    "\n",
    "\n",
    "# print(final_combined_df[\"ServingClass\"].describe())\n",
    "# plt.hist(final_combined_df[\"ServingClass\"], bins=3)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Werte:  Like\n",
      "False    80813\n",
      "True     12244\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Large'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j0/z37d__mj75g9k_jcd30hbw0c0000gn/T/ipykernel_62265/3162791686.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# column names of final_combined_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Model evaluieren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/1. Studium/TUM/Semester/Semester 3/BAML/AC-Cup/ac-cup-venv/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 skip_parameter_validation=(\n\u001b[1;32m   1149\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/1. Studium/TUM/Semester/Semester 3/BAML/AC-Cup/ac-cup-venv/lib/python3.10/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \"\"\"\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         super()._fit(\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/1. Studium/TUM/Semester/Semester 3/BAML/AC-Cup/ac-cup-venv/lib/python3.10/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    238\u001b[0m             check_X_params = dict(\n\u001b[1;32m    239\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             )\n\u001b[1;32m    241\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             )\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/1. Studium/TUM/Semester/Semester 3/BAML/AC-Cup/ac-cup-venv/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    613\u001b[0m                 \u001b[0;31m# :(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"estimator\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                     \u001b[0mcheck_X_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"estimator\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/1. Studium/TUM/Semester/Semester 3/BAML/AC-Cup/ac-cup-venv/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    912\u001b[0m                         )\n\u001b[1;32m    913\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m                 raise ValueError(\n\u001b[1;32m    918\u001b[0m                     \u001b[0;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 ) from complex_warning\n",
      "\u001b[0;32m~/Documents/1. Studium/TUM/Semester/Semester 3/BAML/AC-Cup/ac-cup-venv/lib/python3.10/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/1. Studium/TUM/Semester/Semester 3/BAML/AC-Cup/ac-cup-venv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m         if (\n\u001b[1;32m   2086\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Large'"
     ]
    }
   ],
   "source": [
    "# 1. Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "features = ['Age', 'Time', 'Diet', 'CookTime', 'PrepTime','HighCalories','HighProtein','LowFat','LowSugar','HighFiber', 'Calories', 'FatContent', 'FiberContent', 'ProteinContent', 'Meat', 'Seafood', 'Vegetarian', 'Vegan', 'HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber', 'Time', 'ServingClass']\n",
    "\n",
    "# Diet in numerischen Wert umwandeln -> 1 = Vegetarian, 2 = Vegan, 3 = Omnivore\n",
    "final_combined_df[\"Diet\"] = final_combined_df[\"Diet\"].apply(lambda x: 1 if x == \"Vegetarian\" else (2 if x == \"Vegan\" else 3))\n",
    "\n",
    "print(\"Werte: \", final_combined_df[\"Like\"].value_counts())\n",
    "\n",
    "bool_columns = ['HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber', 'Meat', 'Seafood', 'Vegetarian', 'Vegan']\n",
    "for col in bool_columns:\n",
    "    final_combined_df[col] = final_combined_df[col].astype(int)\n",
    "\n",
    "\n",
    "X = final_combined_df[features]\n",
    "y = final_combined_df['Like']\n",
    "\n",
    "# column names of final_combined_df\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=random_seed)\n",
    "tree = DecisionTreeClassifier(random_state=random_seed)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluieren\n",
    "y_pred = tree.predict(X_val)\n",
    "\n",
    "# Bewertung des Modells\n",
    "rsquared = tree.score(X_val, y_val)\n",
    "\n",
    "print('R Squared: ', rsquared)\n",
    "\n",
    "# Nebeneinander setzen von y_val und y_pred\n",
    "y_val_pred = pd.DataFrame({'y_val': y_val, 'y_pred': y_pred})   \n",
    "\n",
    "summeTrue = 0\n",
    "summeFalse = 0\n",
    "for i in range(y_pred.shape[0]):\n",
    "    if y_pred[i] == True:\n",
    "        summeTrue += 1\n",
    "    else:\n",
    "        summeFalse += 1\n",
    "\n",
    "print(\"Anzahl true: \", summeTrue)\n",
    "print(\"Anzahl false: \", summeFalse)\n",
    "    \n",
    "\n",
    "# Menge an true und false in y_pred\n",
    "\n",
    "\n",
    "# final_combined_df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Modelle Marco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Nützliche Funktionen\n",
    "def perfomance_measure(TP, FP, TN, FN):\n",
    "    recall = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    BAC = (recall + specificity) / 2\n",
    "\n",
    "    return recall, specificity, BAC\n",
    "\n",
    "def getX(X):\n",
    "    features = [\"Diet\" ,\"Age\" ,\"Time\" ,\"Like\", \"HighCalories\" ,\"HighProtein\" ,\"LowFat\" ,\"LowSugar\" ,\"HighFiber\",\"CookTime\" ,\"PrepTime\" ,\"RecipeCategory\" ,\"Calories\" ,\"FatContent\" ,\"SaturatedFatContent\" ,\"CholesterolContent\" ,\"SodiumContent\" ,\"CarbohydrateContent\" ,\"FiberContent\" ,\"SugarContent\" ,\"ProteinContent\", \"Meat\" ,\"Seafood\" ,\"Vegetarian\" ,\"Vegan\" ,\"ServingClass\"]\n",
    "    unneccessary_features = [\"Time\", \"HighFiber\", \"CookTime\", \"PrepTime\", \"Vegan\"]\n",
    " \n",
    "\n",
    "    for feature in unneccessary_features:\n",
    "        features.remove(feature)\n",
    "    return X[features]\n",
    "\n",
    "def oneHotEncoding(df, columns):\n",
    "    df = pd.get_dummies(df, columns=columns, prefix=columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/z37d__mj75g9k_jcd30hbw0c0000gn/T/ipykernel_62265/1839742493.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_OLS[\"Like\"] = X_OLS[\"Like\"].astype(\"int\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Like  HighCalories  HighProtein  LowFat  LowSugar  Calories  \\\n",
      "0   50     0         False        False   False      True  4.932313   \n",
      "1   78     0         False         True   False     False  5.980656   \n",
      "2   25     0         False         True   False      True  4.833102   \n",
      "4   25     0          True        False   False     False  6.647559   \n",
      "5   25     0          True         True   False     False  5.202357   \n",
      "\n",
      "   FatContent  SaturatedFatContent  CholesterolContent  ...  \\\n",
      "0    0.000000             0.000000            0.000000  ...   \n",
      "1    3.005683             1.704748            3.552487  ...   \n",
      "2    2.674149             1.064711            0.000000  ...   \n",
      "4    2.484907             1.410987            4.709530  ...   \n",
      "5    2.424803             1.945910            3.627004  ...   \n",
      "\n",
      "   RecipeCategory_Beverages  RecipeCategory_Bread  RecipeCategory_Breakfast  \\\n",
      "0                      True                 False                     False   \n",
      "1                     False                 False                     False   \n",
      "2                     False                 False                     False   \n",
      "4                     False                  True                     False   \n",
      "5                     False                 False                     False   \n",
      "\n",
      "   RecipeCategory_Lunch  RecipeCategory_One dish meal  RecipeCategory_Other  \\\n",
      "0                 False                         False                 False   \n",
      "1                 False                         False                  True   \n",
      "2                 False                         False                  True   \n",
      "4                 False                         False                 False   \n",
      "5                 False                         False                  True   \n",
      "\n",
      "   RecipeCategory_Soup  ServingClass_Large  ServingClass_Medium  \\\n",
      "0                False               False                False   \n",
      "1                False               False                 True   \n",
      "2                False               False                 True   \n",
      "4                False               False                 True   \n",
      "5                False                True                False   \n",
      "\n",
      "   ServingClass_Small  \n",
      "0                True  \n",
      "1               False  \n",
      "2               False  \n",
      "4               False  \n",
      "5               False  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "True Positives:  956\n",
      "True Negatives:  5121\n",
      "False Positives:  2977\n",
      "False Negatives:  252\n",
      "BAC:  0.7118845467526116\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "\n",
    "# Model 1: Linear Regression\n",
    "X = final_combined_df\n",
    "\n",
    "X_OLS = getX(X)\n",
    "\n",
    "X_OLS[\"Like\"] = X_OLS[\"Like\"].astype(\"int\")\n",
    "\n",
    "X_OLS = oneHotEncoding(X_OLS, [\"Diet\", \"RecipeCategory\", \"ServingClass\"])\n",
    "\n",
    "# Drop one column from each one-hot-encoded column\n",
    "X_OLS = X_OLS.drop([\"Diet_Vegan\", \"RecipeCategory_Breakfast\", \"ServingClass_Small\"], axis=1)\n",
    "\n",
    "# Convariance Matrix\n",
    "X_OLS.corr().to_csv(\"covariance_matrix.csv\")\n",
    "\n",
    "# Split data in train and test set\n",
    "X_train_OLS, X_test_OLS, y_train_OLS, y_test_OLS = train_test_split(X_OLS.drop(\"Like\", axis=1), X_OLS[\"Like\"], test_size=0.1, random_state=random_seed)\n",
    "\n",
    "# print(\"Amount unique Values in y_train: \", y_train.value_counts())\n",
    "\n",
    "X_train_OLS = X_train_OLS.astype(float)\n",
    "X_train_OLS = sm.add_constant(X_train_OLS)\n",
    "X_test_OLS = X_test_OLS.astype(float)\n",
    "X_test_OLS = sm.add_constant(X_test_OLS)\n",
    "\n",
    "\n",
    "# for index, variable_name in enumerate(X_train_OLS.columns):\n",
    "#     if variable_name == \"const\": \n",
    "#         continue\n",
    "#     print(f\"VIF for variable {variable_name} is {vif(X_train_OLS, index)}\")\n",
    "\n",
    "# Gewichtung der Daten - 8 mal so viel Gewicht auf Like = True\n",
    "weigths = y_train_OLS.apply(lambda x: 8 if x == 1 else 1)\n",
    "\n",
    "model_OLS = sm.WLS(y_train_OLS, X_train_OLS, weights=weigths).fit()\n",
    "# print(model_OMS.summary())\n",
    "\n",
    "# Prediction\n",
    "y_pred_OLS = model_OLS.predict(X_test_OLS)\n",
    "y_pred_OLS_binary = (y_pred_OLS > 0.5).astype(int)\n",
    "\n",
    "# Bewertung\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_OLS, y_pred_OLS_binary).ravel()\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n",
    "\n",
    "recall, specificity, BAC = perfomance_measure(tp, fp, tn, fn)\n",
    "print(\"BAC: \", BAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/z37d__mj75g9k_jcd30hbw0c0000gn/T/ipykernel_60427/3346428575.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_LogReg[\"Like\"] = X_LogReg[\"Like\"].astype(\"int\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After OverSampling, counts of label '1': 72715\n",
      "After OverSampling, counts of label '0': 72715\n",
      "y_pred:  [1 0 0 ... 0 1 1]\n",
      "True Positives:  758\n",
      "True Negatives:  6076\n",
      "False Positives:  2022\n",
      "False Negatives:  450\n",
      "BAC:  0.6888960809553173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = final_combined_df\n",
    "\n",
    "X_LogReg = getX(X)\n",
    "\n",
    "# # Unnötige Features aufgrund von Covariance Matrix entfernen\n",
    "# features_OLS_unnecessary = [\"Time\", \"HighFiber\", \"CookTime\", \"PrepTime\", \"Vegan\"]\n",
    "# for feature in features_OLS_unnecessary:\n",
    "#     features_OLS.remove(feature)\n",
    "\n",
    "# One-Hot-Encoding für Like\n",
    "X_LogReg[\"Like\"] = X_LogReg[\"Like\"].astype(\"int\")\n",
    "\n",
    "X_LogReg = oneHotEncoding(X_LogReg, [\"Diet\", \"RecipeCategory\", \"ServingClass\"])\n",
    "\n",
    "# Drop one column from each one-hot-encoded column\n",
    "X_LogReg = X_LogReg.drop([\"Diet_Vegan\", \"RecipeCategory_Breakfast\", \"ServingClass_Small\"], axis=1)\n",
    "\n",
    "X_train_LogReg, X_test_LogReg, y_train_LogReg, y_test_LogReg = train_test_split(X_LogReg.drop(\"Like\", axis=1), X_LogReg[\"Like\"], test_size=0.1, random_state=random_seed)\n",
    "\n",
    "\n",
    "smote_LogReg = SMOTE(random_state=random_seed, sampling_strategy = 1)\n",
    "X_train_resampled, y_train_resampled = smote_LogReg.fit_resample(X_train_LogReg, y_train_LogReg)\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_resampled == 1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_resampled == 0)))\n",
    "\n",
    "\n",
    "model_LogReg = LogisticRegression(random_state=random_seed, max_iter=1000)\n",
    "model_LogReg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Prediction\n",
    "y_pred_LogReg = model_LogReg.predict(X_test_LogReg)\n",
    "print(\"y_pred: \", y_pred_LogReg)\n",
    "\n",
    "# Bewertung\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_LogReg, y_pred_LogReg).ravel()\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n",
    "\n",
    "recall, specificity, BAC = perfomance_measure(tp, fp, tn, fn)\n",
    "\n",
    "print(\"BAC: \", BAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  [1 0 0 ... 0 1 1]\n",
      "True Positives:  762\n",
      "True Negatives:  5743\n",
      "False Positives:  2355\n",
      "False Negatives:  446\n",
      "BAC:  0.6699910778395123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/z37d__mj75g9k_jcd30hbw0c0000gn/T/ipykernel_62265/505813061.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_NB[\"Like\"] = X_NB[\"Like\"].astype(\"int\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = final_combined_df\n",
    "\n",
    "X_NB = getX(X)\n",
    "\n",
    "X_NB[\"Like\"] = X_NB[\"Like\"].astype(\"int\")\n",
    "\n",
    "X_NB = oneHotEncoding(X_NB, [\"Diet\", \"RecipeCategory\", \"ServingClass\"])\n",
    "\n",
    "# Drop one column from each one-hot-encoded column\n",
    "X_NB = X_NB.drop([\"Diet_Vegan\", \"RecipeCategory_Breakfast\", \"ServingClass_Small\"], axis=1)\n",
    "\n",
    "X_train_NB, X_test_NB, y_train_NB, y_test_NB = train_test_split(X_NB.drop(\"Like\", axis=1), X_NB[\"Like\"], test_size=0.1, random_state=random_seed)\n",
    "\n",
    "smote_NB = SMOTE(random_state=random_seed, sampling_strategy = 0.9)\n",
    "X_train_resampled_NB, y_train_resampled_NB = smote_NB.fit_resample(X_train_NB, y_train_NB)\n",
    "\n",
    "model_NB = GaussianNB()\n",
    "model_NB.fit(X_train_resampled_NB, y_train_resampled_NB)\n",
    "\n",
    "# Prediction\n",
    "y_pred_NB = model_NB.predict(X_test_NB)\n",
    "print(\"y_pred: \", y_pred_NB)\n",
    "\n",
    "# Bewertung\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_NB, y_pred_NB).ravel()\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives: \", fp)\n",
    "print(\"False Negatives: \", fn)\n",
    "\n",
    "recall, specificity, BAC = perfomance_measure(tp, fp, tn, fn)\n",
    "print(\"BAC: \", BAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j0/z37d__mj75g9k_jcd30hbw0c0000gn/T/ipykernel_67493/41291547.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_RF[\"Like\"] = X_RF[\"Like\"].astype(\"int\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "X = final_combined_df\n",
    "\n",
    "X_RF = getX(X)\n",
    "\n",
    "X_RF[\"Like\"] = X_RF[\"Like\"].astype(\"int\")\n",
    "\n",
    "X_RF = oneHotEncoding(X_RF, [\"Diet\", \"RecipeCategory\", \"ServingClass\"])\n",
    "\n",
    "# Drop one column from each one-hot-encoded column\n",
    "X_RF = X_RF.drop([\"Diet_Vegan\", \"RecipeCategory_Breakfast\", \"ServingClass_Small\"], axis=1)\n",
    "\n",
    "X_train_RF, X_test_RF, y_train_RF, y_test_RF = train_test_split(X_RF.drop(\"Like\", axis=1), X_RF[\"Like\"], test_size=0.3, random_state=random_seed)\n",
    "\n",
    "smote_RF = SMOTE(random_state=random_seed, sampling_strategy = 0.9)\n",
    "X_train_resampled_RF, y_train_resampled_RF = smote_RF.fit_resample(X_train_RF, y_train_RF)\n",
    "\n",
    "model_RF = RandomForestClassifier(random_state=random_seed, \n",
    "                                  n_estimators=100,\n",
    "                                  max_features=\"sqrt\",   # Einsatz der Qaudratwurzel der Feature-Anzahl = default\n",
    "                                  max_depth=17   \n",
    "                                  )\n",
    "model_RF.fit(X_train_resampled_RF, y_train_resampled_RF)\n",
    "# model_RF.fit(X_train_RF, y_train_RF)\n",
    "\n",
    "# Prediction - Bias vs Variance Tradeoff\n",
    "# Training Data\n",
    "y_train_pred_RF = model_RF.predict(X_train_resampled_RF)\n",
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train_resampled_RF, y_train_pred_RF).ravel()\n",
    "recall_train, specificity_train, BAC_train = perfomance_measure(tp_train, fp_train, tn_train, fn_train)\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(\"BAC: \", BAC_train)\n",
    "\n",
    "# Validation Data\n",
    "y_pred_RF = model_RF.predict(X_test_RF)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_RF, y_pred_RF).ravel()\n",
    "recall, specificity, BAC = perfomance_measure(tp, fp, tn, fn)\n",
    "print(\"Test Set Evaluation:\")\n",
    "print(\"BAC: \", BAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac-cup-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
